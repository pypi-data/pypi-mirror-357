{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# MidiTok Full Workflow Example/Tutorial\n",
    "\n",
    "***\n",
    "\n",
    "Credit for GPT2-RGA code used in this colab goes out @ Sashmark97 https://github.com/Sashmark97/midigen and @ Damon Gwinn https://github.com/gwinndr/MusicTransformer-Pytorch\n",
    "\n",
    "***\n",
    "\n",
    "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please exercise great humility, care, and respect. https://www.nscai.gov/\n",
    "\n",
    "***\n",
    "\n",
    "#### Project Los Angeles\n",
    "\n",
    "#### Tegridy Code 2021\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "# (Setup Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lw-4aqV3sKQG"
   },
   "outputs": [],
   "source": [
    "#@title nvidia-smi gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install miditok\n",
    "\n",
    "!wget 'https://raw.githubusercontent.com/asigalov61/tegridy-tools/main/tegridy-tools/GPT2RGA.py'\n",
    "\n",
    "!wget 'https://github.com/asigalov61/Optimus-VIRTUOSO/raw/main/Samples/Relative-Global-Attention/Optimus-VIRTUOSO-RGA-Edition-Main-Sample.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "z7n9vnKmug1J"
   },
   "outputs": [],
   "source": [
    "#@title Import all needed modules\n",
    "\n",
    "print('Loading needed modules. Please wait...')\n",
    "import os\n",
    "from datetime import datetime\n",
    "import secrets\n",
    "import copy\n",
    "import tqdm\n",
    "from tqdm import auto\n",
    "\n",
    "from GPT2RGA import *\n",
    "\n",
    "from miditok import REMI, CPWord, get_midi_programs\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing source MIDI file...')\n",
    "\n",
    "# Our parameters\n",
    "pitch_range = range(21, 109)\n",
    "beat_res = {(0, 4): 8, (4, 12): 4}\n",
    "num_velocities = 32\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True,\n",
    "                     'rest_range': (2, 8),  # (half, 8 beats)\n",
    "                     'num_tempos': 32,  # num of tempo bins\n",
    "                     'tempo_range': (40, 250),  # (min, max)\n",
    "                     'Program': False}\n",
    "\n",
    "# Creates the tokenizer and loads a MIDI\n",
    "tokenizer = REMI(pitch_range, beat_res, nb_velocities, additional_tokens) # REMI encoding\n",
    "\n",
    "# tokenizer = CPWord(pitch_range, beat_res, nb_velocities, additional_tokens) # CP encoding\n",
    "\n",
    "\n",
    "midi = MidiFile('Optimus-VIRTUOSO-RGA-Edition-Main-Sample.mid')\n",
    "\n",
    "# Converts MIDI to tokens, and back to a MIDI\n",
    "tokens = tokenizer.midi_to_tokens(midi)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oy0R_gNuI85g"
   },
   "source": [
    "# (QUICK DEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lT0TyqUnpxu_"
   },
   "outputs": [],
   "source": [
    "#@title Load processed INTs datasets\n",
    "number_of_batches = 16 #@param {type:\"slider\", min:2, max:32, step:2}\n",
    "n_workers = 6\n",
    "\n",
    "print('=' * 50)\n",
    "print('Prepping INTs datasets...')\n",
    "\n",
    "train_data = []\n",
    "\n",
    "train_data.extend(tokens[0])\n",
    "\n",
    "val_dataset = train_data[:int(len(train_data) * 0.5)]\n",
    "test_dataset = train_data[:int(len(train_data) * 0.5)]\n",
    "\n",
    "train_list = train_data\n",
    "val_list = val_dataset\n",
    "test_list = []\n",
    "print('=' * 50)\n",
    "\n",
    "print('Processing INTs datasets...')\n",
    "train_dataset = EPianoDataset(train_list, max_seq, random_seq)\n",
    "val_dataset = EPianoDataset(val_list, max_seq)\n",
    "test_dataset = EPianoDataset(test_list, max_seq)\n",
    "print('=' * 50)\n",
    "\n",
    "print('Loading INTs datasets...')\n",
    "batch_size = number_of_batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "print('=' * 50)\n",
    "\n",
    "print('Total INTs in the dataset', len(train_data))\n",
    "print('Total unique INTs in the dataset', len(set(train_data)))\n",
    "print('Max INT in the dataset', max(train_data))\n",
    "print('Min INT in the dataset', min(train_data))\n",
    "print('=' * 50)\n",
    "\n",
    "print('Checking datasets shapes...')\n",
    "print('=' * 50)\n",
    "\n",
    "print('Train loader')\n",
    "for x, tgt in train_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "print('=' * 50)\n",
    "\n",
    "print('Validation loader')\n",
    "for x, tgt in val_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "print('=' * 50)\n",
    "\n",
    "print('Test loader')\n",
    "for x, tgt in test_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "print('=' * 50)\n",
    "\n",
    "print('Done! Enjoy! :)')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkVqviDzJOrv"
   },
   "source": [
    "# (TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9CBW8xYupH8"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2moo7uUmpxvC"
   },
   "outputs": [],
   "source": [
    "#@title Train\n",
    "\n",
    "print('MidiTok Model Trainer')\n",
    "\n",
    "config = GPTConfig(VOCAB_SIZE, \n",
    "                   max_seq,\n",
    "                   dim_feedforward=dim_feedforward,\n",
    "                   n_layer=6, \n",
    "                   n_head=8, \n",
    "                   n_embd=512,\n",
    "                   enable_rpr=True,\n",
    "                   er_len=max_seq)\n",
    "model = GPT(config).to(get_device())\n",
    "\n",
    "#=====\n",
    "\n",
    "init_step = 0\n",
    "lr = LR_DEFAULT_START\n",
    "lr_stepper = LrStepTracker(d_model, SCHEDULER_WARMUP_STEPS, init_step)\n",
    "eval_loss_func = nn.CrossEntropyLoss(ignore_index=TOKEN_PAD)\n",
    "train_loss_func = eval_loss_func\n",
    "\n",
    "opt = Adam(model.parameters(), lr=lr, betas=(ADAM_BETA_1, ADAM_BETA_2), eps=ADAM_EPSILON)\n",
    "lr_scheduler = LambdaLR(opt, lr_stepper.step)\n",
    "\n",
    "\n",
    "#===\n",
    "\n",
    "best_eval_acc        = 0.0\n",
    "best_eval_acc_epoch  = -1\n",
    "best_eval_loss       = float(\"inf\")\n",
    "best_eval_loss_epoch = -1\n",
    "best_acc_file = 'gpt2_rpr_acc.pth'\n",
    "best_loss_file = 'gpt2_rpr_loss.pth'\n",
    "loss_train, loss_val, acc_val = [], [], []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    new_best = False\n",
    "    \n",
    "    loss = train(epoch+1, model, train_loader, train_loss_func, opt, lr_scheduler, num_iters=-1)\n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)\n",
    "    loss_val.append(eval_loss)\n",
    "    acc_val.append(eval_acc)\n",
    "    \n",
    "    if(eval_acc > best_eval_acc):\n",
    "        best_eval_acc = eval_acc\n",
    "        best_eval_acc_epoch  = epoch+1\n",
    "        torch.save(model.state_dict(), best_acc_file)\n",
    "        new_best = True\n",
    "\n",
    "    if(eval_loss < best_eval_loss):\n",
    "        best_eval_loss       = eval_loss\n",
    "        best_eval_loss_epoch = epoch+1\n",
    "        torch.save(model.state_dict(), best_loss_file)\n",
    "        new_best = True\n",
    "    \n",
    "    if(new_best):\n",
    "        print(\"Best eval acc epoch:\", best_eval_acc_epoch)\n",
    "        print(\"Best eval acc:\", best_eval_acc)\n",
    "        print(\"\")\n",
    "        print(\"Best eval loss epoch:\", best_eval_loss_epoch)\n",
    "        print(\"Best eval loss:\", best_eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NNqmcFdRyC2M"
   },
   "outputs": [],
   "source": [
    "#@title Plot resulting training loss graph\n",
    "tr_loss_list = [item for sublist in loss_train for item in sublist]\n",
    "plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "plt.savefig('MidiTok-Training-Loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdKFoeke9L7H"
   },
   "source": [
    "# (SAVE/LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gqyDatHC9X1z"
   },
   "outputs": [],
   "source": [
    "#@title Save the model\n",
    "\n",
    "print('Saving the model...')\n",
    "full_path_to_model_checkpoint = \"MidiTok.pth\" #@param {type:\"string\"}\n",
    "torch.save(model.state_dict(), full_path_to_model_checkpoint)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [],
   "source": [
    "#@title Load/Reload the model\n",
    "full_path_to_model_checkpoint = \"MidiTok.pth\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading the model...')\n",
    "config = GPTConfig(VOCAB_SIZE, \n",
    "                   max_seq,\n",
    "                   dim_feedforward=dim_feedforward,\n",
    "                   n_layer=6, \n",
    "                   n_head=8, \n",
    "                   n_embd=512,\n",
    "                   enable_rpr=True,\n",
    "                   er_len=max_seq)\n",
    "\n",
    "model = GPT(config).to(get_device())\n",
    "\n",
    "model.load_state_dict(torch.load(full_path_to_model_checkpoint))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation routine draft code\n",
    "\n",
    "print('Loading source continuation MIDI...')\n",
    "\n",
    "# Creates the tokenizer and loads a MIDI\n",
    "tokenizer1 = REMI(pitch_range, beat_res, nb_velocities, additional_tokens)\n",
    "midi1 = MidiFile('seed.mid')\n",
    "\n",
    "# Converts MIDI to tokens, and back to a MIDI\n",
    "tokens1 = tokenizer1.midi_to_tokens(midi1)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from the model\n",
    "\n",
    "## Seed generator...\n",
    "\n",
    "### If you getting a file error on MIDI save, create test.mid file manually, then re-run\n",
    "\n",
    "\n",
    "print('MidiTok Model Generator')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# rand_seq = model.generate(torch.Tensor(tokens1[0][-64:]), target_seq_length=1024) # Continuation example\n",
    "rand_seq = model.generate(torch.Tensor([1]), target_seq_length=1024)\n",
    "out = rand_seq[0].cpu().numpy().tolist()\n",
    "\n",
    "converted_back_midi = tokenizer.tokens_to_midi([out], get_midi_programs(midi))\n",
    "converted_back_midi.dump('MidiTok-OUTPUT.mid')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auto-Regressive Generator\n",
    "\n",
    "#@markdown NOTE: You much generate a seed composition first or it is not going to start\n",
    "\n",
    "number_of_cycles_to_run = 5 #@param {type:\"slider\", min:1, max:50, step:1}\n",
    "number_of_prime_tokens = 128 #@param {type:\"slider\", min:64, max:256, step:64}\n",
    "\n",
    "print('=' * 70)\n",
    "print('MidiTok Auto-Regressive Model Generator')\n",
    "print('=' * 70)\n",
    "print('Starting up...')\n",
    "print('=' * 70)\n",
    "print('Prime length:', len(out))\n",
    "print('Prime tokens:', number_of_prime_tokens)\n",
    "print('Prime input sequence', out[-8:])\n",
    "\n",
    "if len(out) != 0:\n",
    "  print('=' * 70)\n",
    "  out_all = []\n",
    "  out_all.append(out)\n",
    "  for i in tqdm(range(number_of_cycles_to_run)):\n",
    "      rand_seq1 = model.generate(torch.Tensor(out[-number_of_prime_tokens:]), target_seq_length=1024)\n",
    "      out1 = rand_seq1[0].cpu().numpy().tolist()\n",
    "      out_all.append(out1[number_of_prime_tokens:])\n",
    "      out = out1[number_of_prime_tokens:]\n",
    "      \n",
    "      print(chr(10))\n",
    "      print('=' * 70)\n",
    "      print('Block number:', i+1)\n",
    "      print('Composition length so far:', (i+1) * 1024, 'notes')\n",
    "      print('=' * 70)\n",
    "\n",
    "  print('Done!' * 70)\n",
    "  print('Total blocks:', i+1)\n",
    "  print('Final omposition length:', (i+1) * 1024, 'notes')\n",
    "  print('=' * 70)\n",
    "\n",
    "OUT = []\n",
    "\n",
    "for o in out_all:\n",
    "    OUT.extend(o)\n",
    "    \n",
    "    \n",
    "converted_back_midi = tokenizer.tokens_to_midi([OUT], get_midi_programs(midi))\n",
    "converted_back_midi.dump('MidiTok-OUTPUT.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzCMd94Tu_gz"
   },
   "source": [
    "# Congrats! You did it! :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
