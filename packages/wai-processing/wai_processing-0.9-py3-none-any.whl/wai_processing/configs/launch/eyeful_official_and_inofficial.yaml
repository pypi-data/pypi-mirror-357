# NOTE: No dryrun for eyeful as eyeful has only 20 scenes (13 official ones, and 7 inofficial scenes)
# --> Just directly launch prod run

stage: # set stage via CLI
# Processing all official and inofficial scenes at once, filtering can be done super easily
# with a regex filter afterwards as all official scenes start with "inofficial"
root: /fsx/xrtech/data/eyeful_tower_official_and_inofficial # path of wai-formatted dataset

gpus: 0
cpus: 10
mem: 20
scenes_per_job: 1
conda_env: # pass the name of our conda environment
nodelist: h100-ai-p5en48xlarge-[0-10]

stages:
  conversion:
    # Very fast for all official scenes, as we already have the 4K images a source and just need to softlink them
    # and create the scene_meta_distorted.json
    # For unofficial scenes the images are downscaled from 8K to 4K.
    script: conversion/eyeful_tower_dataset.py
    config: conversion/eyeful_official_and_inofficial.yaml
    mem: 50  # Need more mem for scenes were 8K images are processed, need to investigate RAM OOM
  undistortion:
    script: undistort.py
    config: undistortion/default.yaml
  mesh_render:
  semantics_render:
  metric3dv2:
    script: run_metric3dv2.py
    config: metric3dv2/default.yaml
    gpus: 1
  metric_alignment: null
  covisibility:
    script: covisibility.py
    config: covisibility/covisibility_pred_depth_224x224.yaml
    gpus: 1
  copy_to_s3:
    script: run_copy_to_s3.py
    config: data_transfer/to_s3_default.yaml
