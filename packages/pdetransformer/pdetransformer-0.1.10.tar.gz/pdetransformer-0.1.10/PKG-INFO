Metadata-Version: 2.4
Name: pdetransformer
Version: 0.1.10
Summary: PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations
Author-email: Benjamin Holzschuh <benjamin.holzschuh@tum.de>
License: MIT
Project-URL: Homepage, https://tum-pbs.github.io/pde-transformer/landing.html
Project-URL: Repository, https://github.com/tum-pbs/pde-transformer.git
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Physics
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: LICENSE.txt
Requires-Dist: lightning>=2.2.5
Requires-Dist: omegaconf>=2.3.0
Requires-Dist: einops>=0.8.1
Requires-Dist: lightning-bolts>=0.7.0
Requires-Dist: torchvision>=0.18.0
Requires-Dist: matplotlib>=3.7.0
Requires-Dist: diffusers>=0.28.2
Requires-Dist: torch-fidelity>=0.3.0
Requires-Dist: wandb>=0.17.0
Requires-Dist: numpy>=1.26.4
Requires-Dist: transformers>=4.42.3
Requires-Dist: accelerate>=0.30.1
Requires-Dist: moviepy>=1.0.3
Requires-Dist: pandas>=2.2.2
Requires-Dist: imageio>=2.34.2
Requires-Dist: timm>=1.0.12
Requires-Dist: seaborn>=0.13.2
Requires-Dist: torchprofile>=0.0.4
Requires-Dist: h5py>=3.11.0
Requires-Dist: vape4d>=0.2.7
Dynamic: license-file

# PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations

<div align="center">

<p align="center">
<a href="https://pypi.org/project/pdetransformer/">
  <img src="https://img.shields.io/pypi/v/pretransformer.svg" alt="PyPI">
</a> 
<a href="https://tum-pbs.github.io/pde-transformer">
  <img src="https://img.shields.io/badge/docs-latest-green" alt="docs-latest">
</a>
<a href="https://github.com/tum-pbs/pde-transformer/releases">
  <img src="https://img.shields.io/github/v/release/tum-pbs/pde-transformer?include_prereleases&label=changelog" alt="Changelog">
</a>
<a href="https://github.com/tum-pbs/pde-transformer/blob/main/LICENSE.txt">
  <img src="https://img.shields.io/badge/license-MIT-blue" alt="License">
</a>
</p>

[Paper](https://arxiv.org/pdf/2505.24717.pdf) â€¢ 
[Project Page](https://tum-pbs.github.io/pde-transformer/landing.html) â€¢ 
[ðŸ¤— Hugging Face](https://huggingface.co/thuerey-group/pde-transformer) â€¢ 
[Documentation](https://tum-pbs.github.io/pde-transformer)
<br>
[Installation](#-quick-installation) â€¢
[Description](#-model-description) â€¢
[Features](#-key-highlights) â€¢
[Usage](#-usage)

</div>

---

**Authors:** [Benjamin Holzschuh](), [Qiang Liu](), [Georg Kohl](), [Nils Thuerey](https://ge.in.tum.de/about/n-thuerey/)

**PDE-Transformer** is a state-of-the-art neural architecture for physics simulations, specifically designed for partial differential equations (PDEs) on regular grids. This work will be presented at **ICML 2025**.

### Key Highlights
- **Production Ready**: Available as a pip package for easy installation and experimentation.
- **State-of-the-Art**: Outperforms existing methods across 16 different types of PDEs and three challenging downstream tasks involving complex dynamics. 
- **Transfer Learning**: Improved performance when adapting pre-trained models to new physics problems with limited training data.
- **Open Source**: Full implementation with pre-trained models and comprehensive documentation.

### Quick Installation

```bash
# Install from PyPI
pip install pdetransformer

# Or install from source
git clone https://github.com/pde-transformer/pde-transformer.git
cd pde-transformer
pip install -e .
```

## Model Description

PDE-Transformer is designed to efficiently process and predict the evolution of physical systems described by partial differential equations (PDEs). It can handle multiple types of PDEs, different resolutions, domain extents, boundary conditions, 
and includes deep conditioning mechanisms for PDE- and task-specific information.

Key features:
- **Multi-scale architecture** with token down- and upsampling for efficient modeling.
- **Shifted window attention** for improved scaling to high-resolution data.
- **Mixed Channel (MC) and Separate Channel (SC)** representations for handling multiple physical quantities.
- **Flexible conditioning mechanism** for PDE parameters, boundary conditions, and simulation metadata.
- **Pre-training and fine-tuning capabilities** for transfer learning across different physics domains.

### Training Objectives

The model supports both supervised and diffusion training:

- **Supervised Training**: Direct MSE loss for deterministic, unique solutions. Fast training and inference. 
- **Flow Matching**: For probabilistic modeling and uncertainty quantification.

## Supported PDE Types

PDE-Transformer has been trained and evaluated on 16 different types of PDEs including:

- **Linear PDEs**: Diffusion
- **Nonlinear PDEs**: Burgers, Korteweg-de-Vries, Kuramoto-Sivashinsky
- **Reaction-Diffusion**: Fisher-KPP, Swift-Hohenberg, Gray-Scott
- **Fluid Dynamics**: Navier-Stokes (Decaying Turbulence, Kolmogorov Flow)

## Quick Start

```python
from pdetransformer.core.mixed_channels import PDETransformer
import torch

# Load pre-trained model
model = PDETransformer.from_pretrained('thuerey-group/pde-transformer', subfolder='mc-s').cuda()

# For physics simulation
x = torch.randn((1,2,256,256), dtype=torch.float32).cuda()
predictions = model(x)
```

## Documentation

For detailed documentation, visit [tum-pbs.github.io/pde-transformer](https://tum-pbs.github.io/pde-transformer/).

## Citation

If you use PDE-Transformer in your research, please cite:

```bibtex
@article{holzschuh2025pde,
  title={PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations},
  author={Holzschuh, Benjamin and Liu, Qiang and Kohl, Georg and Thuerey, Nils},
  booktitle={Forty-second International Conference on Machine Learning, {ICML} 2025, Vancouver, Canada, July 13-19, 2025},
  year={2025}
}
```

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE.txt) file for details.

---

**Note**: This is a research project from the Technical University of Munich (TUM) Physics-based Simulation Group. 
For questions and support, please refer to the GitHub repository or contact the authors.
