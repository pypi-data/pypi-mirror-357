"""
å‘½ä»¤è¡Œç•Œé¢
"""

import argparse
import sys
import os
import logging
from pathlib import Path
from typing import List, Optional
import yaml

from .config import load_config, Config
from .downloader import create_downloader
from .transcriber import create_transcriber, TranscriptionError
from .corrector import create_corrector_from_config, DeepSeekAPIError
from .output_formatter import create_formatter, DocumentMetadata, DocumentFormatterError
from .progress import StatusDisplay, EnhancedProgress, create_progress_callback
from .config_manager import (
    get_config_manager, 
    save_current_config_as_profile, 
    load_config_profile,
    list_config_profiles,
    delete_config_profile
)


def setup_logging(level: str = "INFO", log_file: Optional[str] = None):
    """è®¾ç½®æ—¥å¿—"""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    if log_file:
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # é…ç½®æ ¹æ—¥å¿—
    logging.basicConfig(
        level=numeric_level,
        handlers=[console_handler] + ([logging.FileHandler(log_file)] if log_file else []),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


# å…¨å±€å˜é‡ç”¨äºè¿›åº¦æ˜¾ç¤º
_verbose_mode = False


def set_verbose_mode(verbose: bool):
    """è®¾ç½®è¯¦ç»†æ¨¡å¼"""
    global _verbose_mode
    _verbose_mode = verbose


def get_progress_callback(operation_type: str):
    """è·å–é€‚åˆçš„è¿›åº¦å›è°ƒå‡½æ•°"""
    return create_progress_callback(operation_type, _verbose_mode)


def cmd_process(args):
    """å¤„ç†å•ä¸ªURL"""
    # è®¾ç½®è¯¦ç»†æ¨¡å¼
    set_verbose_mode(getattr(args, 'verbose', False))
    
    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
    progress = EnhancedProgress(_verbose_mode)
    
    try:
        # æ­¥éª¤1: åŠ è½½é…ç½®
        progress.start_operation("åŠ è½½é…ç½®")
        
        # å¦‚æœæŒ‡å®šäº†profileï¼Œå…ˆåŠ è½½profileé…ç½®
        config = None
        if getattr(args, 'profile', None):
            config = load_config_profile(args.profile)
            if config is None:
                StatusDisplay.error(f"é…ç½®é¢„è®¾ä¸å­˜åœ¨: {args.profile}")
                return False
            progress.update_operation(f"å·²åŠ è½½é…ç½®é¢„è®¾: {args.profile}")
        
        # åŠ è½½åŸºç¡€é…ç½®
        if config is None:
            config = load_config(args.config)
        
        # ä»å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
        if args.output_dir:
            config.download.output_dir = args.output_dir
        if args.audio_quality:
            config.download.audio_quality = args.audio_quality
        if args.cookie_browser:
            config.download.cookies.from_browser = args.cookie_browser
        if args.cookie_file:
            config.download.cookies.cookie_file = args.cookie_file
        
        progress.complete_operation("é…ç½®åŠ è½½å®Œæˆ")
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(config.logging.level, config.logging.file)
        
        # æ­¥éª¤2: åˆå§‹åŒ–ä¸‹è½½å™¨
        progress.start_operation("åˆå§‹åŒ–ä¸‹è½½å™¨")
        downloader = create_downloader()
        downloader.config = config
        progress.complete_operation("ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # æ­¥éª¤3: éªŒè¯URL
        progress.start_operation("éªŒè¯URL")
        supported, platform = downloader.check_url_support(args.url)
        if not supported:
            StatusDisplay.error(f"ä¸æ”¯æŒçš„å¹³å°", platform, [
                "æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤å¹³å°æ˜¯å¦å—æ”¯æŒ",
                "æŸ¥çœ‹æ–‡æ¡£äº†è§£æ”¯æŒçš„å¹³å°åˆ—è¡¨"
            ])
            return False
        
        progress.update_operation(f"æ£€æµ‹åˆ°å¹³å°: {platform}")
        progress.complete_operation("URLéªŒè¯å®Œæˆ")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        if args.info_only:
            progress.start_operation("è·å–è§†é¢‘ä¿¡æ¯")
            info = downloader.get_video_info(args.url)
            if info:
                StatusDisplay.success("è§†é¢‘ä¿¡æ¯è·å–æˆåŠŸ")
                StatusDisplay.info("è§†é¢‘è¯¦æƒ…", f"""
æ ‡é¢˜: {info['title']}
æ—¶é•¿: {info['duration']}ç§’
ä¸Šä¼ è€…: {info['uploader']}
å¹³å°: {info['platform']}
æ’­æ”¾é‡: {info['view_count']:,}""" if info['view_count'] else "")
                progress.complete_operation()
            else:
                StatusDisplay.error("æ— æ³•è·å–è§†é¢‘ä¿¡æ¯", "å¯èƒ½éœ€è¦cookieéªŒè¯", [
                    "ä½¿ç”¨ --cookie-browser chrome é€‰é¡¹",
                    "ä½¿ç”¨ --cookie-file cookies.txt é€‰é¡¹",
                    "æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„cookieè®¾ç½®"
                ])
                progress.fail_operation("è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥")
            return True
        
        # æ­¥éª¤4: ä¸‹è½½éŸ³é¢‘
        progress.start_operation("ä¸‹è½½éŸ³é¢‘")
        progress_callback = get_progress_callback("download")
        
        result = downloader.download_audio(
            args.url, 
            args.output_name,
            progress_callback
        )
        
        if result.success:
            StatusDisplay.success("éŸ³é¢‘ä¸‹è½½å®Œæˆ", f"æ–‡ä»¶ä½ç½®: {result.file_path}")
            progress.complete_operation()
            
            # è½¬å½•å¤„ç†
            if args.transcribe:
                transcribe_success = perform_transcription(result.file_path, config, args)
                if not transcribe_success:
                    StatusDisplay.error("è½¬å½•å¤„ç†å¤±è´¥")
                    return False
            
            return True
        else:
            error_suggestions = []
            if "cookie" in result.message.lower() or "éªŒè¯" in result.message.lower():
                error_suggestions = [
                    "ä½¿ç”¨æµè§ˆå™¨cookie: --cookie-browser chrome",
                    "ä½¿ç”¨cookieæ–‡ä»¶: --cookie-file cookies.txt",
                    "æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„cookieè®¾ç½®"
                ]
            
            StatusDisplay.error("ä¸‹è½½å¤±è´¥", result.message, error_suggestions)
            progress.fail_operation(result.message, error_suggestions)
            return False
            
    except Exception as e:
        StatusDisplay.error("å¤„ç†å¤±è´¥", str(e), [
            "æ£€æŸ¥ç½‘ç»œè¿æ¥",
            "éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ",
            "æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯"
        ])
        progress.fail_operation(str(e))
        return False


def perform_transcription(audio_file: str, config: Config, args) -> bool:
    """æ‰§è¡ŒéŸ³é¢‘è½¬å½•"""
    try:
        audio_path = Path(audio_file)
        if not audio_path.exists():
            print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
            return False
        
        # åˆ›å»ºè½¬å½•å™¨
        transcriber = create_transcriber(
            model_size=config.transcription.model_size,
            device="auto",
            compute_type="auto"
        )
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = audio_path.parent
        if hasattr(args, 'output_dir') and args.output_dir:
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®è¯­è¨€
        language = config.transcription.language if config.transcription.language != 'auto' else None
        
        print(f"ğŸ”„ æ­£åœ¨è½¬å½•: {audio_path.name}")
        print(f"ğŸ“‹ æ¨¡å‹: {config.transcription.model_size}")
        print(f"ğŸŒ è¯­è¨€: {language or 'è‡ªåŠ¨æ£€æµ‹'}")
        
        # è¿›åº¦å›è°ƒ
        def transcription_progress(segment_count, segment):
            print(f"\rğŸ™ï¸ å·²å¤„ç†: {segment_count} æ®µ", end='', flush=True)
        
        # æ‰§è¡Œè½¬å½•
        result = transcriber.transcribe_audio(
            audio_path,
            language=language,
            beam_size=config.transcription.beam_size,
            temperature=config.transcription.temperature,
            progress_callback=transcription_progress
        )
        
        print(f"\nğŸ“Š è½¬å½•å®Œæˆ:")
        print(f"  ğŸŒ æ£€æµ‹è¯­è¨€: {result.language} ({result.language_probability:.2f})")
        print(f"  â±ï¸ éŸ³é¢‘æ—¶é•¿: {result.duration:.2f}ç§’")
        print(f"  ğŸ“ æ–‡æœ¬æ®µæ•°: {len(result.segments)}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        base_name = audio_path.stem
        
        # ç”Ÿæˆå­—å¹•æ–‡ä»¶
        srt_path = output_dir / f"{base_name}.srt"
        transcriber.generate_srt(result, srt_path)
        print(f"ğŸ“„ SRTå­—å¹•: {srt_path}")
        
        vtt_path = output_dir / f"{base_name}.vtt"
        transcriber.generate_vtt(result, vtt_path)
        print(f"ğŸ“„ VTTå­—å¹•: {vtt_path}")
        
        # ç”Ÿæˆçº¯æ–‡æœ¬
        txt_path = output_dir / f"{base_name}_transcript.txt"
        transcriber.generate_text(result, txt_path)
        print(f"ğŸ“„ è½¬å½•æ–‡æœ¬: {txt_path}")
        
        # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
        preview_length = 200
        if len(result.text) > preview_length:
            preview = result.text[:preview_length] + "..."
        else:
            preview = result.text
        
        print(f"\nğŸ“ åŸå§‹è½¬å½•é¢„è§ˆ:")
        print(f"ã€Œ{preview}ã€")
        
        # æ–‡æœ¬çº é”™å¤„ç†
        final_text = result.text
        if hasattr(args, 'correct') and args.correct:
            print(f"\nğŸ¤– å¼€å§‹æ–‡æœ¬çº é”™...")
            corrected_text = perform_correction(result.text, output_dir, base_name, config, result.language)
            if corrected_text:
                print("âœ… æ–‡æœ¬çº é”™å®Œæˆ")
                final_text = corrected_text
            else:
                print("âŒ æ–‡æœ¬çº é”™å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è½¬å½•æ–‡æœ¬")
        
        # æ‘˜è¦ç”Ÿæˆå¤„ç†
        if hasattr(args, 'summarize') and args.summarize:
            print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆæ‘˜è¦...")
            summarization_success = perform_summarization(final_text, output_dir, base_name, config, result.language)
            if summarization_success:
                print("âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            else:
                print("âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥")
        
        # å…³é”®è¯æå–å¤„ç†
        if hasattr(args, 'keywords') and args.keywords:
            print(f"\nğŸ” å¼€å§‹æå–å…³é”®è¯...")
            keywords_success = perform_keywords_extraction(final_text, output_dir, base_name, config, result.language)
            if keywords_success:
                print("âœ… å…³é”®è¯æå–å®Œæˆ")
            else:
                print("âŒ å…³é”®è¯æå–å¤±è´¥")
        
        # æ–‡æ¡£æ ¼å¼åŒ–å¤„ç†
        if hasattr(args, 'format') and args.format:
            print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆæ ¼å¼åŒ–æ–‡æ¡£...")
            formats = [args.format] if isinstance(args.format, str) else args.format
            source_info = f"éŸ³é¢‘æ–‡ä»¶: {audio_path.name}"
            
            formatting_success = perform_document_formatting(
                content=final_text,
                output_dir=output_dir,
                base_name=f"{base_name}_formatted",
                formats=formats,
                config=config,
                source_info=source_info
            )
            
            if formatting_success:
                print("âœ… æ–‡æ¡£æ ¼å¼åŒ–å®Œæˆ")
            else:
                print("âŒ æ–‡æ¡£æ ¼å¼åŒ–å¤±è´¥")
        
        return True
        
    except TranscriptionError as e:
        print(f"âŒ è½¬å½•å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ è½¬å½•è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def perform_correction(text: str, output_dir: Path, base_name: str, config: Config, language: str) -> bool:
    """æ‰§è¡Œæ–‡æœ¬çº é”™"""
    try:
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†APIå¯†é’¥
        if not hasattr(config, 'correction') or not config.correction.api_key:
            print("âŒ æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œè·³è¿‡æ–‡æœ¬çº é”™")
            print("ğŸ’¡ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®correction.api_key")
            return False
        
        # åˆ›å»ºçº é”™å™¨
        corrector = create_corrector_from_config(config.to_dict())
        
        # æµ‹è¯•APIè¿æ¥
        print("ğŸ” æµ‹è¯•APIè¿æ¥...")
        if not corrector.test_connection():
            print("âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥")
            return False
        
        print("âœ… APIè¿æ¥æ­£å¸¸")
        
        # ç¡®å®šè¯­è¨€
        correction_language = "zh" if language in ["zh", "chinese"] else "en"
        
        print(f"ğŸ”„ æ­£åœ¨çº é”™æ–‡æœ¬...")
        print(f"ğŸ“‹ è¯­è¨€: {correction_language}")
        print(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # æ‰§è¡Œçº é”™
        result = corrector.correct_text(text, language=correction_language)
        
        print(f"ğŸ“Š çº é”™å®Œæˆ:")
        print(f"  â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        print(f"  ğŸ¯ ä½¿ç”¨æ¨¡å‹: {result.model_used}")
        if result.tokens_used:
            print(f"  ğŸ”¢ ä½¿ç”¨Tokens: {result.tokens_used}")
        
        # ä¿å­˜çº é”™åçš„æ–‡æœ¬
        corrected_path = output_dir / f"{base_name}_corrected.txt"
        with open(corrected_path, 'w', encoding='utf-8') as f:
            f.write(result.corrected_text)
        
        print(f"ğŸ“„ çº é”™æ–‡æœ¬: {corrected_path}")
        
        # æ˜¾ç¤ºçº é”™åçš„é¢„è§ˆ
        preview_length = 200
        if len(result.corrected_text) > preview_length:
            corrected_preview = result.corrected_text[:preview_length] + "..."
        else:
            corrected_preview = result.corrected_text
        
        print(f"\nğŸ“ çº é”™åé¢„è§ˆ:")
        print(f"ã€Œ{corrected_preview}ã€")
        
        # ç”Ÿæˆå¯¹æ¯”æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        comparison_path = output_dir / f"{base_name}_comparison.txt"
        with open(comparison_path, 'w', encoding='utf-8') as f:
            f.write("=== åŸå§‹è½¬å½• ===\n\n")
            f.write(result.original_text)
            f.write("\n\n=== çº é”™åæ–‡æœ¬ ===\n\n")
            f.write(result.corrected_text)
            f.write(f"\n\n=== å¤„ç†ä¿¡æ¯ ===\n")
            f.write(f"å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’\n")
            f.write(f"ä½¿ç”¨æ¨¡å‹: {result.model_used}\n")
            if result.tokens_used:
                f.write(f"ä½¿ç”¨Tokens: {result.tokens_used}\n")
        
        print(f"ğŸ“„ å¯¹æ¯”æ–‡ä»¶: {comparison_path}")
        
        return result.corrected_text
        
    except DeepSeekAPIError as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ–‡æœ¬çº é”™è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def perform_summarization(text: str, output_dir: Path, base_name: str, config: Config, language: str) -> bool:
    """æ‰§è¡Œæ–‡æœ¬æ‘˜è¦"""
    try:
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†APIå¯†é’¥
        if not hasattr(config, 'correction') or not config.correction.api_key:
            print("âŒ æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œè·³è¿‡æ–‡æœ¬æ‘˜è¦")
            print("ğŸ’¡ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®correction.api_key")
            return False
        
        # åˆ›å»ºçº é”™å™¨
        corrector = create_corrector_from_config(config.correction)
        
        # ç¡®å®šè¯­è¨€
        summary_language = "zh" if language in ["zh", "chinese"] else "en"
        
        print(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        print(f"ğŸ“‹ è¯­è¨€: {summary_language}")
        print(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # æ‰§è¡Œæ‘˜è¦
        result = corrector.summarize_text(text, language=summary_language)
        
        print(f"ğŸ“Š æ‘˜è¦ç”Ÿæˆå®Œæˆ:")
        print(f"  â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        print(f"  ğŸ¯ ä½¿ç”¨æ¨¡å‹: {result.model_used}")
        if result.tokens_used:
            print(f"  ğŸ”¢ ä½¿ç”¨Tokens: {result.tokens_used}")
        
        # ä¿å­˜æ‘˜è¦
        summary_path = output_dir / f"{base_name}_summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(result.summary)
        
        print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {summary_path}")
        
        # æ˜¾ç¤ºæ‘˜è¦é¢„è§ˆ
        preview_length = 200
        preview = result.summary[:preview_length] + "..." if len(result.summary) > preview_length else result.summary
        print(f"\nğŸ“ æ‘˜è¦é¢„è§ˆ:")
        print(f"ã€Œ{preview}ã€")
        
        return True
        
    except DeepSeekAPIError as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        return False


def perform_keywords_extraction(text: str, output_dir: Path, base_name: str, config: Config, language: str) -> bool:
    """æ‰§è¡Œå…³é”®è¯æå–"""
    try:
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†APIå¯†é’¥
        if not hasattr(config, 'correction') or not config.correction.api_key:
            print("âŒ æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œè·³è¿‡å…³é”®è¯æå–")
            print("ğŸ’¡ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®correction.api_key")
            return False
        
        # åˆ›å»ºçº é”™å™¨
        corrector = create_corrector_from_config(config.correction)
        
        # ç¡®å®šè¯­è¨€
        keywords_language = "zh" if language in ["zh", "chinese"] else "en"
        
        print(f"ğŸ”„ æ­£åœ¨æå–å…³é”®è¯...")
        print(f"ğŸ“‹ è¯­è¨€: {keywords_language}")
        print(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # æ‰§è¡Œå…³é”®è¯æå–
        result = corrector.extract_keywords(text, language=keywords_language)
        
        print(f"ğŸ“Š å…³é”®è¯æå–å®Œæˆ:")
        print(f"  â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        print(f"  ğŸ¯ ä½¿ç”¨æ¨¡å‹: {result.model_used}")
        if result.tokens_used:
            print(f"  ğŸ”¢ ä½¿ç”¨Tokens: {result.tokens_used}")
        print(f"  ğŸ” æå–åˆ° {len(result.keywords)} ä¸ªå…³é”®è¯")
        
        # ä¿å­˜å…³é”®è¯
        keywords_path = output_dir / f"{base_name}_keywords.txt"
        with open(keywords_path, 'w', encoding='utf-8') as f:
            f.write(', '.join(result.keywords))
        
        print(f"ğŸ“„ å…³é”®è¯æ–‡ä»¶: {keywords_path}")
        
        # æ˜¾ç¤ºå…³é”®è¯åˆ—è¡¨
        print(f"\nğŸ” å…³é”®è¯åˆ—è¡¨:")
        for i, keyword in enumerate(result.keywords, 1):
            print(f"  {i}. {keyword}")
        
        return True
        
    except DeepSeekAPIError as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def perform_document_formatting(
    content: str, 
    output_dir: Path, 
    base_name: str, 
    formats: list, 
    config: Config,
    source_info: str = ""
) -> bool:
    """æ‰§è¡Œæ–‡æ¡£æ ¼å¼åŒ–"""
    try:
        # åˆ›å»ºæ–‡æ¡£æ ¼å¼åŒ–å™¨
        formatter = create_formatter()
        
        # åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®
        metadata = DocumentMetadata(
            title=f"{base_name}æ–‡æ¡£",
            author=getattr(config.output, 'author', ''),
            source=source_info,
            language='zh'
        )
        
        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆæ–‡æ¡£æ ¼å¼...")
        
        # æ‰¹é‡ç”Ÿæˆå¤šç§æ ¼å¼
        results = formatter.batch_format(
            content=content,
            output_dir=output_dir,
            base_name=base_name,
            formats=formats,
            metadata=metadata
        )
        
        if results:
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(results)} ç§æ ¼å¼çš„æ–‡æ¡£:")
            for result in results:
                print(f"  ğŸ“„ {result.format.upper()}: {result.file_path} ({result.size_bytes} å­—èŠ‚)")
            return True
        else:
            print("âŒ æœªç”Ÿæˆä»»ä½•æ–‡æ¡£")
            return False
            
    except DocumentFormatterError as e:
        print(f"âŒ æ–‡æ¡£æ ¼å¼åŒ–å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ–‡æ¡£æ ¼å¼åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def cmd_batch(args):
    """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
    print(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¤„ç†: {args.input_file}")
    
    try:
        # è¯»å–URLåˆ—è¡¨
        if not os.path.exists(args.input_file):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return False
        
        with open(args.input_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        if not urls:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„URL")
            return False
        
        print(f"ğŸ“‹ å‘ç° {len(urls)} ä¸ªURL")
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        if args.output_dir:
            config.download.output_dir = args.output_dir
        if args.cookie_browser:
            config.download.cookies.from_browser = args.cookie_browser
        if args.cookie_file:
            config.download.cookies.cookie_file = args.cookie_file
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(config.logging.level, config.logging.file)
        
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = create_downloader()
        downloader.config = config
        
        # ç¡®å®šå¹¶è¡Œå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°
        max_workers = args.max_workers
        use_parallel = not args.sequential
        progress_callback = None if args.no_progress else batch_progress_callback
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½... (å¹¶è¡Œ: {use_parallel}, å·¥ä½œçº¿ç¨‹: {max_workers})")
        
        # æ‰¹é‡ä¸‹è½½
        if use_parallel:
            results = downloader.download_batch(urls, progress_callback, max_workers)
        else:
            results = downloader.download_batch_sequential(urls, progress_callback)
        
        print()  # æ¢è¡Œ
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for result in results if result.success)
        print(f"\nğŸ“Š æ‰¹é‡ä¸‹è½½å®Œæˆ:")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(urls)}")
        print(f"âŒ å¤±è´¥: {len(urls) - success_count}/{len(urls)}")
        
        # æ˜¾ç¤ºä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        if success_count > 0:
            total_size = sum(result.file_size or 0 for result in results if result.success and result.file_size)
            total_duration = sum(result.duration or 0 for result in results if result.success and result.duration)
            total_time = sum(result.download_time or 0 for result in results if result.download_time)
            
            print(f"ğŸ“ˆ ä¸‹è½½ç»Ÿè®¡:")
            print(f"  æ€»æ–‡ä»¶å¤§å°: {total_size / (1024*1024):.1f} MB")
            print(f"  æ€»éŸ³é¢‘æ—¶é•¿: {total_duration / 60:.1f} åˆ†é’Ÿ")
            print(f"  æ€»ä¸‹è½½æ—¶é—´: {total_time / 60:.1f} åˆ†é’Ÿ")
            if total_time > 0:
                print(f"  å¹³å‡ä¸‹è½½é€Ÿåº¦: {(total_size / (1024*1024)) / (total_time / 60):.1f} MB/åˆ†é’Ÿ")
        
        # æ‰¹é‡è½¬å½•å¤„ç†
        if args.transcribe and success_count > 0:
            print(f"\nğŸ™ï¸ å¼€å§‹æ‰¹é‡è½¬å½•...")
            transcribe_success_count = 0
            
            for result in results:
                if result.success and result.file_path:
                    print(f"\nğŸ”„ è½¬å½•: {Path(result.file_path).name}")
                    if perform_transcription(result.file_path, config, args):
                        transcribe_success_count += 1
                    else:
                        print(f"âŒ è½¬å½•å¤±è´¥: {Path(result.file_path).name}")
            
            print(f"\nğŸ“Š æ‰¹é‡è½¬å½•å®Œæˆ:")
            print(f"âœ… è½¬å½•æˆåŠŸ: {transcribe_success_count}/{success_count}")
            print(f"âŒ è½¬å½•å¤±è´¥: {success_count - transcribe_success_count}/{success_count}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„URL
        failed_results = [result for result in results if not result.success]
        if failed_results:
            print("\nâŒ å¤±è´¥çš„URL:")
            for result in failed_results:
                print(f"  {result.url}: {result.message}")
                if "cookie" in result.message.lower() or "éªŒè¯" in result.message.lower():
                    print("    ğŸ’¡ å¯èƒ½éœ€è¦cookieéªŒè¯")
        
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        return False


def cmd_config(args):
    """é…ç½®ç®¡ç†"""
    if args.show:
        # æ˜¾ç¤ºå½“å‰é…ç½®
        try:
            config = load_config(args.config_file)
            print("ğŸ“‹ å½“å‰é…ç½®:")
            print("=" * 50)
            
            # éšè—æ•æ„Ÿä¿¡æ¯
            config_dict = config.to_dict()
            if config_dict.get('deepseek', {}).get('api_key'):
                config_dict['deepseek']['api_key'] = '***å·²è®¾ç½®***'
            
            # æ˜¾ç¤ºcookieé…ç½®
            print("\nğŸª Cookieé…ç½®:")
            cookies = config_dict.get('download', {}).get('cookies', {})
            print(f"  æµè§ˆå™¨: {cookies.get('from_browser', 'None')}")
            print(f"  Cookieæ–‡ä»¶: {cookies.get('cookie_file', 'None')}")
            print(f"  è‡ªåŠ¨éªŒè¯: {cookies.get('auto_captcha', False)}")
            
            print(f"\nğŸ“¥ ä¸‹è½½é…ç½®:")
            download = config_dict.get('download', {})
            print(f"  è¾“å‡ºç›®å½•: {download.get('output_dir', './temp')}")
            print(f"  éŸ³é¢‘è´¨é‡: {download.get('audio_quality', 'best')}")
            print(f"  æ”¯æŒæ ¼å¼: {', '.join(download.get('supported_formats', []))}")
            
            # ç½‘ç»œé…ç½®
            network = download.get('network', {})
            print(f"  è¶…æ—¶æ—¶é—´: {network.get('timeout', 30)}ç§’")
            print(f"  é‡è¯•æ¬¡æ•°: {network.get('retries', 3)}")
            
            print(f"\nğŸ”„ è½¬å½•é…ç½®:")
            transcription = config_dict.get('transcription', {})
            print(f"  æ¨¡å‹å¤§å°: {transcription.get('model_size', 'base')}")
            print(f"  è¯­è¨€: {transcription.get('language', 'auto')}")
            
            print(f"\nğŸ¤– AIé…ç½®:")
            deepseek = config_dict.get('deepseek', {})
            print(f"  APIå¯†é’¥: {deepseek.get('api_key', 'æœªè®¾ç½®')}")
            print(f"  æ¨¡å‹: {deepseek.get('model', 'deepseek-chat')}")
            
            # éªŒè¯é…ç½®
            is_valid, msg = config.validate()
            if is_valid:
                print(f"\nâœ… é…ç½®éªŒè¯: {msg}")
            else:
                print(f"\nâŒ é…ç½®é—®é¢˜: {msg}")
            
        except Exception as e:
            print(f"âŒ æ˜¾ç¤ºé…ç½®å¤±è´¥: {e}")
            return False
    
    elif args.init:
        # åˆå§‹åŒ–é…ç½®æ–‡ä»¶
        config_path = Path(args.config_file or "./config/config.yaml")
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹é…ç½®
        example_config = {
            'deepseek': {
                'api_key': 'your_deepseek_api_key_here',
                'base_url': 'https://api.deepseek.com',
                'model': 'deepseek-chat'
            },
            'download': {
                'output_dir': './temp',
                'audio_quality': 'best',
                'supported_formats': ['mp3', 'wav', 'm4a'],
                'cookies': {
                    'from_browser': 'chrome',
                    'cookie_file': None,
                    'auto_captcha': True
                },
                'network': {
                    'timeout': 30,
                    'retries': 3,
                    'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
            },
            'transcription': {
                'model_size': 'base',
                'language': 'auto',
                'temperature': 0.0,
                'beam_size': 5
            },
            'output': {
                'default_format': 'markdown',
                'include_timestamps': True,
                'include_summary': True,
                'include_keywords': True
            },
            'logging': {
                'level': 'INFO',
                'file': './logs/video_draft_creator.log'
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(example_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
        print("ğŸ’¡ è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶å¹¶è®¾ç½®æ‚¨çš„APIå¯†é’¥")
    
    elif args.test:
        # æµ‹è¯•é…ç½®
        try:
            config = load_config(args.config_file)
            is_valid, msg = config.validate()
            
            if is_valid:
                print(f"âœ… é…ç½®æµ‹è¯•é€šè¿‡: {msg}")
                
                # æµ‹è¯•cookieé…ç½®
                print("ğŸª æµ‹è¯•Cookieé…ç½®...")
                cookies = config.download.cookies
                if cookies.from_browser:
                    print(f"  âœ… æµè§ˆå™¨cookie: {cookies.from_browser}")
                elif cookies.cookie_file:
                    if os.path.exists(cookies.cookie_file):
                        print(f"  âœ… Cookieæ–‡ä»¶: {cookies.cookie_file}")
                    else:
                        print(f"  âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookies.cookie_file}")
                else:
                    print("  âš ï¸ æœªé…ç½®cookieï¼Œå¯èƒ½å½±å“æŸäº›å¹³å°çš„ä¸‹è½½")
                
                return True
            else:
                print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {msg}")
                return False
                
        except Exception as e:
            print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    return True


def cmd_correct(args):
    """çº é”™æ–‡æœ¬æ–‡ä»¶"""
    print(f"ğŸ¤– å¼€å§‹çº é”™æ–‡æœ¬æ–‡ä»¶: {args.text_file}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        text_path = Path(args.text_file)
        if not text_path.exists():
            print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {args.text_file}")
            return False
        
        # è¯»å–æ–‡æœ¬å†…å®¹
        with open(text_path, 'r', encoding='utf-8') as f:
            text_content = f.read().strip()
        
        if not text_content:
            print("âŒ æ–‡æœ¬æ–‡ä»¶ä¸ºç©º")
            return False
        
        print(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = text_path.parent
        if args.output_dir:
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¡®å®šè¯­è¨€
        if args.language == 'auto':
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            chinese_chars = len([c for c in text_content if '\u4e00' <= c <= '\u9fff'])
            total_chars = len([c for c in text_content if c.isalpha()])
            if total_chars > 0 and chinese_chars / total_chars > 0.3:
                language = 'zh'
            else:
                language = 'en'
            print(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹è¯­è¨€: {language}")
        else:
            language = args.language
        
        # æ‰§è¡Œçº é”™
        base_name = text_path.stem
        success = perform_correction(text_content, output_dir, base_name, config, language)
        
        return success
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬çº é”™å¤±è´¥: {e}")
        return False


def cmd_format(args):
    """æ ¼å¼åŒ–æ–‡æœ¬æ–‡ä»¶ä¸ºæ–‡æ¡£"""
    print(f"ğŸ“ å¼€å§‹æ ¼å¼åŒ–æ–‡æœ¬æ–‡ä»¶: {args.text_file}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        text_path = Path(args.text_file)
        if not text_path.exists():
            print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {args.text_file}")
            return False
        
        # è¯»å–æ–‡æœ¬å†…å®¹
        with open(text_path, 'r', encoding='utf-8') as f:
            text_content = f.read().strip()
        
        if not text_content:
            print("âŒ æ–‡æœ¬æ–‡ä»¶ä¸ºç©º")
            return False
        
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir) if args.output_dir else text_path.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ–‡æ¡£æ ¼å¼åŒ–å™¨
        formatter = create_formatter()
        
        # åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®
        metadata = DocumentMetadata(
            title=args.title or f"{text_path.stem}æ–‡æ¡£",
            author=args.author or getattr(config.output, 'author', ''),
            source=args.source or f"æ–‡æœ¬æ–‡ä»¶: {text_path.name}",
            language='zh'
        )
        
        print(f"ğŸ“‹ æ–‡æ¡£ä¿¡æ¯:")
        print(f"  æ ‡é¢˜: {metadata.title}")
        print(f"  ä½œè€…: {metadata.author}")
        print(f"  æ¥æº: {metadata.source}")
        
        # æ‰¹é‡ç”Ÿæˆå¤šç§æ ¼å¼
        base_name = text_path.stem
        results = formatter.batch_format(
            content=text_content,
            output_dir=output_dir,
            base_name=base_name,
            formats=args.formats,
            metadata=metadata
        )
        
        if results:
            print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(results)} ç§æ ¼å¼çš„æ–‡æ¡£:")
            for result in results:
                print(f"  ğŸ“„ {result.format.upper()}: {result.file_path} ({result.size_bytes} å­—èŠ‚)")
            
            # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
            preview_length = 200
            if len(text_content) > preview_length:
                preview = text_content[:preview_length] + "..."
            else:
                preview = text_content
            
            print(f"\nğŸ“ å†…å®¹é¢„è§ˆ:")
            print(f"ã€Œ{preview}ã€")
            
            return True
        else:
            print("âŒ æœªç”Ÿæˆä»»ä½•æ–‡æ¡£")
            return False
            
    except DocumentFormatterError as e:
        print(f"âŒ æ–‡æ¡£æ ¼å¼åŒ–å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ ¼å¼åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def cmd_summarize(args):
    """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
    print(f"ğŸ“ å¼€å§‹ç”Ÿæˆæ‘˜è¦: {args.text_file}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶
        text_file = Path(args.text_file)
        if not text_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.text_file}")
            return False
        
        # è¯»å–æ–‡æœ¬
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read().strip()
        
        if not text:
            print("âŒ æ–‡ä»¶å†…å®¹ä¸ºç©º")
            return False
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(config.logging.level, config.logging.file)
        
        # æ£€æµ‹è¯­è¨€
        language = args.language
        if language == 'auto':
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            total_chars = len(text)
            language = 'zh' if chinese_chars / total_chars > 0.3 else 'en'
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {language}")
        
        # åˆ›å»ºçº é”™å™¨
        corrector = create_corrector_from_config(config.correction)
        
        # ç”Ÿæˆæ‘˜è¦
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        result = corrector.summarize_text(text, language)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir) if args.output_dir else text_file.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ‘˜è¦
        base_name = text_file.stem
        summary_file = output_dir / f"{base_name}_summary.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(result.summary)
        
        print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ")
        print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {summary_file}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        if result.tokens_used:
            print(f"ğŸ¯ ä½¿ç”¨tokens: {result.tokens_used}")
        
        # æ˜¾ç¤ºæ‘˜è¦é¢„è§ˆ
        preview_length = 200
        preview = result.summary[:preview_length] + "..." if len(result.summary) > preview_length else result.summary
        print(f"\nğŸ“ æ‘˜è¦é¢„è§ˆ:")
        print(f"ã€Œ{preview}ã€")
        
        return True
        
    except DeepSeekAPIError as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
        return False


def cmd_keywords(args):
    """æå–å…³é”®è¯"""
    print(f"ğŸ” å¼€å§‹æå–å…³é”®è¯: {args.text_file}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶
        text_file = Path(args.text_file)
        if not text_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.text_file}")
            return False
        
        # è¯»å–æ–‡æœ¬
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read().strip()
        
        if not text:
            print("âŒ æ–‡ä»¶å†…å®¹ä¸ºç©º")
            return False
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(config.logging.level, config.logging.file)
        
        # æ£€æµ‹è¯­è¨€
        language = args.language
        if language == 'auto':
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            total_chars = len(text)
            language = 'zh' if chinese_chars / total_chars > 0.3 else 'en'
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {language}")
        
        # åˆ›å»ºçº é”™å™¨
        corrector = create_corrector_from_config(config.correction)
        
        # æå–å…³é”®è¯
        print("ğŸ”„ æ­£åœ¨æå–å…³é”®è¯...")
        result = corrector.extract_keywords(text, language)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir) if args.output_dir else text_file.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å…³é”®è¯
        base_name = text_file.stem
        keywords_file = output_dir / f"{base_name}_keywords.txt"
        
        with open(keywords_file, 'w', encoding='utf-8') as f:
            f.write(', '.join(result.keywords))
        
        print(f"âœ… å…³é”®è¯æå–å®Œæˆ")
        print(f"ğŸ“„ å…³é”®è¯æ–‡ä»¶: {keywords_file}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        if result.tokens_used:
            print(f"ğŸ¯ ä½¿ç”¨tokens: {result.tokens_used}")
        print(f"ğŸ”¢ æå–åˆ° {len(result.keywords)} ä¸ªå…³é”®è¯")
        
        # æ˜¾ç¤ºå…³é”®è¯
        print(f"\nğŸ” å…³é”®è¯åˆ—è¡¨:")
        for i, keyword in enumerate(result.keywords, 1):
            print(f"  {i}. {keyword}")
        
        return True
        
    except DeepSeekAPIError as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
        return False


def cmd_analyze(args):
    """ç»¼åˆæ–‡æœ¬åˆ†æ"""
    print(f"ğŸ”¬ å¼€å§‹ç»¼åˆæ–‡æœ¬åˆ†æ: {args.text_file}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶
        text_file = Path(args.text_file)
        if not text_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.text_file}")
            return False
        
        # è¯»å–æ–‡æœ¬
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read().strip()
        
        if not text:
            print("âŒ æ–‡ä»¶å†…å®¹ä¸ºç©º")
            return False
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(config.logging.level, config.logging.file)
        
        # æ£€æµ‹è¯­è¨€
        language = args.language
        if language == 'auto':
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            total_chars = len(text)
            language = 'zh' if chinese_chars / total_chars > 0.3 else 'en'
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {language}")
        
        # åˆ›å»ºçº é”™å™¨
        corrector = create_corrector_from_config(config.correction)
        
        # æ‰§è¡Œç»¼åˆåˆ†æ
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œç»¼åˆåˆ†æ...")
        result = corrector.analyze_text(
            text, 
            language,
            include_correction=not args.no_correction,
            include_summary=not args.no_summary,
            include_keywords=not args.no_keywords
        )
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir) if args.output_dir else text_file.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        base_name = text_file.stem
        
        # ä¿å­˜ç»“æœ
        if result.corrected_text:
            corrected_file = output_dir / f"{base_name}_corrected.txt"
            with open(corrected_file, 'w', encoding='utf-8') as f:
                f.write(result.corrected_text)
            print(f"ğŸ“„ çº é”™æ–‡æœ¬: {corrected_file}")
        
        if result.summary:
            summary_file = output_dir / f"{base_name}_summary.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(result.summary)
            print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {summary_file}")
        
        if result.keywords:
            keywords_file = output_dir / f"{base_name}_keywords.txt"
            with open(keywords_file, 'w', encoding='utf-8') as f:
                f.write(', '.join(result.keywords))
            print(f"ğŸ“„ å…³é”®è¯æ–‡ä»¶: {keywords_file}")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_file = output_dir / f"{base_name}_analysis_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 50 + "\n")
            f.write("æ–‡æœ¬åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"åŸæ–‡ä»¶: {text_file.name}\n")
            f.write(f"åˆ†ææ—¶é—´: {result.total_processing_time:.2f}ç§’\n")
            if result.total_tokens_used:
                f.write(f"æ€»tokens: {result.total_tokens_used}\n")
            f.write(f"è¯­è¨€: {language}\n\n")
            
            if result.corrected_text:
                f.write("çº é”™æ–‡æœ¬:\n")
                f.write("-" * 20 + "\n")
                f.write(result.corrected_text + "\n\n")
            
            if result.summary:
                f.write("æ–‡æœ¬æ‘˜è¦:\n")
                f.write("-" * 20 + "\n")
                f.write(result.summary + "\n\n")
            
            if result.keywords:
                f.write("å…³é”®è¯:\n")
                f.write("-" * 20 + "\n")
                f.write(', '.join(result.keywords) + "\n\n")
        
        print(f"âœ… ç»¼åˆåˆ†æå®Œæˆ")
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_file}")
        print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {result.total_processing_time:.2f}ç§’")
        if result.total_tokens_used:
            print(f"ğŸ¯ æ€»tokens: {result.total_tokens_used}")
        
        # æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯
        if result.summary:
            preview_length = 150
            preview = result.summary[:preview_length] + "..." if len(result.summary) > preview_length else result.summary
            print(f"\nğŸ“ æ‘˜è¦é¢„è§ˆ:")
            print(f"ã€Œ{preview}ã€")
        
        if result.keywords:
            print(f"\nğŸ” å…³é”®è¯ ({len(result.keywords)}ä¸ª):")
            print(f"ã€Œ{', '.join(result.keywords[:10])}{'...' if len(result.keywords) > 10 else ''}ã€")
        
        return True
        
    except DeepSeekAPIError as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
        return False


def cmd_test(args):
    """æµ‹è¯•åŠŸèƒ½"""
    print("ğŸ§ª è¿è¡Œæµ‹è¯•...")
    
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ",  # ç»å…¸æµ‹è¯•è§†é¢‘
        "https://www.bilibili.com/video/BV1GJ411x7h7"   # Bç«™æµ‹è¯•è§†é¢‘
    ]
    
    try:
        config = load_config(args.config)
        if args.cookie_browser:
            config.download.cookies.from_browser = args.cookie_browser
        if args.cookie_file:
            config.download.cookies.cookie_file = args.cookie_file
        
        downloader = create_downloader()
        downloader.config = config
        
        print("ğŸ” æµ‹è¯•URLæ”¯æŒæ£€æŸ¥...")
        for url in test_urls:
            supported, platform = downloader.check_url_support(url)
            status = "âœ…" if supported else "âŒ"
            print(f"  {status} {url} - {platform}")
        
        print("\nğŸ“‹ æµ‹è¯•è§†é¢‘ä¿¡æ¯è·å–...")
        test_url = test_urls[0]  # YouTubeæµ‹è¯•
        info = downloader.get_video_info(test_url)
        
        if info:
            print(f"  âœ… æˆåŠŸè·å–ä¿¡æ¯:")
            print(f"    æ ‡é¢˜: {info['title'][:50]}...")
            print(f"    å¹³å°: {info['platform']}")
            print(f"    æ—¶é•¿: {info['duration']}ç§’")
        else:
            print(f"  âŒ æ— æ³•è·å–è§†é¢‘ä¿¡æ¯")
            print(f"  ğŸ’¡ å¯èƒ½éœ€è¦cookieéªŒè¯")
        
        print("\nâœ… æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def cmd_transcribe(args):
    """è½¬å½•éŸ³é¢‘æ–‡ä»¶"""
    print(f"ğŸ™ï¸ å¼€å§‹è½¬å½•: {args.audio_file}")
    
    try:
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
        audio_path = Path(args.audio_file)
        if not audio_path.exists():
            print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.audio_file}")
            return False
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # ä»å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
        if args.model_size:
            config.transcription.model_size = args.model_size
        if args.language:
            config.transcription.language = args.language
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if args.output_dir:
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        else:
            output_dir = audio_path.parent
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(config.logging.level, config.logging.file)
        
        # åˆ›å»ºè½¬å½•å™¨
        transcriber = create_transcriber(
            model_size=config.transcription.model_size,
            device="auto",
            compute_type="auto"
        )
        
        # è®¾ç½®è¯­è¨€
        language = config.transcription.language if config.transcription.language != 'auto' else None
        
        print(f"ğŸ”„ æ­£åœ¨è½¬å½•: {audio_path.name}")
        print(f"ğŸ“‹ æ¨¡å‹: {config.transcription.model_size}")
        print(f"ğŸŒ è¯­è¨€: {language or 'è‡ªåŠ¨æ£€æµ‹'}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # è¿›åº¦å›è°ƒ
        def transcription_progress(segment_count, segment):
            print(f"\rğŸ™ï¸ å·²å¤„ç†: {segment_count} æ®µ", end='', flush=True)
        
        # æ‰§è¡Œè½¬å½•
        result = transcriber.transcribe_audio(
            audio_path,
            language=language,
            beam_size=config.transcription.beam_size,
            temperature=config.transcription.temperature,
            progress_callback=transcription_progress
        )
        
        print(f"\nğŸ“Š è½¬å½•å®Œæˆ:")
        print(f"  ğŸŒ æ£€æµ‹è¯­è¨€: {result.language} ({result.language_probability:.2f})")
        print(f"  â±ï¸ éŸ³é¢‘æ—¶é•¿: {result.duration:.2f}ç§’")
        print(f"  ğŸ“ æ–‡æœ¬æ®µæ•°: {len(result.segments)}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        base_name = audio_path.stem
        output_format = args.format
        
        if output_format in ['srt', 'all']:
            srt_path = output_dir / f"{base_name}.srt"
            transcriber.generate_srt(result, srt_path)
            print(f"ğŸ“„ SRTå­—å¹•: {srt_path}")
        
        if output_format in ['vtt', 'all']:
            vtt_path = output_dir / f"{base_name}.vtt"
            transcriber.generate_vtt(result, vtt_path)
            print(f"ğŸ“„ VTTå­—å¹•: {vtt_path}")
        
        if output_format in ['txt', 'all']:
            txt_path = output_dir / f"{base_name}_transcript.txt"
            transcriber.generate_text(result, txt_path)
            print(f"ğŸ“„ è½¬å½•æ–‡æœ¬: {txt_path}")
        
        # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
        preview_length = 200
        if len(result.text) > preview_length:
            preview = result.text[:preview_length] + "..."
        else:
            preview = result.text
        
        print(f"\nğŸ“ æ–‡æœ¬é¢„è§ˆ:")
        print(f"ã€Œ{preview}ã€")
        
        return True
        
    except TranscriptionError as e:
        print(f"âŒ è½¬å½•å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ è½¬å½•è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="è§†é¢‘è‰ç¨¿åˆ›å»ºå·¥å…· - ä¸‹è½½ã€è½¬å½•ã€AIå¤„ç†è§†é¢‘å†…å®¹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ä¸‹è½½
  %(prog)s process https://www.youtube.com/watch?v=VIDEO_ID

  # ä½¿ç”¨Chromeæµè§ˆå™¨cookieä¸‹è½½
  %(prog)s process https://www.youtube.com/watch?v=VIDEO_ID --cookie-browser chrome

  # ä½¿ç”¨cookieæ–‡ä»¶ä¸‹è½½
  %(prog)s process https://www.youtube.com/watch?v=VIDEO_ID --cookie-file cookies.txt

  # æ‰¹é‡ä¸‹è½½
  %(prog)s batch urls.txt --cookie-browser firefox

  # è½¬å½•éŸ³é¢‘æ–‡ä»¶
  %(prog)s transcribe audio.mp3 --model-size base --language zh

  # æŸ¥çœ‹é…ç½®
  %(prog)s config --show

  # åˆå§‹åŒ–é…ç½®æ–‡ä»¶
  %(prog)s config --init
        """
    )
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--config', '-c', 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./config/config.yaml)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    
    # Cookieç›¸å…³å…¨å±€å‚æ•°
    parser.add_argument('--cookie-browser', 
                       choices=['chrome', 'firefox', 'safari', 'edge', 'opera', 'brave'],
                       help='ä»æµè§ˆå™¨å¯¼å…¥cookie')
    parser.add_argument('--cookie-file',
                       help='Cookieæ–‡ä»¶è·¯å¾„')
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # process å‘½ä»¤
    parser_process = subparsers.add_parser('process', help='å¤„ç†å•ä¸ªè§†é¢‘URL')
    parser_process.add_argument('url', help='è§†é¢‘URL')
    parser_process.add_argument('--output-dir', '-o', 
                              help='è¾“å‡ºç›®å½• (è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)')
    parser_process.add_argument('--output-name', '-n',
                              help='è¾“å‡ºæ–‡ä»¶å(ä¸å«æ‰©å±•å)')
    parser_process.add_argument('--audio-quality', '-q',
                              choices=['best', 'worst', '128', '192', '256', '320'],
                              help='éŸ³é¢‘è´¨é‡')
    parser_process.add_argument('--transcribe', '-t', action='store_true',
                              help='ä¸‹è½½åè¿›è¡Œè½¬å½•')
    parser_process.add_argument('--correct', action='store_true',
                              help='è½¬å½•åè¿›è¡ŒAIæ–‡æœ¬çº é”™ (éœ€è¦--transcribe)')
    parser_process.add_argument('--summarize', action='store_true',
                              help='ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ (éœ€è¦--transcribe)')
    parser_process.add_argument('--keywords', action='store_true',
                              help='æå–å…³é”®è¯ (éœ€è¦--transcribe)')
    parser_process.add_argument('--info-only', action='store_true',
                              help='ä»…è·å–è§†é¢‘ä¿¡æ¯ï¼Œä¸ä¸‹è½½')
    parser_process.add_argument('--format', '-f',
                              choices=['markdown', 'txt', 'docx'],
                              help='è¾“å‡ºæ–‡æ¡£æ ¼å¼')
    parser_process.add_argument('--profile', '-p',
                              help='ä½¿ç”¨æŒ‡å®šçš„é…ç½®é¢„è®¾')
    parser_process.add_argument('--verbose', '-v', action='store_true',
                              help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    # batch å‘½ä»¤
    parser_batch = subparsers.add_parser('batch', help='æ‰¹é‡å¤„ç†å¤šä¸ªURL')
    parser_batch.add_argument('input_file', help='åŒ…å«URLçš„æ–‡æœ¬æ–‡ä»¶')
    parser_batch.add_argument('--output-dir', '-o',
                            help='è¾“å‡ºç›®å½•')
    parser_batch.add_argument('--transcribe', '-t', action='store_true',
                            help='ä¸‹è½½åè¿›è¡Œè½¬å½•')
    parser_batch.add_argument('--correct', action='store_true',
                            help='è½¬å½•åè¿›è¡ŒAIæ–‡æœ¬çº é”™ (éœ€è¦--transcribe)')
    parser_batch.add_argument('--summarize', action='store_true',
                            help='ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ (éœ€è¦--transcribe)')
    parser_batch.add_argument('--keywords', action='store_true',
                            help='æå–å…³é”®è¯ (éœ€è¦--transcribe)')
    parser_batch.add_argument('--format', '-f',
                            choices=['markdown', 'txt', 'docx'],
                            help='è¾“å‡ºæ–‡æ¡£æ ¼å¼')
    parser_batch.add_argument('--max-workers', '-w', type=int, default=3,
                            help='æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 3)')
    parser_batch.add_argument('--sequential', action='store_true',
                            help='ä½¿ç”¨é¡ºåºå¤„ç†è€Œéå¹¶è¡Œå¤„ç†')
    parser_batch.add_argument('--no-progress', action='store_true',
                            help='ç¦ç”¨è¿›åº¦æ˜¾ç¤º')
    parser_batch.add_argument('--profile', '-p',
                            help='ä½¿ç”¨æŒ‡å®šçš„é…ç½®é¢„è®¾')
    parser_batch.add_argument('--verbose', '-v', action='store_true',
                            help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    # config å‘½ä»¤
    parser_config = subparsers.add_parser('config', help='é…ç½®ç®¡ç†')
    config_group = parser_config.add_mutually_exclusive_group(required=True)
    config_group.add_argument('--show', action='store_true',
                            help='æ˜¾ç¤ºå½“å‰é…ç½®')
    config_group.add_argument('--init', action='store_true',
                            help='åˆå§‹åŒ–é…ç½®æ–‡ä»¶')
    config_group.add_argument('--test', action='store_true',
                            help='æµ‹è¯•é…ç½®')
    config_group.add_argument('--list-profiles', action='store_true',
                            help='åˆ—å‡ºæ‰€æœ‰é…ç½®é¢„è®¾')
    config_group.add_argument('--save-profile',
                            help='ä¿å­˜å½“å‰é…ç½®ä¸ºé¢„è®¾')
    config_group.add_argument('--show-profile',
                            help='æ˜¾ç¤ºæŒ‡å®šé…ç½®é¢„è®¾çš„è¯¦æƒ…')
    config_group.add_argument('--delete-profile',
                            help='åˆ é™¤æŒ‡å®šçš„é…ç½®é¢„è®¾')
    parser_config.add_argument('--config-file',
                             help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser_config.add_argument('--description',
                             help='é…ç½®é¢„è®¾çš„æè¿°ä¿¡æ¯ (ä¸--save-profileä¸€èµ·ä½¿ç”¨)')
    
    # transcribe å‘½ä»¤
    parser_transcribe = subparsers.add_parser('transcribe', help='è½¬å½•éŸ³é¢‘æ–‡ä»¶')
    parser_transcribe.add_argument('audio_file', help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser_transcribe.add_argument('--output-dir', '-o',
                                 help='è¾“å‡ºç›®å½•')
    parser_transcribe.add_argument('--model-size', '-m',
                                 choices=['tiny', 'base', 'small', 'base', 'large', 'large-v2', 'large-v3'],
                                 help='è½¬å½•æ¨¡å‹å¤§å°')
    parser_transcribe.add_argument('--language', '-l',
                                 help='éŸ³é¢‘è¯­è¨€ (å¦‚: zh, en, auto)')
    parser_transcribe.add_argument('--format', '-f',
                                 choices=['srt', 'vtt', 'txt', 'all'],
                                 default='all',
                                 help='è¾“å‡ºæ ¼å¼')
    
    # correct å‘½ä»¤
    parser_correct = subparsers.add_parser('correct', help='çº é”™æ–‡æœ¬æ–‡ä»¶')
    parser_correct.add_argument('text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser_correct.add_argument('--output-dir', '-o',
                               help='è¾“å‡ºç›®å½•')
    parser_correct.add_argument('--language', '-l',
                               choices=['zh', 'en', 'auto'],
                               default='auto',
                               help='æ–‡æœ¬è¯­è¨€')
    
    # format å‘½ä»¤
    parser_format = subparsers.add_parser('format', help='æ ¼å¼åŒ–æ–‡æœ¬æ–‡ä»¶ä¸ºæ–‡æ¡£')
    parser_format.add_argument('text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser_format.add_argument('--output-dir', '-o',
                              help='è¾“å‡ºç›®å½•')
    parser_format.add_argument('--formats', '-f',
                              nargs='+',
                              choices=['markdown', 'txt', 'docx'],
                              default=['markdown'],
                              help='è¾“å‡ºæ ¼å¼ (å¯å¤šé€‰)')
    parser_format.add_argument('--title', '-t',
                              help='æ–‡æ¡£æ ‡é¢˜')
    parser_format.add_argument('--author', '-a',
                              help='æ–‡æ¡£ä½œè€…')
    parser_format.add_argument('--source', '-s',
                              help='æ–‡æ¡£æ¥æºä¿¡æ¯')
    
    # summarize å‘½ä»¤
    parser_summarize = subparsers.add_parser('summarize', help='ç”Ÿæˆæ–‡æœ¬æ‘˜è¦')
    parser_summarize.add_argument('text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser_summarize.add_argument('--output-dir', '-o',
                                 help='è¾“å‡ºç›®å½•')
    parser_summarize.add_argument('--language', '-l',
                                 choices=['zh', 'en', 'auto'],
                                 default='auto',
                                 help='æ–‡æœ¬è¯­è¨€')
    
    # keywords å‘½ä»¤
    parser_keywords = subparsers.add_parser('keywords', help='æå–å…³é”®è¯')
    parser_keywords.add_argument('text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser_keywords.add_argument('--output-dir', '-o',
                                help='è¾“å‡ºç›®å½•')
    parser_keywords.add_argument('--language', '-l',
                                choices=['zh', 'en', 'auto'],
                                default='auto',
                                help='æ–‡æœ¬è¯­è¨€')
    
    # analyze å‘½ä»¤ (ç»¼åˆåˆ†æ)
    parser_analyze = subparsers.add_parser('analyze', help='ç»¼åˆæ–‡æœ¬åˆ†æ(çº é”™+æ‘˜è¦+å…³é”®è¯)')
    parser_analyze.add_argument('text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser_analyze.add_argument('--output-dir', '-o',
                               help='è¾“å‡ºç›®å½•')
    parser_analyze.add_argument('--language', '-l',
                               choices=['zh', 'en', 'auto'],
                               default='auto',
                               help='æ–‡æœ¬è¯­è¨€')
    parser_analyze.add_argument('--no-correction', action='store_true',
                               help='è·³è¿‡æ–‡æœ¬çº é”™')
    parser_analyze.add_argument('--no-summary', action='store_true',
                               help='è·³è¿‡æ‘˜è¦ç”Ÿæˆ')
    parser_analyze.add_argument('--no-keywords', action='store_true',
                               help='è·³è¿‡å…³é”®è¯æå–')
    
    # test å‘½ä»¤
    parser_test = subparsers.add_parser('test', help='è¿è¡Œæµ‹è¯•')
    
    return parser


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    # æ ¹æ®å‘½ä»¤æ‰§è¡Œç›¸åº”å‡½æ•°
    try:
        if args.command == 'process':
            success = cmd_process(args)
        elif args.command == 'batch':
            success = cmd_batch(args)
        elif args.command == 'config':
            success = cmd_config(args)
        elif args.command == 'transcribe':
            success = cmd_transcribe(args)
        elif args.command == 'correct':
            success = cmd_correct(args)
        elif args.command == 'format':
            success = cmd_format(args)
        elif args.command == 'summarize':
            success = cmd_summarize(args)
        elif args.command == 'keywords':
            success = cmd_keywords(args)
        elif args.command == 'analyze':
            success = cmd_analyze(args)
        elif args.command == 'test':
            success = cmd_test(args)
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {args.command}")
            return 1
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main()) 