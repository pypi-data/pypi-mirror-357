import json
import os
import re
from pathlib import Path

import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


class DocumentManager:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ï¼ˆä¸€èˆ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ï¼‰
    DEFAULT_EXTENSIONS = [
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç³»
        ".md",
        ".mdx",
        ".txt",
        ".rst",
        ".asciidoc",
        ".org",
        # ãƒ‡ãƒ¼ã‚¿ãƒ»è¨­å®šç³»
        ".json",
        ".yaml",
        ".yml",
        ".toml",
        ".ini",
        ".cfg",
        ".conf",
        ".xml",
        ".csv",
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
        ".py",
        ".js",
        ".jsx",
        ".ts",
        ".tsx",
        ".java",
        ".cpp",
        ".c",
        ".h",
        ".hpp",
        ".cs",
        ".go",
        ".rs",
        ".rb",
        ".php",
        ".swift",
        ".kt",
        ".scala",
        ".r",
        ".m",
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ã‚·ã‚§ãƒ«
        ".sh",
        ".bash",
        ".zsh",
        ".fish",
        ".ps1",
        ".bat",
        ".cmd",
        # Webç³»
        ".html",
        ".htm",
        ".css",
        ".scss",
        ".sass",
        ".less",
        ".vue",
        ".svelte",
        ".astro",
        # è¨­å®šãƒ»ãƒ“ãƒ«ãƒ‰ç³»
        ".dockerfile",
        ".dockerignore",
        ".gitignore",
        ".env",
        ".env.example",
        ".editorconfig",
        ".prettierrc",
        ".eslintrc",
        ".babelrc",
        # ãã®ä»–
        ".sql",
        ".graphql",
        ".proto",
        ".ipynb",
    ]

    def __init__(self, allowed_folders: list[str] | None = None):
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        docs_base_dir = os.getenv("DOCS_BASE_DIR", os.getcwd())
        self.base_dir = Path(docs_base_dir)
        self.docs_dir = self.base_dir / "docs"
        self.metadata_file = self.base_dir / "docs_metadata.json"
        self.embeddings_file = self.base_dir / "docs_embeddings.json"

        # è¨±å¯ã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã®ãƒªã‚¹ãƒˆ
        self.allowed_folders = allowed_folders

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã®è¨­å®š
        extensions_env = os.getenv("DOCS_FILE_EXTENSIONS")
        if extensions_env:
            # ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ä½¿ç”¨ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
            self.allowed_extensions = [
                ext.strip() for ext in extensions_env.split(",") if ext.strip()
            ]
            # ãƒ‰ãƒƒãƒˆãŒãªã„å ´åˆã¯è¿½åŠ 
            self.allowed_extensions = [
                ext if ext.startswith(".") else f".{ext}"
                for ext in self.allowed_extensions
            ]
            print(f"Using custom file extensions: {', '.join(self.allowed_extensions)}")
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ‹¡å¼µå­ã‚’ä½¿ç”¨
            self.allowed_extensions = self.DEFAULT_EXTENSIONS

        self.docs_content: dict[str, str] = {}
        self.docs_metadata: dict[str, str] = {}
        self.embeddings_cache: dict[str, list[float]] = {}

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        self.max_chars_per_page = int(os.getenv("DOCS_MAX_CHARS_PER_PAGE", "10000"))
        self.large_file_threshold = int(os.getenv("DOCS_LARGE_FILE_THRESHOLD", "15000"))  # æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹

        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key) if api_key else None

    def load_documents(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€embeddingsã‚’èª­ã¿è¾¼ã¿"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        if self.metadata_file.exists():
            with open(self.metadata_file, encoding="utf-8") as f:
                self.docs_metadata = json.load(f)

        # Embeddingsã‚’èª­ã¿è¾¼ã¿
        if self.embeddings_file.exists():
            with open(self.embeddings_file, encoding="utf-8") as f:
                self.embeddings_cache = json.load(f)
                print(f"Loaded {len(self.embeddings_cache)} embeddings from cache")

        # èª­ã¿è¾¼ã‚€ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ±ºå®š
        if self.allowed_folders:
            # æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã®ã¿ã‚’èª­ã¿è¾¼ã‚€
            for folder_name in self.allowed_folders:
                folder_path = self.docs_dir / folder_name
                if folder_path.exists() and folder_path.is_dir():
                    self._load_folder(folder_path)
                else:
                    print(f"Warning: Folder not found: {folder_name}")
        else:
            # å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå¾“æ¥ã®å‹•ä½œï¼‰
            self._load_all_files()

    def _load_folder(self, folder_path: Path):
        """ç‰¹å®šã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        for file_path in folder_path.rglob("*"):
            if (
                file_path.is_file()
                and file_path.suffix.lower() in self.allowed_extensions
            ):
                # docs/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
                doc_path = str(file_path.relative_to(self.docs_dir)).replace("\\", "/")
                try:
                    with open(file_path, encoding="utf-8") as f:
                        content = f.read()
                        self.docs_content[doc_path] = content
                except Exception as e:
                    print(f"Error loading {doc_path}: {e}")

    def _load_all_files(self):
        """docså†…ã®ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        for file_path in self.docs_dir.rglob("*"):
            if (
                file_path.is_file()
                and file_path.suffix.lower() in self.allowed_extensions
            ):
                # docs/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
                doc_path = str(file_path.relative_to(self.docs_dir)).replace("\\", "/")
                try:
                    with open(file_path, encoding="utf-8") as f:
                        content = f.read()
                        self.docs_content[doc_path] = content
                except Exception as e:
                    print(f"Error loading {doc_path}: {e}")

    def list_documents(self) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’è¿”ã™"""
        result = []
        for path in sorted(self.docs_content.keys()):
            description = self.docs_metadata.get(path, "")
            if description:
                result.append(f"{path} - {description}")
            else:
                result.append(path)
        return "\n".join(result)

    def get_document(self, path: str, page: int | None = None) -> str:
        """æŒ‡å®šã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’è¿”ã™ï¼ˆæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
        
        Args:
            path: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            page: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1ã‹ã‚‰é–‹å§‹ã€Noneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
        """
        if path not in self.docs_content:
            return f"Error: Document not found: {path}"
        
        content = self.docs_content[path]
        total_chars = len(content)
        
        # ãƒšãƒ¼ã‚¸æŒ‡å®šãŒãªã„å ´åˆï¼ˆå¾“æ¥ã®å‹•ä½œï¼‰
        if page is None:
            # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯è‡ªå‹•çš„ã«1ãƒšãƒ¼ã‚¸ç›®ã‚’è¿”ã™
            if total_chars > self.large_file_threshold:
                # 1ãƒšãƒ¼ã‚¸ç›®ã¨ã—ã¦å‡¦ç†
                page = 1
            else:
                # å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã¯å¾“æ¥é€šã‚Šå…¨æ–‡ã‚’è¿”ã™
                return content
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ï¼ˆæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        total_pages = (total_chars + self.max_chars_per_page - 1) // self.max_chars_per_page
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        if page < 1:
            return f"Error: Page number must be 1 or greater"
        if page > total_pages:
            return f"Error: Page {page} not found. Total pages: {total_pages} (max chars per page: {self.max_chars_per_page:,})"
        
        # ãƒšãƒ¼ã‚¸ç¯„å›²è¨ˆç®—ï¼ˆæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã€è¡Œã‚’åˆ†å‰²ã—ãªã„ã‚ˆã†èª¿æ•´ï¼‰
        start_char = (page - 1) * self.max_chars_per_page
        end_char = min(start_char + self.max_chars_per_page, total_chars)
        
        # è¡Œã®é€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†èª¿æ•´
        if end_char < total_chars:
            # æ¬¡ã®æ”¹è¡Œæ–‡å­—ã¾ã§å«ã‚ã‚‹
            next_newline = content.find('\n', end_char)
            if next_newline != -1:
                end_char = next_newline + 1
        
        page_content = content[start_char:end_char]
        
        # è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆè¡¨ç¤ºç”¨ï¼‰
        lines_before_start = content[:start_char].count('\n')
        page_lines = page_content.count('\n')
        total_lines = content.count('\n') + 1
        start_line = lines_before_start + 1
        end_line = min(start_line + page_lines, total_lines)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ˜ãƒƒãƒ€ãƒ¼
        header = f"ğŸ“„ Document: {path}\n"
        header += f"ğŸ“– Page {page}/{total_pages} (chars {start_char+1:,}-{end_char:,}/{total_chars:,})\n"
        header += f"ğŸ“ Lines {start_line}-{end_line}/{total_lines:,} | Max chars per page: {self.max_chars_per_page:,}\n"
        
        # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã§è‡ªå‹•çš„ã«ãƒšãƒ¼ã‚¸1ã‚’è¡¨ç¤ºã—ãŸå ´åˆã¯ä½¿ã„æ–¹ã‚’è¿½åŠ 
        if page == 1 and total_chars > self.large_file_threshold:
            header += f"âš ï¸  Large document auto-paginated. To see other pages:\n"
            header += f"ğŸ’¡ get_doc('{path}', page=2)  # Next page\n"
            header += f"ğŸ’¡ get_doc('{path}', page={total_pages})  # Last page\n"
        
        header += "â”€" * 60 + "\n\n"
        
        return header + page_content

    def grep_search(self, pattern: str, ignore_case: bool = True) -> str:
        """æ­£è¦è¡¨ç¾ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        try:
            flags = re.IGNORECASE if ignore_case else 0
            regex = re.compile(pattern, flags)
        except re.error as e:
            return f"Error: Invalid regex pattern: {e}"

        results = []
        for doc_path, content in sorted(self.docs_content.items()):
            lines = content.split("\n")
            for i, line in enumerate(lines, 1):
                if regex.search(line):
                    line_preview = line.strip()
                    if len(line_preview) > 120:
                        line_preview = line_preview[:117] + "..."
                    results.append(f"{doc_path}:{i}: {line_preview}")

        if not results:
            return "No matches found"

        # çµæœãŒå¤šã™ãã‚‹å ´åˆã¯åˆ¶é™
        if len(results) > 100:
            total = len(results)
            results = results[:100]
            results.append(f"\n... and {total - 100} more matches")

        return "\n".join(results)

    def semantic_search(self, query: str, limit: int = 5) -> str:
        """æ„å‘³çš„ã«é–¢é€£ã™ã‚‹å†…å®¹ã‚’æ¤œç´¢"""
        if not self.client:
            return "Error: OpenAI API key not configured"

        if not self.embeddings_cache:
            return "Error: No embeddings available. Run 'python scripts/generate_metadata.py' first."

        try:
            # ã‚¯ã‚¨ãƒªã®embeddingã‚’å–å¾—
            query_embedding = self._get_embedding(query)

            # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            similarities = []
            for doc_path, doc_embedding in self.embeddings_cache.items():
                # embeddingãŒãƒªã‚¹ãƒˆã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
                similarity = self._cosine_similarity(query_embedding, doc_embedding)
                similarities.append((doc_path, similarity))

            # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            similarities.sort(key=lambda x: x[1], reverse=True)

            # çµæœã‚’æ§‹ç¯‰
            results = []
            for doc_path, similarity in similarities[:limit]:
                description = self.docs_metadata.get(doc_path, "")
                result_line = f"{doc_path} (ç›¸ä¼¼åº¦: {similarity:.3f})"
                if description:
                    result_line += f" - {description}"
                results.append(result_line)

                # é–¢é€£ã™ã‚‹å†…å®¹ã‚’ä¸€éƒ¨æŠ½å‡º
                if doc_path in self.docs_content:
                    content = self.docs_content[doc_path]
                    preview = self._extract_preview(content, query)
                    if preview:
                        results.append(f"  â†’ {preview}")

            return "\n\n".join(results)

        except Exception as e:
            return f"Error during semantic search: {e}"

    def get_doc_count(self) -> int:
        """èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’è¿”ã™"""
        return len(self.docs_content)

    def _get_embedding(self, text: str) -> list[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’å–å¾—"""
        text = text.replace("\n", " ")
        if self.client is None:
            raise ValueError("OpenAI client not initialized")
        response = self.client.embeddings.create(
            input=[text], model="text-embedding-3-large"
        )
        return response.data[0].embedding

    def _cosine_similarity(self, vec1: list[float], vec2: list[float]) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

    def _extract_preview(self, content: str, query: str, max_length: int = 200) -> str:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º"""
        lines = content.split("\n")
        query_words = query.lower().split()

        for line in lines:
            line_lower = line.lower()
            if (
                any(word in line_lower for word in query_words)
                and len(line.strip()) > 20
            ):
                preview = line.strip()
                if len(preview) > max_length:
                    preview = preview[: max_length - 3] + "..."
                return preview

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®æ„å‘³ã®ã‚ã‚‹è¡Œã‚’è¿”ã™
        for line in lines:
            if len(line.strip()) > 20:
                preview = line.strip()
                if len(preview) > max_length:
                    preview = preview[: max_length - 3] + "..."
                return preview

        return ""
