---
title: Introduction
---

## Overview

Evaluation is the process of **scoring** an LLM system's outputs with metrics; an evaluation is composed of:
- An evaluation dataset
- Metrics we are interested in tracking


## Examples 

In `judgeval`, an Example is a unit of data that allows you to use evaluation scorers on your LLM system.

<CodeGroup>
```Python Python
from judgeval.data import Example

example = Example(
    input="Who founded Microsoft?",
    actual_output="Bill Gates and Paul Allen.",
    retrieval_context=["Bill Gates co-founded Microsoft with Paul Allen in 1975."],
)
```

```Typescript Typescript
import { ExampleBuilder } from 'judgeval';

const example = new ExampleBuilder()
  .input("Based on the context, what is the capital of France?") 
  .actualOutput("According to the context, the capital of France is Paris.")
  .context([ 
    "France is a country in Western Europe.",
    "Paris is the capital and most populous city of France.",
    "The Eiffel Tower is located in Paris."
  ])
  .build();
```
</CodeGroup>

In this example, `input` represents a user talking with a RAG-based LLM application, where `actual_output` is the 
output of your chatbot and `retrieval_context` (Python) or `context` (Typescript) is the retrieved context. 

<Tip>
There are many fields in an `Example` that can be used in an evaluation. 
To learn more about the `Example` class, click [here](/evaluation/data_examples).
</Tip>

Creating an Example allows you to evaluate using 
`judgeval`'s default scorers:

<CodeGroup>
```Python Python
from judgeval import JudgmentClient
from judgeval.scorers import FaithfulnessScorer
from judgeval.data import Example

# Assume example is defined as above
example = Example(
    input="Who founded Microsoft?",
    actual_output="Bill Gates and Paul Allen.",
    retrieval_context=["Bill Gates co-founded Microsoft with Paul Allen in 1975."],
)

client = JudgmentClient()

faithfulness_scorer = FaithfulnessScorer(threshold=0.5)

results = client.run_evaluation(
    examples=[example],
    scorers=[faithfulness_scorer],
    model="gpt-4.1",
)

# You also run evaluations asynchronously like so:
results = client.a_run_evaluation(
    examples=[example],
    scorers=[faithfulness_scorer],
    model="gpt-4.1",
)
print(results)
```

```Typescript Typescript
import { JudgmentClient, ExampleBuilder, FaithfulnessScorer, logger } from 'judgeval';

const example = new ExampleBuilder()
  .input("Based on the context, what is the capital of France?")
  .actualOutput("According to the context, the capital of France is Paris.")
  .context([
    "France is a country in Western Europe.",
    "Paris is the capital and most populous city of France.",
    "The Eiffel Tower is located in Paris."
  ])
  .build();

const client = JudgmentClient.getInstance();

const faithfulnessScorer = new FaithfulnessScorer(0.5);

const results = await client.evaluate({
  examples: [example],
  scorers: [faithfulnessScorer],
  model: "gpt-4.1",
  projectName: "my-intro-project",
  evalName: "intro-evaluation-run"
});

logger.print(results);
```
</CodeGroup>

## Datasets

An Evaluation Dataset is a collection of Examples. It provides an interface for running **scaled evaluations** of 
your LLM system using one or more scorers.

<CodeGroup>
```python Python
from judgeval.data import Example
from judgeval.data.datasets import EvalDataset

example1 = Example(input="...", actual_output="...", retrieval_context="...")
example2 = Example(input="...", actual_output="...", retrieval_context="...")

dataset = EvalDataset(examples=[example1, example2])
```

```typescript Typescript
import { ExampleBuilder, EvalDataset } from 'judgeval';

const example1 = new ExampleBuilder()
  .input("...")
  .actualOutput("...")
  .context(["..."])
  .build();

const example2 = new ExampleBuilder()
  .input("...")
  .actualOutput("...")
  .context(["..."])
  .build();

// Assuming you have an EvalDatasetClient instance named 'datasetClient'
// const dataset = datasetClient.createDataset([example1, example2]); 
// Or initialize directly:
const dataset = new EvalDataset([example1, example2]);
```
</CodeGroup>

`EvalDataset`s can be saved (loaded) to (from) disk in `csv`, `yaml`, and `json` format or uploaded to the [Judgment platform](/judgment/introduction).

<Note>
For more information on how to use `EvalDataset`s, please see the [EvalDataset docs](/evaluation/data_datasets).
</Note>
Then, you can run evaluations on the dataset:

<CodeGroup>
```python Python
from judgeval import JudgmentClient
from judgeval.scorers import FaithfulnessScorer
# Assume dataset is defined as above

client = JudgmentClient()
scorer = FaithfulnessScorer(threshold=0.5)
results = client.run_evaluation(
    examples=dataset.examples,
    scorers=[scorer],
    model="Qwen/Qwen2.5-72B-Instruct-Turbo",
)
```

```typescript Typescript
import { JudgmentClient, FaithfulnessScorer, logger } from 'judgeval';
// Assume dataset is defined as above and is an instance of EvalDataset

const client = JudgmentClient.getInstance();
const scorer = new FaithfulnessScorer(0.5);

// Use the standard evaluate method, passing the examples from the dataset
const results = await client.evaluate({
  examples: dataset.examples, // Pass the examples array from the dataset
  scorers: [scorer],
  model: "Qwen/Qwen2.5-72B-Instruct-Turbo",
  projectName: "my-dataset-project", // Specify project and eval names
  evalName: "dataset-evaluation-run"
});

logger.print(results);
```
</CodeGroup>

## Metrics 

`judgeval` comes with a set of 10+ built-in evaluation metrics. These metrics are accessible through `judgeval`'s `Scorer` interface. 
Every `Scorer` has a `threshold` parameter that you can use in the context of unit testing your app.

<CodeGroup>
```python Python
from judgeval.scorers import FaithfulnessScorer

scorer = FaithfulnessScorer(threshold=1.0)
```

```typescript Typescript
import { FaithfulnessScorer } from 'judgeval';

const scorer = new FaithfulnessScorer(1.0);
```
</CodeGroup>

You can use scorers to evaluate your LLM system's outputs by using `Example`s.

<Tip>
We're always working on adding new scorers, so if you have a metric you'd like to add, please [let us know!](mailto:contact@judgmentlabs.ai)
</Tip>


**Congratulations!** ðŸŽ‰ 

You've learned the basics of building and running evaluations with `judgeval`. 

For a deep dive into all the metrics you can run using `judgeval` scorers, click [here](/evaluation/scorers).
