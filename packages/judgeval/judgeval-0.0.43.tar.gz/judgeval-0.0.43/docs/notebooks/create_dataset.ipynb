{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Dataset**\n",
    "\n",
    "In practice, you will have multiple `Example`s. `Judgeval` structures collections of `Examples` into `EvalDataset` objects. Let's see how to create and use one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexshan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/alexshan/Desktop/judgment_labs/judgeval/\")  # root of judgeval\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from judgeval.data.datasets import EvalDataset\n",
    "from judgeval.data import Example\n",
    "\n",
    "example_1 = Example(input=\"sample input\", actual_output=\"sample output\")\n",
    "example_2 = Example(input=\"another input\", actual_output=\"another output\")\n",
    "\n",
    "dataset = EvalDataset([example_1, example_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can directly use the Judgment Client in order to create datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized JudgmentClient, welcome back Joseph Camyre!\n"
     ]
    }
   ],
   "source": [
    "from judgeval.judgment_client import JudgmentClient\n",
    "\n",
    "client = JudgmentClient()\n",
    "\n",
    "dataset = client.create_dataset()\n",
    "dataset.add_example(example_1)\n",
    "dataset.add_example(example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we may not want to precompute the `actual_output` field in our `Example` objects and instead create our outputs at test-time to reflect the current state of our workflow. \n",
    "\n",
    "In this case, we can add `GroundTruthExample`s to our `EvalDataset`, which do not require having the `actual_output` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.data.datasets import GroundTruthExample\n",
    "\n",
    "ground_truth_1 = GroundTruthExample(input=\"sample input\")\n",
    "ground_truth_2 = GroundTruthExample(input=\"another input\")\n",
    "\n",
    "dataset.add_ground_truth(ground_truth_1)\n",
    "dataset.add_ground_truth(ground_truth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving datasets**\n",
    "\n",
    "Now that we've created our dataset, it would be nice to be able to save it so that we don't have to go through the work to create it each time we run evaluation. \n",
    "\n",
    "Lucky for us, we can save our dataset directly to the Judgment platform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cf0169b1fa4ce283ff17e7ab097f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_NAME = \"My awesome dataset\"\n",
    "\n",
    "client.push_dataset(DATASET_NAME, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next time we use Judgment to run an evaluation, we can now fetch our dataset under the same name we saved it with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a42a163ffcd45f5949252b4c3eb965c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalDataset(ground_truths=[GroundTruthExample(input='sample input', actual_output=None, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None), GroundTruthExample(input='another input', actual_output=None, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None), GroundTruthExample(input='sample input', actual_output=None, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None), GroundTruthExample(input='another input', actual_output=None, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None)], examples=[Example(input='sample input', actual_output='sample output', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, tools_called=None, expected_tools=None, name=None), Example(input='another input', actual_output='another output', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, tools_called=None, expected_tools=None, name=None), Example(input='sample input', actual_output='sample output', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, tools_called=None, expected_tools=None, name=None), Example(input='another input', actual_output='another output', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, tools_called=None, expected_tools=None, name=None)], _alias=None, _id=None)\n"
     ]
    }
   ],
   "source": [
    "dataset = client.pull_dataset(DATASET_NAME)\n",
    "\n",
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
