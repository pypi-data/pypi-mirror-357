import json
import os
from pathlib import Path

from phs.timeline.event_resolver import EventResolver
from phs.timeline.schema_validator import SchemaValidator

# Function to create directories
def create_directories(base_path, directories):
    for directory in directories:
        full_path = os.path.join(base_path, directory)
        os.makedirs(full_path, exist_ok=True)
        print(f"Created directory: {full_path}")


# Function to modify and save a new JSON file for each unit
def create_json_files(base_path, directories, json_template, operational_period, reference_event, event_counter):
    # Load the provided JSON template from the string
    json_data = json.loads(json_template)

    for unit in directories:

        if unit == 'PEP':
            # Modify the "unit" field and save a new JSON file for each unit
            json_data["activities"][0]["unit"] = "PEL"
            json_data["activities"][0]["id"] = "PEL-01"
            json_data["activities"][0]["counter"] = f"{event_counter}"
            json_data["activities"][0]["event"] = f"{reference_event}"

            # Define the new filename and save the file
            new_filename = f"APL_{unit}_PEL_{operational_period}_S00P00.json"
            new_file_path = os.path.join(base_path, unit, new_filename)

            with open(new_file_path, 'w') as outfile:
                json.dump(json_data, outfile, indent=4)

            print(f"Created JSON file: {new_file_path}")

            # Modify the "unit" field and save a new JSON file for each unit
            json_data["activities"][0]["unit"] = "PEH"
            json_data["activities"][0]["id"] = "PEH-01"
            json_data["activities"][0]["counter"] = f"{event_counter}"
            json_data["activities"][0]["event"] = f"{reference_event}"

            # Define the new filename and save the file
            new_filename = f"APL_{unit}_PEH_{operational_period}_S00P00.json"
            new_file_path = os.path.join(base_path, unit, new_filename)

            with open(new_file_path, 'w') as outfile:
                json.dump(json_data, outfile, indent=4)

            print(f"Created JSON file: {new_file_path}")

        else:

            # Modify the "unit" field and save a new JSON file for each unit
            json_data["activities"][0]["unit"] = unit
            json_data["activities"][0]["id"] = f"{unit}-01"
            json_data["activities"][0]["counter"] = f"{event_counter}"
            json_data["activities"][0]["event"] = f"{reference_event}"

            # Define the new filename and save the file
            new_filename = f"APL_{unit}_{unit}_{operational_period}_S00P00.json"
            new_file_path = os.path.join(base_path, unit, new_filename)

            with open(new_file_path, 'w') as outfile:
                json.dump(json_data, outfile, indent=4)

            print(f"Created JSON file: {new_file_path}")


def generate_initial_files(base_path, operational_period="PCW3", reference_event="COEV", event_counter='3'):
    # Define the directories based on the last part of each path
    directories = [
        "3GM", "GAL", "JAN", "JMC", "MAG", "MAJ", "NAV", "PEP", "RIM", "RPW", "SWI", "SYS", "UVS"
    ]

    # Paths and operational period
    # Define the JSON template as a string
    json_template = """
    {
      "activities": [
        {
          "id": "Identification of the activity, e.g.: SWI_F_0-2",
          "unit": "SWI",
          "stack": "Identification for a number of activities that will be scheduled WRT the same event or will be in a POR, e.g.: SWI_PDOR",
          "event": "COEV",
          "counter": 2,
          "duration": "Duration of the activity in +DDD.HH:MM:SS.MMM format, e.g: +000.01:30:00.000",
          "relative": "Relative time of the activity with respect to the event (and counter) in +DDD.HH:MM:SS.MMM format, e.g: -001.01:00:01.000",
          "description": "Free text with high-level description of the activity.",
          "scheduling_rules": "Free text indicating scheduling rules for the activity.",
          "tc_budget": "Integer that provides the estimated number of TCs generated by the activity. e.g. 15 (without quotes)",
          "hk_budget": "String with housekeeping data volume (DV) or data rate (DR); DV allowed units are: bits, kbits, mbits, gbits; DR allowed units are: bps, kbps, mbps. E.g.: 35.5 mbits",
          "data_resource_x": "String with science data volume (DV) or data rate (DR) using X-Band; DV allowed units are: bits, kbits, mbits, gbits; DR allowed units are: bps, kbps, mbps. E.g.: 35.5 mbits",
          "data_resource_ka": "String with science data volume (DV) or data rate (DR) using Ka-Band; DV allowed units are: bits, kbits, mbits, gbits; DR allowed units are: bps, kbps, mbps. E.g.: 35.5 mbits",
          "start": "Not an input; start time of the activity in absolute time, UTC date format, i.e. 2024-01-24T07:33:36.000Z.",
          "end": "Not an input; end time of the activity in absolute time, UTC date format, i.e. 2024-01-24T07:33:36.000Z.",
          "soc_event": "If a SOC event is used to schedule the activity the event will appear with a counter and its relative time when exporting from the Timeline Tool.",
          "soc_counter": 1,
          "soc_relative": "Relative time of the activity with respect to a SOC event (and counter) in +DDD.HH:MM:SS.MMM format, e.g: -001.01:00:01.000"
      }
      ]
    }
    """

    # Create directories
    create_directories(base_path, directories)

    # Create JSON files
    create_json_files(base_path, directories, json_template, operational_period, reference_event, event_counter)


def concatenate_files(directory, output_file, pattern='SXXPYY'):
    """
    Concatenate APL JSON files matching a pattern from a directory.

    Parameters
    ----------
    directory : str
        The directory to search for JSON files.
    output_file : str
        The file to write the concatenated JSON data.
    pattern : str, optional
        Pattern to match in file names (default is 'SXXPYY').

    Returns
    -------
    None
        This function does not return a value, it writes data to a file.
    """
    concatenated_activities = []

    # Walk through all subdirectories and find JSON files matching the pattern
    for root, dirs, files in os.walk(directory):
        if root != directory:  # Exclude top-level directory
            for file in files:
                if pattern in file and file.endswith(".json"):
                    file_path = os.path.join(root, file)
                    with open(file_path, 'r') as f:
                        try:
                            data = json.load(f)
                        except ValueError as e:
                            print(f'Invalid APL {file_path}: %s' % e)
                            return None  # or: raise
                        if "activities" in data:
                            concatenated_activities.extend(data["activities"])

    # Write the concatenated activities to the output file
    with open(output_file, 'w+') as f:
        json.dump({"activities": concatenated_activities}, f, indent=4)

    return


def check_duplicate_ids(apl_file):
    """
    Check for duplicate IDs in an APL JSON file.

    Parameters
    ----------
    apl_file : str
        The path to the JSON file to check for duplicate IDs.

    Returns
    -------
    None
        This function does not return a value, it prints duplicate IDs if found.
    """
    # Open the file containing JSON data
    with open(apl_file, 'r') as file:
        data = file.read()

    # Parse the JSON
    parsed_data = json.loads(data)

    # Extract the activities
    activities = parsed_data['activities']

    # Create a set to store unique IDs
    unique_ids = set()

    # List to store duplicate IDs
    duplicate_ids = []

    # Iterate through activities
    for activity in activities:
        activity_id = activity['id']
        # Check if ID is already in the unique set
        if activity_id in unique_ids:
            duplicate_ids.append(activity_id)
        else:
            unique_ids.add(activity_id)

    # Print duplicate IDs, if any
    if duplicate_ids:
        print("Duplicate IDs found:")
        for duplicate_id in duplicate_ids:
            print(duplicate_id)
    else:
        print("No duplicate IDs found.")

    return


def resolve_apl(apl_file: str, uevt: str):
    """
    Check for duplicate IDs in an APL JSON file.

    Parameters
    ----------
    apl_file : str
        The path to the JSON file to check for duplicate IDs.

    uevt : str
        The name of the user event

    """
    apl_file_path = Path(apl_file)

    resolver = EventResolver.from_rest_api(uevt)

    # Open the file containing JSON data
    with apl_file_path.open('r') as file:
        data = file.read()

    # Parse the JSON
    parsed_data = json.loads(data)
    resolver.resolve_apl(parsed_data)

    # Dump the results in an new subfixed with 'resolved'
    output_file = Path(str(apl_file_path.absolute()).replace('.json', '_resolved.json'))
    with open(output_file, 'w') as file:
        json.dump(parsed_data, file, indent=2)

    return output_file


def schema_validate_apl(apl_file):
    """
    Checks the validity against its Schema of an APL JSON file.

    Parameters
    ----------
    apl_file : str
        The path to the JSON file to check for duplicate IDs.

    Raises
    ------
        `SchemaValidatorException`:
            if the remote schema is not accessible
        `jsonschema.exceptions.ValidationError`:
            if the instance is invalid
        `jsonschema.exceptions.SchemaError`:
            if the schema itself is invalid
    """

    url = 'https://juicesoc.esac.esa.int/data/schemas/jsoc-apl-schema.json'
    
    file_path = Path(apl_file)
    with file_path.open('r') as file:
        instance = json.load(file)
    validator = SchemaValidator(url)
    validator.validate(instance)