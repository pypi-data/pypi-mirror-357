import json

from dataclasses import dataclass, field
from typing import List, Literal, Optional, Dict, Any


@dataclass
class FunctionCall:
    """
    The function that the model called.
    """
    name: Optional[str] = field(
        default=None,
        metadata={"description": "The name of the function to call."}
    )
    arguments: Optional[str] = field(
        default=None,
        metadata={
            "description": (
                "The arguments to call the function with, as generated by the "
                "model in JSON format. Note that the model does not always "
                "generate valid JSON, and may hallucinate parameters not "
                "defined by your function schema. Validate the arguments in "
                "your code before calling your function."
            )
        }
    )

    def parse_arguments(self) -> dict:
        """Attempts to parse the arguments string as JSON."""

        if self.arguments is None:
            return {}

        return json.loads(self.arguments)


@dataclass
class MessageToolCall:
    """
    Represents a tool call within a message, specifically a function call.
    """
    id: str = field(metadata={"description": "The ID of the tool call."})
    type: Literal["function"] = field(
        metadata={
            "description": (
                "The type of the tool. Currently, only `function` is supported."
            )
        }
    )
    function: FunctionCall = field(
        metadata={"description": "The function that the model called."}
    )

    def to_dict(self):
        return {
            "id": self.id,
            "type": self.type,
            "function": {
                "name": self.function.name,
                "arguments": self.function.arguments,
            }
        }


@dataclass
class ToolCallChunk:
    index: int
    id: Optional[str] = None
    type: Optional[str] = None
    function: Optional[FunctionCall] = None


@dataclass
class ChatDelta:
    """
    Represents a partial update in a streaming chat response.

    Attributes:
        content: The text content of the chat delta.
        role: Optional role identifier (e.g., 'assistant', 'user') for the message delta.
    """
    content: str
    role: Optional[str] = None
    refusal: Optional[str] = None
    tool_calls: Optional[List[ToolCallChunk]] = None


@dataclass
class LLMUsage:
    """
    Represents the token usage statistics for an LLM (Large Language Model) operation.

    Attributes:
        completion_tokens: The number of tokens generated by the model in the response.
        prompt_tokens: The number of tokens in the input prompt.
        total_tokens: The total number of tokens used, including both prompt and completion tokens.
    """
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int


@dataclass
class ResponseChatMessage:
    role: str
    content: str
    refusal: Optional[str] = None
    tool_calls: Optional[List[MessageToolCall]] = None

    def to_dict(self):
        base = {
            "role": self.role,
            "content": self.content,
        }

        if self.refusal is not None:
            base["refusal"] = self.refusal

        if self.tool_calls:
            base["tool_calls"] = [tool_call.to_dict() for tool_call in self.tool_calls]

        return base


@dataclass
class LLMChoice:
    """
    Represents an individual choice or response generated by the LLM.
    
    Attributes:
        index: The index of this particular choice in the list of possible choices.
        message: The message object containing the role and content of the response.
        finish_reason: The reason why the model stopped generating tokens.
        logprobs: Optional dictionary containing token log probabilities.
    """
    index: int
    message: ResponseChatMessage
    finish_reason: str
    logprobs: Optional[Dict[str, float]] = None


@dataclass
class LLMChoiceStreaming:
    """
        Represents a streaming choice or partial response generated by the LLM.

        Attributes:
            index: The index of this particular choice in the list of possible choices.
            delta: The partial update (ChatDelta) for this choice.
            finish_reason: Optional reason for why the generation stopped, may be None during streaming.
            logprobs: Optional dictionary containing token log probabilities.
    """
    index: int
    delta: ChatDelta
    finish_reason: Optional[str] = None
    logprobs: Optional[Dict[str, float]] = None


@dataclass
class BaseLLMResult:
    """
    Base class for LLM results containing common attributes.

    Attributes:
        id: Unique identifier for the LLM operation.
        object: Type of object returned (e.g., "chat.completion").
        created: Timestamp when this result was created.
        model: Name or identifier of the model used.
    """
    id: str
    object: str
    created: int
    model: str


@dataclass
class LLMResult(BaseLLMResult):
    """
    Represents the complete result from an LLM operation.

    Attributes:
        id: The unique identifier for this LLM operation.
        object: The type of object returned (typically "chat.completion").
        created: The timestamp when this result was created.
        model: The name or identifier of the model used for generating the result.
        choices: A list of possible choices generated by the LLM.
        usage: The token usage statistics for this operation.
        system_fingerprint: An optional system fingerprint for tracking the model used.
    """
    choices: List[LLMChoice]
    usage: LLMUsage
    system_fingerprint: Optional[str] = None


@dataclass
class LLMResultStreaming(BaseLLMResult):
    """
        Represents a streaming result from an LLM operation.

        Attributes:
            id: The unique identifier for this LLM operation.
            object: The type of object returned (typically "chat.completion.chunk").
            created: The timestamp when this result was created.
            model: The name or identifier of the model used.
            choices: A list of streaming choices generated by the LLM.
            system_fingerprint: An optional system fingerprint for tracking the model used.
    """
    choices: List[LLMChoiceStreaming]
    system_fingerprint: Optional[str] = None


@dataclass
class GenericModuleResult:
    """
    Represents a generic module result in the orchestration process.

    Attributes:
        message: A message or description generated by the module.
        data: Additional data relevant to the module result.
    """
    message: str
    data: Optional[Dict[str, Any]] = None


@dataclass
class BaseModuleResults:
    """
        Base class for module results containing grounding, common filtering and masking attributes.

        Attributes:
            input_filtering: Results from the input filtering module.
            output_filtering: Results from the output filtering module.
            input_masking: Results from the input masking module.
            grounding: A list of extracted text to be provided as grounding context.
            input_translation: Results from the input translation module.
            output_translation: Results from the output translation module.
    """
    input_filtering: Optional[GenericModuleResult] = None
    output_filtering: Optional[GenericModuleResult] = None
    input_masking: Optional[GenericModuleResult] = None
    grounding: Optional[GenericModuleResult] = None
    input_translation: Optional[GenericModuleResult] = None
    output_translation: Optional[GenericModuleResult] = None


@dataclass
class ModuleResults(BaseModuleResults):
    """
    Represents the results of various modules used in processing an orchestration request.

    Attributes:
        templating: A list of messages that define the conversation's context or template.
        llm: The result from the LLM operation.
        input_filtering: The result of any input filtering, if applicable.
        output_filtering: The result of any output filtering, if applicable.
        input_masking: The result of input masking, if applicable.
        output_unmasking: The result of output unmasking, if applicable.
    """
    llm: Optional[LLMResult] = None
    templating: Optional[List[ResponseChatMessage]] = None
    output_unmasking: Optional[List[LLMChoice]] = None


@dataclass
class ModuleResultsStreaming(BaseModuleResults):
    """
        Represents the streaming results of various modules used in processing an orchestration request.

        Attributes:
            llm: The streaming result from the LLM operation.
            templating: A list of chat deltas that define the conversation's context or template.
            input_filtering: The result of any input filtering, if applicable.
            output_filtering: The result of any output filtering, if applicable.
            input_masking: The result of input masking, if applicable.
            output_unmasking: The result of output unmasking for streaming responses.
    """
    llm: Optional[LLMResultStreaming] = None
    templating: Optional[List[ChatDelta]] = None
    output_unmasking: Optional[List[LLMChoiceStreaming]] = None


@dataclass
class OrchestrationResponse:
    """
    Represents the complete response from an orchestration process.

    Attributes:
        request_id: The unique identifier for the request being processed.
        module_results: The results from the various modules involved in processing the request.
        orchestration_result: The final result from the orchestration, typically mirroring the LLM result.
    """
    request_id: str
    module_results: ModuleResults
    orchestration_result: LLMResult


@dataclass
class OrchestrationResponseStreaming:
    """
        Represents the streaming response from an orchestration process.

        Attributes:
            request_id: The unique identifier for the request being processed.
            module_results: The streaming results from the various modules involved in processing the request.
            orchestration_result: The streaming result from the orchestration.
    """
    request_id: str
    module_results: ModuleResultsStreaming
    orchestration_result: LLMResultStreaming
