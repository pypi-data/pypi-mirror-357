# Tests

Unit tests were used to validate the accuracy of the conversion code at each edge of the knowledge graph. 

To validate each edge, which connects a source representation to a target representation, the tests start with a randomly generated test image in the default representation: an RGB image as a NumPy ndarray in CPU memory, with no minibatching, the color channel as the last dimension, and pixel values represented as 8-bit unsigned integers ranging from 0 to 255. This test image is transferred to the source representation using a manually developed code path in the test framework (not using our im2im). The automatically converted image generated by our im2im using the conversion code at the test edge is then compared to the manually converted source image. 

numpy.ndarray with shape (20, 20, 3) in uint8 format to torch.tensor with shape (3, 20, 20) in float [0, 1] on gpu

```python
# manual code path
import torch
image = torch.from_numpy(source_image)
image = image.permute(2, 0, 1)
image = image.unsqueeze(0)
image = image / 255.0
target_image = image.cuda()
# actuall the code generate from the im2im
from PIL import Image
from torchvision.transforms import functional as F
image = Image.fromarray(source_image)
image = F.to_tensor(image)
image = image.unsqueeze(0)
target_image = image.cuda()
```





The comparison is conducted using pixel-wise checks to ensure accuracy within predefined tolerances. As the knowledge graph grows, and new conversion codes are added, users just need to implement the function to generate the new test images from the reference RGB image, and the unit tests will automatically verify the new code.