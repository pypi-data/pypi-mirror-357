{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR5XStOKa5gY"
      },
      "source": [
        "# Using `mcpconn` with OpenAI in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T58pGE7ua5ga"
      },
      "source": [
        "This notebook demonstrates how to use the `mcpconn` library to interact with OpenAI's API. We will cover:\n",
        "\n",
        "1. Installing the necessary libraries.\n",
        "2. Setting up your OpenAI API key in Colab.\n",
        "3. Making a basic query to an OpenAI model.\n",
        "4. Managing conversation state to have contextual conversations.\n",
        "5. Using built-in guardrails to protect your application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66uplXAta5ga"
      },
      "source": [
        "## 1. Installation\n",
        "\n",
        "First, we need to install `mcpconn` and the `openai` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyQO0gl_a5gb",
        "outputId": "56aae5c8-8ae3-40cf-bb9a-fd00def2914a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mcpconn\n",
            "  Downloading mcpconn-0.1.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Collecting anthropic>=0.34.0 (from mcpconn)\n",
            "  Downloading anthropic-0.54.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from mcpconn) (0.28.1)\n",
            "Collecting mcp>=1.2.0 (from mcpconn)\n",
            "  Downloading mcp-1.9.4-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting python-dotenv>=1.0.0 (from mcpconn)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->mcpconn) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->mcpconn) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->mcpconn) (0.16.0)\n",
            "Collecting httpx-sse>=0.4 (from mcp>=1.2.0->mcpconn)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings>=2.5.2 (from mcp>=1.2.0->mcpconn)\n",
            "  Downloading pydantic_settings-2.10.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.2.0->mcpconn) (0.0.20)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp>=1.2.0->mcpconn)\n",
            "  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.2.0->mcpconn) (0.46.2)\n",
            "Requirement already satisfied: uvicorn>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.2.0->mcpconn) (0.34.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.1->mcp>=1.2.0->mcpconn) (8.2.1)\n",
            "Downloading mcpconn-0.1.2-py3-none-any.whl (16 kB)\n",
            "Downloading anthropic-0.54.0-py3-none-any.whl (288 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mcp-1.9.4-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.10.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: python-dotenv, httpx-sse, sse-starlette, pydantic-settings, anthropic, mcp, mcpconn\n",
            "Successfully installed anthropic-0.54.0 httpx-sse-0.4.0 mcp-1.9.4 mcpconn-0.1.2 pydantic-settings-2.10.0 python-dotenv-1.1.0 sse-starlette-2.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install mcpconn openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuItUfbva5gc"
      },
      "source": [
        "## 2. Setup OpenAI API Key\n",
        "\n",
        "To use OpenAI models, you need an API key. `mcpconn` will automatically look for the `OPENAI_API_KEY` in your environment variables.\n",
        "\n",
        "In Google Colab, you should set this using the **Secrets** tab in the left-hand sidebar.\n",
        "\n",
        "1. Click on the key icon (ðŸ”‘) in the left sidebar.\n",
        "2. Add a new secret with the name `OPENAI_API_KEY`.\n",
        "3. Paste your OpenAI API key into the value field.\n",
        "4. Make sure the \"Notebook access\" toggle is enabled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcUkdVNra5gc"
      },
      "source": [
        "## 3. Basic Query\n",
        "\n",
        "Now, let's make a simple query to OpenAI. We will instantiate `mcpconn` for the `openai` provider and call `connect()` to prepare the client."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "zyLDijJ7bctP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ApluPGpa5gc",
        "outputId": "12ee50d2-c7f5-4307-a4c1-03fd121d584b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client connected.\n",
            "\n",
            "Sending a basic query to OpenAI...\n",
            "OpenAI: Absolutely! Here are the main tools and capabilities I can use to help you:\n",
            "\n",
            "### 1. **Image Analysis**\n",
            "- I can analyze and interpret images you upload.\n",
            "- This includes text extraction (OCR), object recognition, diagrams, and more.\n",
            "\n",
            "### 2. **GitHub Documentation & Code Assistance**\n",
            "- I can fetch, read, and answer questions about documentation or code from public GitHub repositories.\n",
            "- This is useful for exploring project structure, usage, setup, and technical questions.\n",
            "\n",
            "### 3. **General Knowledge and Reasoning**\n",
            "- I have access to knowledge up until June 2024.\n",
            "- I can provide explanations, how-to guides, and advice on a wide range of subjects.\n",
            "\n",
            "### 4. **Text Processing**\n",
            "- I can summarize, paraphrase, or translate text.\n",
            "- I can help with writing, debugging, or refactoring code.\n",
            "\n",
            "If you have a specific task or type of tool in mind, let me know and I can tell you if I provide that service!\n",
            "\n",
            "Client disconnected.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import os\n",
        "from mclpclient import mcpconn\n",
        "\n",
        "# In Colab, we need to handle the asyncio event loop specially.\n",
        "try:\n",
        "  import nest_asyncio\n",
        "  nest_asyncio.apply()\n",
        "except ImportError:\n",
        "  !pip install nest_asyncio\n",
        "  import nest_asyncio\n",
        "  nest_asyncio.apply()\n",
        "\n",
        "MCP_SERVER_URL=\"https://mcp.deepwiki.com/mcp\"\n",
        "\n",
        "\n",
        "async def run_basic_query():\n",
        "    # mcpconn will automatically pick up the OPENAI_API_KEY from the environment.\n",
        "    client = None\n",
        "    try:\n",
        "        client = mcpconn(llm_provider=\"openai\", ssl_verify=False)\n",
        "\n",
        "        # With the OpenAI provider, we must call connect() to initialize the client state,\n",
        "        # even if we are not using any MCP tool servers.\n",
        "        # We can provide a placeholder URL.\n",
        "        await client.connect(MCP_SERVER_URL)\n",
        "        print(\"Client connected.\")\n",
        "\n",
        "        print(\"\\nSending a basic query to OpenAI...\")\n",
        "        response = await client.query(\"can you provide tools you have\")\n",
        "        print(f\"OpenAI: {response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"Please ensure your OPENAI_API_KEY is set correctly in Colab Secrets.\")\n",
        "    finally:\n",
        "        if client and client.is_connected():\n",
        "            await client.disconnect()\n",
        "            print(\"\\nClient disconnected.\")\n",
        "\n",
        "# Run the async main function\n",
        "asyncio.run(run_basic_query())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81IMi6_ta5gc"
      },
      "source": [
        "## 4. Managing Conversation State\n",
        "\n",
        "`mcpconn` makes it easy to manage conversations. The client can maintain the history of a conversation, allowing the LLM to have context for follow-up questions.\n",
        "\n",
        "Here's how to start a conversation and have a multi-turn dialogue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgzOZHm6a5gc",
        "outputId": "00eab7e1-c68b-4b12-d79f-ca95fbb5a607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started new conversation: 80e89a55-ae8d-4de7-b4eb-2f95041f1c98\n",
            "\n",
            "Query 1:\n",
            "AI: Here is the **README.md** for [meta-llama/llama](https://github.com/meta-llama/llama) as summarized from the repository:\n",
            "\n",
            "---\n",
            "\n",
            "# Llama 2\n",
            "\n",
            "This repository provides minimal code and utilities to load Meta's Llama 2 large language models (LLMs) and run inference. If you're looking for more detailed examples, particularly using Hugging Face, check out the [llama-cookbook](https://github.com/facebookresearch/llama-recipes/).\n",
            "\n",
            "> **As of the Llama 3.1 release, this repository is deprecated in favor of a consolidated ecosystem of repositories with expanded functionality.**\n",
            "\n",
            "## What is Llama 2?\n",
            "\n",
            "Llama 2 is a family of large language models ranging from 7 billion to 70 billion parameters, developed by Meta. This repository contains:\n",
            "- Pretrained models: base models for general tasks.\n",
            "- Fine-tuned chat models: optimized for dialogue and chat use.\n",
            "\n",
            "Model sizes available:\n",
            "| Model Size | Parameters | Model Parallel Value | Context Length |\n",
            "|------------|------------|---------------------|----------------|\n",
            "| 7B         | 7          | 1                   | 4096 tokens    |\n",
            "| 13B        | 13         | 2                   | 4096 tokens    |\n",
            "| 70B        | 70         | 8                   | 4096 tokens    |\n",
            "\n",
            "## Repository Structure\n",
            "\n",
            "- `model.py` â€” Transformer architecture\n",
            "- `tokenizer.py` â€” Text tokenization\n",
            "- `generation.py` â€” Text and chat generation\n",
            "- `example_text_completion.py` â€” How to use the base model\n",
            "- `example_chat_completion.py` â€” How to use the chat-optimized model\n",
            "- `download.sh` â€” Script to download models with license\n",
            "\n",
            "## User Workflow\n",
            "\n",
            "**1. Download model weights:**\n",
            "Run `download.sh` with a presigned URL from the Meta download portal.\n",
            "\n",
            "**2. Initialize the model:**\n",
            "Use provided scripts like `example_text_completion.py` and `example_chat_completion.py` to run inference using your downloaded weights.\n",
            "\n",
            "**3. Example usage:**\n",
            "- Use `text_completion()` for general text generation.\n",
            "- Use `chat_completion()` for dialogue, following the special formatting required by Llama-2-Chat models.\n",
            "\n",
            "## Pretrained vs. Chat Models\n",
            "\n",
            "- **Pretrained**: Use for text continuation, requires no special formatting.\n",
            "- **Chat**: Designed for dialogue, requires special prompt formatting and additional tags.\n",
            "\n",
            "## Model Download and Initialization\n",
            "\n",
            "- Run `download.sh`.\n",
            "- Initialize paths and parameters (e.g., checkpoint directory, tokenizer path, context length, etc.).\n",
            "- Follow the examples to load and use the model for inference.\n",
            "\n",
            "## References\n",
            "\n",
            "- [Llama 2 Research Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
            "- [Llama Cookbook](https://github.com/facebookresearch/llama-recipes/)\n",
            "- For more in-depth documentation, see the repository's further Markdown files and scripts.\n",
            "\n",
            "---\n",
            "\n",
            "Let me know if you want the raw (long-form) README.md contents or details from a specific section!\n",
            "\n",
            "Query 2:\n",
            "AI: Your question is a bit general, so I need a little more context to answer accurately. \"Models\" can refer to many things depending on the subject:\n",
            "\n",
            "- **AI and Machine Learning:** Refers to different neural network models (e.g., GPT-3, GPT-4, BERT, DALL-E, etc.). There are **hundreds** if not thousands of different AI models.\n",
            "- **Physical Models:** Could refer to models in science (e.g., climate models, physics models), or even things like model cars or airplanes.\n",
            "- **Fashion:** Refers to people who model clothing or products.\n",
            "- **Other fields:** Each field (statistics, economics, architecture, etc.) has its own concept of \"models.\"\n",
            "\n",
            "**Please clarify:**  \n",
            "- Are you referring to AI/ML models (like GPT)?\n",
            "- Are you asking about models in a specific software or dataset?\n",
            "- Or do you mean \"models\" in a completely different context?\n",
            "\n",
            "Let me know, and I'll provide a more specific answer!\n",
            "\n",
            "--- Conversation History ---\n",
            "- user: how many models are there\n",
            "- assistant: Your question is a bit general, so I need a little more context to answer accurately. \"Models\" can refer to many things depending on the subject:\n",
            "\n",
            "- **AI and Machine Learning:** Refers to different neural network models (e.g., GPT-3, GPT-4, BERT, DALL-E, etc.). There are **hundreds** if not thousands of different AI models.\n",
            "- **Physical Models:** Could refer to models in science (e.g., climate models, physics models), or even things like model cars or airplanes.\n",
            "- **Fashion:** Refers to people who model clothing or products.\n",
            "- **Other fields:** Each field (statistics, economics, architecture, etc.) has its own concept of \"models.\"\n",
            "\n",
            "**Please clarify:**  \n",
            "- Are you referring to AI/ML models (like GPT)?\n",
            "- Are you asking about models in a specific software or dataset?\n",
            "- Or do you mean \"models\" in a completely different context?\n",
            "\n",
            "Let me know, and I'll provide a more specific answer!\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import os\n",
        "from mclpclient import mcpconn\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_conversation():\n",
        "    client = None\n",
        "    try:\n",
        "        client = mcpconn(llm_provider=\"openai\", ssl_verify=False)\n",
        "        await client.connect(MCP_SERVER_URL)\n",
        "\n",
        "        # Start a new conversation. This creates a unique ID for the conversation.\n",
        "        conversation_id = client.start_conversation()\n",
        "        print(f\"Started new conversation: {conversation_id}\")\n",
        "\n",
        "        print(\"\\nQuery 1:\")\n",
        "        response1 = await client.query(\"can you provide readme of https://github.com/meta-llama/llama\")\n",
        "        print(f\"AI: {response1}\")\n",
        "\n",
        "        # The second query will have the context of the first one.\n",
        "        print(\"\\nQuery 2:\")\n",
        "        response2 = await client.query(\"how many models are there\")\n",
        "        print(f\"AI: {response2}\")\n",
        "\n",
        "        # You can inspect the history at any time.\n",
        "        history = client.get_conversation_history()\n",
        "        print(\"\\n--- Conversation History ---\")\n",
        "        for message in history:\n",
        "            print(f\"- {message['role']}: {message['content']}\")\n",
        "        print(\"--------------------------\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        if client and client.is_connected():\n",
        "            await client.disconnect()\n",
        "\n",
        "asyncio.run(run_conversation())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}