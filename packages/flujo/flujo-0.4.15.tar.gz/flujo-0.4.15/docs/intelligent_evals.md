# Intelligent Evaluations

This guide explains how to run automated evaluations and use the self-improvement agent introduced in v2.1.

## Quick start

```python
from flujo.application.eval_adapter import run_pipeline_async
from flujo.application.self_improvement import evaluate_and_improve, SelfImprovementAgent
from flujo import Flujo
from flujo.domain import Step
from pydantic_evals import Dataset, Case
from flujo.infra.agents import self_improvement_agent

pipeline = Step.solution(lambda x: x)
runner = Flujo(pipeline)
dataset = Dataset(cases=[Case(inputs="hi", expected_output="hi")])
agent = SelfImprovementAgent(self_improvement_agent)
report = await evaluate_and_improve(
    lambda x: run_pipeline_async(x, runner=runner),
    dataset,
    agent,
)
print(report)
```

The `ImprovementReport` contains structured suggestions for updating your pipeline or evaluation suite.

## ImprovementSuggestion Model

The self-improvement agent returns an `ImprovementReport` which contains a list of `ImprovementSuggestion` objects. Each suggestion has a `suggestion_type` indicating the general category of improvement along with additional fields describing the issue and proposed fix.

```
class ImprovementSuggestion(BaseModel):
    target_step_name: Optional[str]
    suggestion_type: SuggestionType
    failure_pattern_summary: str
    detailed_explanation: str
    prompt_modification_details: Optional[PromptModificationDetail]
    config_change_details: Optional[List[ConfigChangeDetail]]
    example_failing_input_snippets: List[str]
    suggested_new_eval_case_description: Optional[str]
    estimated_impact: Optional[Literal["HIGH", "MEDIUM", "LOW"]]
    estimated_effort_to_implement: Optional[Literal["HIGH", "MEDIUM", "LOW"]]
```

`SuggestionType` values include things like `PROMPT_MODIFICATION`, `CONFIG_ADJUSTMENT`, and `NEW_EVAL_CASE`. For prompt modifications or config adjustments, the relevant detail objects provide the exact change proposed.

## End-to-end Example

1. Define a simple pipeline and dataset with a failing case.
2. Run `flujo improve pipeline.py data.py`.
3. Review the suggestions printed in the formatted table.
4. Apply one of the suggested prompt tweaks.
5. Re-run the evaluation to see the improvement.

Suggestions are advisory and may vary in quality depending on the underlying LLM.

## Context for SelfImprovementAgent

When building the prompt for the self-improvement agent, each step now includes
its `StepConfig` parameters and a redacted summary of the step's system prompt.
This gives the agent more insight into how your pipeline is configured and helps
it provide targeted `CONFIG_ADJUSTMENT` or `PROMPT_MODIFICATION` suggestions.

Example snippet of the context:

```
Case: test_sql_error
- GenerateSQL: Output(content="SELEC * FROM t") (success=True)
  Config(retries=1, timeout=30s, temperature=0.7)
  SystemPromptSummary: "You are a SQL expert..."
```

## Acting on Suggestions: Adding New Evaluation Cases

For suggestions of type `NEW_EVAL_CASE`, use the helper command:

```bash
flujo add-eval-case -d path/to/my_evals.py -n test_new_case -i "user input"
```

The command prints a `Case(...)` definition that you can copy into your dataset
file.

## Configuring the Self-Improvement Agent

The model used by the self-improvement agent can be changed via the
`default_self_improvement_model` setting or overridden at the CLI using
`flujo improve --improvement-model MODEL_NAME`.
### Interpreting Suggestion Types
The `suggestion_type` field indicates how you might act on the advice:
- **PROMPT_MODIFICATION** – adjust the text of a step's system prompt as described.
- **CONFIG_ADJUSTMENT** – tweak temperature or other parameters in the step configuration.
- **NEW_EVAL_CASE / EVAL_CASE_REFINEMENT** – create or refine dataset cases to exercise the pipeline more thoroughly.
- **OTHER** – miscellaneous guidance not captured by the above categories.

### Dataset Best Practices
When authoring evaluation datasets:
- Provide clear `expected_output` values so failures are easy to diagnose.
- Give cases descriptive names using the `name` field.
- Include metadata if extra context helps the agent understand the scenario.

### Limitations
Self‑improvement suggestions are generated by an LLM and should be reviewed
critically. The agent does not modify your code automatically.
