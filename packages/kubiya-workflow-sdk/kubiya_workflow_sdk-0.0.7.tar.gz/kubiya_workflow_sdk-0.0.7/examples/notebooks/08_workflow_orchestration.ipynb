{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Workflow Orchestration Patterns\n",
        "\n",
        "This notebook demonstrates advanced workflow orchestration patterns including:\n",
        "- Parallel execution with concurrency control\n",
        "- Sub-workflows and workflow composition\n",
        "- Queue management and rate limiting\n",
        "- Scheduling and cron-based execution\n",
        "- Lifecycle handlers and notifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from kubiya_workflow_sdk import workflow, step, execute_workflow_logged\n",
        "from kubiya_workflow_sdk.execution import LogLevel\n",
        "from kubiya_workflow_sdk.dsl import retry_policy, when, continue_on\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"KUBIYA_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"KUBIYA_API_KEY is required\")\n",
        "\n",
        "print(\"‚úÖ SDK loaded for orchestration examples\")\n",
        "print(f\"üîó Connected to Kubiya API\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Parallel Execution with Concurrency Control\n",
        "parallel_workflow = (\n",
        "    workflow(\"parallel-processing\")\n",
        "    .description(\"Process multiple files in parallel\")\n",
        "    .max_active_steps(3)  # Limit concurrent steps\n",
        ")\n",
        "\n",
        "# Add parallel steps\n",
        "files = [\"data1.csv\", \"data2.csv\", \"data3.csv\", \"data4.csv\", \"data5.csv\"]\n",
        "parallel_workflow.parallel_steps(\n",
        "    name=\"process-files\",\n",
        "    items=files,\n",
        "    command=\"echo 'Processing ${ITEM}' && sleep 2\",\n",
        "    max_concurrent=2  # Process max 2 files at a time\n",
        ")\n",
        "\n",
        "# Add aggregation step after parallel processing\n",
        "parallel_workflow.step(\n",
        "    \"aggregate-results\",\n",
        "    \"echo 'Aggregating results from all files'\"\n",
        ")\n",
        "\n",
        "print(\"üìã Parallel workflow created with concurrency control\")\n",
        "print(f\"   Files to process: {len(files)}\")\n",
        "print(f\"   Max concurrent: 2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Sub-workflow Composition\n",
        "main_workflow = (\n",
        "    workflow(\"etl-pipeline\")\n",
        "    .description(\"Main ETL pipeline with sub-workflows\")\n",
        "    .params(date=\"${DATE:-$(date +%Y-%m-%d)}\")\n",
        ")\n",
        "\n",
        "# Step 1: Extract data (could be a sub-workflow)\n",
        "main_workflow.step(\n",
        "    \"extract-data\",\n",
        "    \"echo 'Extracting data for date: ${date}'\"\n",
        ")\n",
        "\n",
        "# Step 2: Transform data with sub-workflow\n",
        "main_workflow.sub_workflow(\n",
        "    name=\"transform-data\",\n",
        "    workflow=\"data-transformation\",  # Reference to another workflow\n",
        "    params='{\"input_date\": \"${date}\", \"format\": \"parquet\"}'\n",
        ")\n",
        "\n",
        "# Step 3: Load results\n",
        "main_workflow.step(\n",
        "    \"load-results\",\n",
        "    \"echo 'Loading transformed data to warehouse'\"\n",
        ")\n",
        "\n",
        "print(\"üìã Main workflow with sub-workflow composition created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the parallel workflow with real API\n",
        "print(\"\\nüöÄ Executing parallel workflow...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # Execute the workflow\n",
        "    for event in execute_workflow_logged(\n",
        "        workflow_definition=parallel_workflow.to_dict(),\n",
        "        api_key=api_key,\n",
        "        log_level=LogLevel.NORMAL\n",
        "    ):\n",
        "        pass  # The logger handles output\n",
        "        \n",
        "    print(\"\\n‚úÖ Parallel workflow execution complete!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "    print(\"\\nNote: Parallel execution requires proper runner configuration\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
