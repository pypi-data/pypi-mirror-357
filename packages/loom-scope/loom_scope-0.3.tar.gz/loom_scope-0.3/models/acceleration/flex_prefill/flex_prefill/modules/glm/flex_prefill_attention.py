# Copyright 2024 GLM-4-9B Model Team @ Zhipu AI
# Copyright 2024 ByteDance and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch ChatGLM model."""

from ...ops.flex_prefill_attention import flex_prefill_attention


def glm_flex_prefill_attention_forward(
    self, query_states, key_states, value_states, attention_mask
):
    query_states = query_states.transpose(1, 2)
    key_states = key_states.transpose(1, 2)
    value_states = value_states.transpose(1, 2)
    batch_size, query_length = query_states.shape[:2]

    # you can use flex_prefill_attention both in prefilling and decoding, which will use full attention while in decoding phase
    attn_output = flex_prefill_attention(
        query_states,
        key_states,
        value_states,
        gamma=self.config.flex_prefill_gamma,
        tau=self.config.flex_prefill_tau,
        block_size=self.config.block_size,
        min_budget=getattr(self.config, "flex_prefill_min_budget", None),
        max_budget=getattr(self.config, "flex_prefill_max_budget", None),
    )

    attn_output = attn_output.reshape(
        batch_size, query_length, self.hidden_size_per_partition
    ).contiguous()
    return attn_output
