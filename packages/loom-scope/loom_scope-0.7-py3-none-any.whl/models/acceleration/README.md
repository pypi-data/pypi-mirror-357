# ðŸš„ Acceleration

[English Version](./README.md) | [ä¸­æ–‡ç‰ˆæœ¬](./README_zh.md) | [ä¸»é¡µ](../../README.md)

Welcome to the **Acceleration Toolkit**! This repository provides a comprehensive suite of cutting-edge acceleration methods designed to optimize the performance of various Large Language Models (LLMs). Whether you're working on inference or training, this toolkit offers a range of advanced techniques to significantly speed up your workflows.

All supported acceleration methods are fully optimized for multi-GPU inference and compatible with high-performance GPUs including **NVIDIA 3090**, **A-series**, and **H-series** cards, ensuring seamless scalability across different hardware configurations. Note that **XAttention** is specifically optimized for A100 GPUs.

With the Acceleration Toolkit, you can unlock the full potential of your models while reducing computational overhead and resource consumption. Letâ€™s accelerate your AI journey!

## ðŸ” Overview

The Acceleration Toolkit currently supports the following acceleration methods:

| **Acceleration Type**     | **Method**                                          | **Remark**                                     |
| :------------------------------ | :-------------------------------------------------------- | :--------------------------------------------------- |
| **Token Eviction**        | [H2O](https://github.com/FMInference/H2O)                    | Attention-based selection                            |
|                                 | [StreamingLLM](https://github.com/mit-han-lab/streaming-llm) | Retain first few tokens                              |
|                                 | [SnapKV](https://github.com/FasterDecoding/SnapKV)           | Attention Pooling before selection                   |
|                                 | [L2Compress](https://github.com/alessiodevoto/l2compress)    | L2 Norm is better than attention as a metrics        |
| **Layer-wise**            | [PyramidKV](https://github.com/Zefan-Cai/KVCache-Factory)    | Layer-wise budget allocation                         |
|                                 | [CakeKV](https://github.com/antgroup/cakekv)                 | Layer-specific preference score                      |
| **Quantization**          | [KIVI](https://github.com/jy-yuan/KIVI)                      | Asymmetric 2bit Quantization                         |
| **Quantization+Eviction** | [ThinK](https://github.com/SalesforceAIResearch/ThinK)       | Thinner Key Cache by Query-Driven Pruning            |
| **Token Merge**           | [CaM](https://github.com/zyxxmu/cam)                         | Cache Merging for Memory-efficient LLMs Inference    |
| **Sparse Attention**             | [FlexPrefill](https://github.com/bytedance/FlexPrefill)      | Dynamic and context-aware sparse attention mechanism |
|             | [XAttention](https://github.com/mit-han-lab/x-attention)      | Dynamic and context-aware sparse attention mechanism by the Strided Antidiagonal Scoring|

## ðŸ’» Environment & Installation

### > General Installation

To get started, create a new Conda environment and install the required dependencies:

```bash
conda create -n acceleration python=3.10.16
conda activate acceleration
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 -f https://mirrors.aliyun.com/pytorch-wheels/cu121
pip install transformers==4.44.2
pip install ninja
pip install flash-attn==2.5.6 --no-build-isolation
pip install -r requirements.txt
```

### > FlexPrefill Installation

If you plan to use **FlexPrefill**, you'll need to install additional packagesï¼ˆflex_prefill/install.shï¼‰:

```bash
conda create -n FlexPrefill python=3.10.16
conda activate FlexPrefill
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -f https://mirrors.aliyun.com/pytorch-wheels/cu121
pip install triton==3.0.0 transformers==4.44.0  vllm==0.5.4
pip install ninja
pip install flash_attn==2.6.3 --no-build-isolation
pip install nemo-toolkit[all]==1.21 --no-deps -i https://pypi.tuna.tsinghua.edu.cn/simple
pip3 install Cython==3.0.11 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -r ./models/acceleration/flexprefill_requirements.txt
```

Additionally, download the appropriate version of `flashinfer` from [flashinfer.ai/whl](https://flashinfer.ai/whl) like ``pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/``.

### > XAttention Installation

If you plan to use **XAttention**:

```bash
conda create -yn xattn python=3.10
conda activate xattn

conda install -y git
conda install -y nvidia/label/cuda-12.4.0::cuda-toolkit
conda install -y nvidia::cuda-cudart-dev
conda install -y pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia

pip install transformers==4.46 accelerate sentencepiece minference datasets wandb zstandard matplotlib huggingface_hub==0.23.2 torch torchaudio torchvision xformers  vllm==0.6.3.post1 vllm-flash-attn==2.6.1
pip install tensor_parallel==2.0.0

pip install ninja packaging
pip install flash-attn==2.6.3 --no-build-isolation
pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/

# LongBench evaluation
pip install seaborn rouge_score einops pandas


# Install xAttention
pip install -e .

# Install Block Sparse Streaming Attention
git clone https://github.com/mit-han-lab/Block-Sparse-Attention.git
cd Block-Sparse-Attention
python setup.py install
cd ..

export PYTHONPATH="$PYTHONPATH:$(pwd)"

```

## ðŸš€ Quick Start

### > General Acceleration

To use **KIVI** for accelerated inference:

```bash
cd KIVI/quant
pip install -e .
```

To use **ThinK** for accelerated inference:

```bash
cd think/load_tiny_cuda_resource
make i
```

> Note: Other general acceleration methods do not require additional installation.

### > FlexPrefill

For **FlexPrefill**, simply run:

```bash
cd flex_prefil
pip install -e .
```

### > Quick Start

You can easily enable any of these methods by specifying the `--acceleration` flag followed by the method name, e.g., `--acceleration L2Compress`.

| **Category**        | **Parameter** | **Type/Format** | **Description**                                               | **Required/Default**                                                                                                                |
| :------------------------ | :------------------ | :-------------------- | :------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------- |
| **Core Parameters** |                     |                       |                                                                     |                                                                                                                                           |
|                           | `--model_path`    | `str`               | Model path/service provider (e.g., Meta-Llama-3-70B-Instruct)       | Required                                                                                                                                  |
|                           | `--cfg_path`      | `str` (YAML)        | Benchmark config file (e.g., benchmarks/General/LongBench.yaml)     | Required                                                                                                                                  |
|                           | `--device`        | `List[int]`         | GPU indices (e.g.,`0 1 3`)                                        | Default: All available GPUs                                                                                                               |
|                           | `--torch_dtype`   | `str \| torch.dtype` | Precision (e.g.,`float16`, `bfloat16`)                          | Default:`torch.bfloat16`                                                                                                                |
| **GPU Allocation**  |                     |                       |                                                                     |                                                                                                                                           |
|                           | `--gp_num`        | `int \| str`         | GPUs per model (e.g.,`2`) *or* models per GPU (e.g., `'1/2'`) | Default: 1                                                                                                                                |
| **Optimization**    |                     |                       |                                                                     |                                                                                                                                           |
|                           | `--acceleration`  | `str`               | Experimental methods (`H2O`, `SnapKV`, `StreamingLLM`, ...)   | Choice: {â€œH2Oâ€, â€œStreamingLLMâ€, â€œSnapKVâ€, â€œL2Compressâ€, â€œFlexPrefillâ€, â€œPyramidKVâ€, â€œCakeKVâ€, â€œKIVIâ€, â€œThinKâ€, â€œCaMâ€} |

## ðŸ¤– Supported Models

The Acceleration Toolkit supports a wide range of models:

- **All Methods Except FlexPrefill**: Supports `llama` and `mistral` series models.
- **FlexPrefill**: Supports `llama`, `glm`, and `qwen2` series models.
- **CakeKV**: Supports `llama`, `mistral`, and `qwen2` series models.
- **XAttention**: Supports `llama` series models.

### âš ï¸ Precautions

Before running inference, please ensure you customize the relevant configuration files in the `acceleration/xxx/config` directory to match your specific requirements. For example, you may need to adjust settings in files such as `.models/acceleration/cake/config/cake_config.json`.

### âš ï¸ Some tips

All acceleration methods except L2Compress can be used for inference tasks with a length of 128K on a single 40GB A100 card. L2Compress is recommended for three-card inference because during KV cache compression, it skips the shallow layers and preserves their complete KV cache, resulting in higher VRAM usage compared to other methods for long-text tasks. If the VRAM size is only 24GB, multi-card inference is recommended in all cases.

## ðŸ“Š Experimental Results

Here we present the performance metrics of acceleration methods tested on Llama-3.1-8B-instruct and RULER across different GPU hardware (NVIDIA 3090, 40GB A100, and H20):

### > Reasoning Time

Experimental settings is:

```ymal
benchmark_name: RULER
task_names: ["cwe","niah_multikey_1", "niah_multikey_2", "niah_multikey_3", "niah_multiquery", "niah_multivalue", "niah_single_1", "niah_single_2", "niah_single_3", "qa_1", "qa_2", "vt"]
length: [4096,8192,16384,32768,65536,131072] 

num_samples: 10 # Set the number of samples for each task to num_samples
no_template_tasks: ["fwe"]  # task_names or "all" 
template: NULL  # choose from ./models/utils/build_chat.py
```

The dash '-' indicates Out Of Memory (OOM) errors. Among these methods, CaM cannot effectively handle long-text inference tasks, while FlexPrefill demonstrates the best performance and speed across all tested scenarios.

#### Performance on NVIDIA 3090

| Sequence Length | KIVI    | H2O     | StreamingLLM | L2Compress | CaM      | CakeKV  | PyramidKV | FlexPrefill | SnapKV  | ThinK   | Transformers | vLLM    |
| --------------- | ------- | ------- | ------------ | ---------- | -------- | ------- | --------- | ----------- | ------- | ------- | ------------ | ------- |
| 4K              | 94.83   | 46.13   | 27.79        | 36.42      | 67.39    | 91.88   | 77.42     | 93.33       | 81.54   | 81.83   | 94.83        | 93.67   |
| 8K              | 94.29   | 34.42   | 22.21        | 27.75      | 59.60    | 82.42   | 71.25     | 87.92       | 73.29   | 72.88   | 94.42        | 93.04   |
| 16K             | 91.08   | 21.63   | 11.46        | 27.58      | 48.53    | 78.96   | 68.75     | 83.38       | 67.88   | 67.25   | 92.50        | 91.50   |
| 32K             | 89.5    | 19.67   | 10.79        | 22.17      | 43.42    | 79.54   | 64.33     | 83.38       | 66.29   | 66.21   | 92.75        | 88.17   |
| 64K             | -       | 10.54   | 7.67         | 18.25      | 33.25    | 62.88   | 57.29     | 65.04       | 55.58   | 56.92   | 83.71        | 78.71   |
| 128K            | -       | 6.38    | 9.79         | 12.17      | 28.10    | 53.17   | 48.29     | 46.58       | 47.25   | 48.71   | -            | 16.67   |
| Reasoning time  | 1:29:58 | 6:11:21 | 6:07:27      | 6:08:31    | 50:31:13 | 6:13:01 | 6:14:12   | 3:58:17     | 6:08:31 | 6:18:08 | 3:11:15      | 4:47:37 |

#### Performance on 40GB A100

| Sequence Length | KIVI    | H2O     | StreamingLLM | L2Compress | CaM      | CakeKV  | PyramidKV | FlexPrefill | SnapKV  | ThinK   | Transformers | vLLM    |
| --------------- | ------- | ------- | ------------ | ---------- | -------- | ------- | --------- | ----------- | ------- | ------- | ------------ | ------- |
| 4K              | 95.46   | 46.38   | 28.83        | 33.96      | 67.29    | 89.67   | 77.00     | 93.58       | 79.75   | 79.33   | 95.04        | 90.71   |
| 8K              | 94.42   | 31.96   | 21.58        | 26.83      | 59.67    | 82.46   | 70.79     | 89.04       | 72.67   | 71.92   | 94.46        | 91.63   |
| 16K             | 90.04   | 23.00   | 10.63        | 27.92      | 48.53    | 77.29   | 67.54     | 84.83       | 66.63   | 65.25   | 92.50        | 91.75   |
| 32K             | 89.00   | 19.86   | 10.79        | 20.83      | 43.21    | 76.42   | 65.25     | 79.63       | 63.88   | 66.46   | 92.08        | 86.25   |
| 64K             | 90.25   | 10.50   | 7.67         | 17.25      | 33.35    | 65.79   | 55.54     | 62.50       | 54.75   | 55.75   | 83.71        | 78.83   |
| 128K            | 67.92   | 6.17    | 10.63        | 12.17      | 28.03    | 55.21   | 49.54     | 49.42       | 50.79   | 51.00   | 75.21        | 17.50   |
| Reasoning time  | 3:00:05 | 2:50:26 | 2:44:12      | 2:56:36    | 25:41:21 | 2:58:21 | 2:57:54   | 1:52:41     | 2:52:51 | 3:09:58 | 3:23:10      | 2:15:38 |
| Batch size=8    | 0:32:57 | 0:27:00 | 0:26:35      | 0:27:26    | -        | 0:28:26 | 0:26:39   | 0:15:58     | 0:26:44 | 0:35:46 | -            | 0:27:20 |

#### Performance on NVIDIA H20

| Sequence Length | KIVI    | H2O     | StreamingLLM | L2Compress | CaM      | CakeKV  | PyramidKV | FlexPrefill | SnapKV  | ThinK   | Transformers | vLLM    |
| --------------- | ------- | ------- | ------------ | ---------- | -------- | ------- | --------- | ----------- | ------- | ------- | ------------ | ------- |
| 4K              | 93.79   | 54.33   | 28.00        | 34.21      | 66.29    | 89.67   | 77.13     | 92.63       | 82.22   | 80.83   | 94.00        | 91.75   |
| 8K              | 94.33   | 41.25   | 21.58        | 26.75      | 59.67    | 82.63   | 71.50     | 87.88       | 70.46   | 72.33   | 94.46        | 88.79   |
| 16K             | 90.71   | 24.17   | 10.63        | 27.00      | 48.33    | 77.21   | 65.33     | 84.54       | 64.57   | 65.92   | 92.50        | 84.58   |
| 32K             | 88.08   | 19.42   | 10.79        | 22.58      | 43.42    | 76.42   | 66.21     | 80.71       | 69.27   | 66.46   | 91.88        | 79.50   |
| 64K             | 80.91   | 12.33   | 6.83         | 16.58      | 34.25    | 65.71   | 58.25     | 62.96       | 57.73   | 56.25   | 83.71        | 77.79   |
| 128K            | 69.13   | 10.92   | 10.63        | 12.25      | 28.13    | 54.21   | 47.58     | 48.08       | 45.94   | 47.25   | 75.21        | 57.88   |
| Reasoning time  | 3:31:35 | 3:20:18 | 3:21:25      | 3:29:34    | 26:11:23 | 3:22:06 | 3:22:05   | 1:49:35     | 3:26:09 | 3:35:32 | 3:57:44      | 2:22:45 |
| Batch size=8    | 0:33:44 | 0:33:08 | 0:32:35      | 00:33:48   | -        | 0:32:37 | 0:32:41   | 0:13:10     | 0:32:41 | 0:34:25 | -            | 0:38:56 |
