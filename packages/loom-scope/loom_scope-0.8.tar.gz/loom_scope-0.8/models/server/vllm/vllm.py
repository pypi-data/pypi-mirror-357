import sys
import time
from copy import deepcopy

from loguru import logger
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

from ...base.AccelerationModel import AccelerationModel
from ...utils.build_chat import RetrieveModelBuildChat

logger.remove()
logger.add(sys.stdout,colorize=True,format="<level>{message}</level>")
class Vllm(AccelerationModel):
    def __init__(self,model_path,torch_dtype,adapter_path,gp_num,max_model_len,gpu_memory_utilization,**kwargs):
        super().__init__(**kwargs)
        self.model_path= model_path
        self.torch_dtype = torch_dtype
        self.gp_num = gp_num
        self.adapter_path = adapter_path
        self.params_dict = {}
        self.max_model_len = max_model_len
        self.gpu_memory_utilization = gpu_memory_utilization
   
    def deploy(self):
        time_start = time.time()
        
        tokenizer =  AutoTokenizer.from_pretrained(self.model_path,mean_resizing=False,trust_remote_code=True)
        # Load the model and tokenizer
        if self.max_model_len:
            model = LLM(model=self.model_path,trust_remote_code = True,tensor_parallel_size=self.gp_num,max_model_len = self.max_model_len,gpu_memory_utilization=self.gpu_memory_utilization,swap_space = 4, enforce_eager=True)
        else:
            model = LLM(model=self.model_path,trust_remote_code = True,tensor_parallel_size=self.gp_num,gpu_memory_utilization=self.gpu_memory_utilization,swap_space = 4, enforce_eager=True)
        # Check and add pad token if necessary
        logger.info("Model and tokenizer initialized.",flush=True)
        time_cost = time.time()-time_start
        logger.info("Model_deploy time :{}".format(time_cost),flush=True )
        return model,tokenizer
        
    def updata_params(self,params,tokenizer,enable_thinking):
        
        params_ = deepcopy(self.params_dict)
        params_.update(params)
        del_list = ["do_sample","num_beams","min_length","max_length"]
        for i in del_list:
            if i in params:params_.pop(i)
        if "temperature" in params_ and not params_["temperature"]:params_["temperature"] = 0.0
        del_list = []
        for i in params_:
            if not params_[i]:del_list.append(i)
        for i in del_list: params_.pop(i)
        # return {"temperature":0.7, "top_p":0.95,"max_tokens":10000}
        return params_ 


    def generate(self,model,tokenizer,input,params,**kwargs):
        outputs = model.generate(input,SamplingParams(**params))
        outputs = outputs[0].outputs[0].text
        if "</think>" not in outputs:
            return outputs 
        else:
            outputs = outputs.split("</think>")[-1].strip()
            return outputs



        








  








  
