


import os
import sys
import time
from copy import deepcopy

import torch
import yaml
from loguru import logger
from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS

logger.remove();logger.add(sys.stdout,colorize=True, format="<level>{message}</level>")

os.environ['RWKV_JIT_ON'] = '1' # '1' for better speed
os.environ["RWKV_CUDA_ON"] = '1' # '1' to compile CUDA kernel (10x faster), requires c++ compiler & cuda libraries
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cuda.matmul.allow_tf32 = True
class Rwkv():
    def __init__(self,model_path,torch_dtype,adapter_path,**kwargs):
        self.model_path= model_path
        self.torch_dtype = torch_dtype
        self.adapter_path = adapter_path
        self.params_dict = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7, top_k = 100, # top_k = 0 then ignore
                     alpha_frequency = 0.25,
                     alpha_presence = 0.25,
                     alpha_decay = 0.996, # gradually decay the penalty
                     token_ban = [], # ban the generation of some tokens
                     token_stop = [], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)
        with open("./models/server/RNN/config.yaml") as f:
            self.config = yaml.safe_load(f)
    def deploy(self):
        time_start = time.time()
        model = RWKV(model=self.model_path, strategy=self.config["strategy"]).to("cuda")
        self.pipeline = PIPELINE(model, "rwkv_vocab_v20230424") if "world" in self.model_path.lower() else PIPELINE(model, "20B_tokenizer.json")# for "world" models

        tokenizer = self.pipeline.tokenizer
        logger.info("Model and tokenizer initialized.",flush=True )
        time_cost = time.time()-time_start
        logger.info("Model_deploy time :{}".format(time_cost),flush=True)
        return model,tokenizer
    def updata_params(self,params,tokenizer,enable_thinking):
        params_ = deepcopy(self.params_dict)
        for key in params:
            if hasattr(params_, key) and params[key]:
                setattr(params_, key, params[key])
        if "stop" in params:
            setattr(params_, "token_stop", tokenizer.encode("{}".format(params["stop"][0])))
        return {"params":params_,"token_count":params["max_tokens"]}
    def generate(self,input,params,**kwargs):
        pred = self.pipeline.generate(input, token_count=params["token_count"], args=params["params"])
        return pred




