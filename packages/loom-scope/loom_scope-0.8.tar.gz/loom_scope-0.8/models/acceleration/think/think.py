import functools
import json
import sys
import time
import types
from copy import deepcopy

import torch
import transformers
from loguru import logger
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from transformers.models.llama.modeling_llama import (
    LlamaMLP,
)

from ...base.AccelerationModel import AccelerationModel
from .think_utils.monkey_patch import replace_llama

logger.remove()
logger.add(sys.stdout, colorize=True, format="<level>{message}</level>")


class think(AccelerationModel):
    def __init__(self, model_path, torch_dtype, adapter_path, **kwargs):
        super().__init__()
        self.model_path = model_path
        self.torch_dtype = torch_dtype
        self.adapter_path = adapter_path
        self.params_dict = {}

    def deploy(self):
        if "llama" in self.model_path.lower():
            replace_llama()
        else:
            raise (
                "Model not supported by snapkv.Please refer to the documentation at './models/acceleration/readme.md'."
            )
        time_start = time.time()
        # Load the model and tokenizer
        model_config = AutoConfig.from_pretrained(
            self.model_path, trust_remote_code=True
        )

        with open("./models/acceleration/think/config/think_config.json", "r") as f:
            config_dict = json.load(f)
            model_config.max_capacity_prompt = config_dict["max_capacity_prompt"]
            model_config.window_size = config_dict["window_size"]
            model_config.kernel_size = config_dict["kernel_size"]
            model_config.pooling = config_dict["pooling"]
            model_config.recent_size = config_dict["recent_size"]
            model_config.ratio = config_dict["ratio"]

        if "long" in self.model_path.lower() or "mamba" in self.model_path.lower():
            model = transformers.AutoModelForCausalLM.from_pretrained(
                self.model_path,
                config=model_config,
                device_map="auto",
                torch_dtype=eval(self.torch_dtype),
                trust_remote_code=True,
            ).eval()
        else:
            model = transformers.AutoModelForCausalLM.from_pretrained(
                self.model_path,
                config=model_config,
                attn_implementation="flash_attention_2",
                device_map="auto",
                torch_dtype=eval(self.torch_dtype),
                trust_remote_code=True,
            ).eval()
            if "llama" in self.model_path.lower():
                from .think_utils.causal_model_forward import (
                    llama_causal_model_forward,
                )

                # multiple gpus inference using Accelerate
                if isinstance(model.forward, functools.partial):
                    model.forward.__wrapped__ = types.MethodType(
                        llama_causal_model_forward, model
                    )
                else:
                    model.forward = types.MethodType(llama_causal_model_forward, model)

                from .think_utils.llama_mlp_forward import llama_mlp_forward

                for idx, m in model.named_modules():
                    if isinstance(m, LlamaMLP):
                        m.forward = types.MethodType(llama_mlp_forward, m)
        if self.adapter_path:
            from peft import PeftModelForCausalLM

            model = PeftModelForCausalLM.from_pretrained(model, self.adapter_path)

        tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, mean_resizing=False, trust_remote_code=True
        )
        logger.info("Model and tokenizer initialized.", flush=True)
        time_cost = time.time() - time_start
        logger.info("Model_deploy time :{}".format(time_cost), flush=True)
        return model, tokenizer
