import torch
from flex_prefill import patch_model
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "/opt/data/private/hf_models/Llama-3.1-8B-Instruct"
)

model = AutoModelForCausalLM.from_pretrained(
    "/opt/data/private/hf_models/Llama-3.1-8B-Instruct",
    torch_dtype=torch.bfloat16,
    _attn_implementation="flash_attention_2",
    device_map="auto",
).eval()

flex_prefill_config = {
    "block_size": 128,
    "flex_prefill_gamma": 0.9,
    "flex_prefill_tau": 0.1,
    "flex_prefill_min_budget": 512,
    "flex_prefill_max_budget": None,
}

patch_model(model, "flex_prefill", flex_prefill_config)

input_ids = tokenizer(
    """Traceback (most recent call last):
  File "/opt/data/private/sora/lab/longtexteval/scripts/run.py", line 142, in <module>
    main()
  File "/opt/data/private/sora/lab/longtexteval/scripts/run.py", line 135, in main
    evaluate_benchmark(args.folder_name,args.model_name,args.cfg_path)
  File "/opt/data/private/sora/lab/longtexteval/scripts/eval.py", line 20, in evaluate_benchmark
    benchmark.evaluate_data(DataPath=folder_path,OutPath=results_path,model_name=model_name)
  File "/opt/data/private/sora/lab/longtexteval/benchmarks/Reasoning/babilong/babilong.py", line 74, in evaluate_data
    evaluate(DataPath,OutPath,self.config)
  File "/opt/data/private/sora/lab/longtexteval/benchmarks/Reasoning/babilong/utils/evaluation/eval.py", line 33, in evaluate
    save_results_xlsx(OutPath,"task",scores_task,config["task_names"])
  File "/opt/data/private/sora/lab/longtexteval/benchmarks/Reasoning/babilong/utils/evaluation/eval.py", line 15, in save_results_xlsx
    ConstructResultTable(f"babilong_{category}",scores,OutPath+f"/results_{category}.xlsx",order_list)
  File "/opt/data/private/sora/lab/longtexteval/benchmarks/utils/ConstructResultTable.py", line 33, in ConstructResultTable
    task_name_max_len = max(len(task) for tasks in data.values() for task in tasks.keys())
ValueError: max() arg is an empty sequence""",
    return_tensors="pt",
    return_attention_mask=False,
).input_ids.to(model.device)
output_ids = model.generate(input_ids, max_new_tokens=64)
output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(f"=================={output}======================")