# Copyright 2024 GLM-4-9B Model Team @ Zhipu AI
# Copyright 2024 ByteDance and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch ChatGLM model."""

from flash_attn import flash_attn_func


def glm_flash_attention_forward(
    self, query_states, key_states, value_states, attention_mask
):
    query_states = query_states.transpose(1, 2)
    key_states = key_states.transpose(1, 2)
    value_states = value_states.transpose(1, 2)
    batch_size, query_length = query_states.shape[:2]

    if query_length > 1:
        attn_output = flash_attn_func(
            query_states, key_states, value_states, causal=self.is_causal
        )
    else:
        attn_output = flash_attn_func(query_states, key_states, value_states)
    attn_output = attn_output.reshape(
        batch_size, query_length, self.hidden_size_per_partition
    ).contiguous()
    return attn_output
