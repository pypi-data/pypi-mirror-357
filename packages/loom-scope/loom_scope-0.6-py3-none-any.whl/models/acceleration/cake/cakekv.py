
import functools
import json
import sys
import time
import types
from copy import deepcopy

import torch
import transformers
from loguru import logger
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from transformers.models.llama.modeling_llama import (
    LlamaForCausalLM,
    LlamaMLP,
)

from ...base.AccelerationModel import AccelerationModel
from .cake.cake_cache import CakeprefillKVCache
from .cake.utils import CompressConfig

logger.remove()
logger.add(sys.stdout, colorize=True, format="<level>{message}</level>")


class cakekv(AccelerationModel):
    def __init__(self, model_path, torch_dtype, adapter_path, **kwargs):
        super().__init__()
        self.model_path = model_path
        self.torch_dtype = torch_dtype
        self.adapter_path = adapter_path
        self.params_dict = {}

    def deploy(self):
        if "llama" in self.model_path.lower():
            from .cake.monkeypatch import replace_flashllama_attn_with_cakeattn

            replace_flashllama_attn_with_cakeattn()
        elif "mistral" in self.model_path.lower():
            from .cake.monkeypatch import replace_flashmistral_attn_with_cakeattn

            replace_flashmistral_attn_with_cakeattn()
        elif "qwen2" in self.model_path.lower():
            from .cake.monkeypatch import replace_flashqwen2_attn_with_cakeattn

            replace_flashqwen2_attn_with_cakeattn()
        else:
            raise (
                "Model not supported by snapkv.Please refer to the documentation at './models/acceleration/readme.md'."
            )
        if "qwen2" in self.model_path.lower():
            self.torch_dtype = torch.bfloat16
        else:
            self.torch_dtype = torch.float16
        time_start = time.time()
        # Load the model and tokenizer
        model_config = AutoConfig.from_pretrained(
            self.model_path, trust_remote_code=True
        )
        if hasattr(model_config, "num_hidden_layers"):
            layers = model_config.num_hidden_layers

        with open("./models/acceleration/cake/config/cake_config.json", "r") as f:
            cake_config = json.load(f)
            compress_config = CompressConfig(
                cake_config["compress"], cake_config["cascading"]
            )
            if cake_config["compress"] == "True":
                compress_config.cache_size = cake_config["cache_size"]
                compress_config.window_size = cake_config["window_size"]
                with open(
                    "./models/acceleration/cake/config/model2tau.json", "r"
                ) as file:
                    model2tau = json.load(file)
                try:
                    tau1 = model2tau[cake_config["model_name"]][
                        f"{cake_config['cache_size']}"
                    ]["tau1"]
                    tau2 = model2tau[cake_config["model_name"]][
                        f"{cake_config['cache_size']}"
                    ]["tau2"]
                except Exception as e:
                    print(f"Error loading tau values: {e}")
                    tau1, tau2 = 1.0, 1.0
                gamma = cake_config["gamma"]
                hyper = [tau1, tau2, gamma]
                compress_config.hyper = hyper
        

        if "long" in self.model_path.lower() or "mamba" in self.model_path.lower():
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                config=model_config,
                device_map="auto",
                torch_dtype=self.torch_dtype,
                trust_remote_code=True,
            ).eval()
        else:
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                config=model_config,
                attn_implementation="flash_attention_2",
                device_map="auto",
                torch_dtype=self.torch_dtype,
                trust_remote_code=True,
            ).eval()
            if "llama" in self.model_path.lower():
                from .cake.model.causal_model_forward import (
                    llama_causal_model_forward,
                )

                # multiple gpus inference using Accelerate
                if isinstance(model.forward, functools.partial):
                    model.forward.__wrapped__ = types.MethodType(llama_causal_model_forward, model)
                else:
                    model.forward = types.MethodType(llama_causal_model_forward, model)

                from .cake.model.llama_mlp_forward import llama_mlp_forward

                for idx, m in model.named_modules():
                    if isinstance(m, LlamaMLP):
                        m.forward = types.MethodType(llama_mlp_forward, m)
        if self.adapter_path:
            from peft import PeftModelForCausalLM

            model = PeftModelForCausalLM.from_pretrained(model, self.adapter_path)
        for i in range(layers):
            model.model.layers[i].self_attn.config.key_size = [
                compress_config.cache_size - compress_config.window_size
            ] * layers
            model.model.layers[i].self_attn.config.window_size = [
                compress_config.window_size
            ] * layers
            model.model.layers[i].self_attn.config.prefill = [True] * layers
            model.model.layers[i].self_attn.config.decoding_evict = [None] * layers
            model.model.layers[i].self_attn.config.tau1 = compress_config.hyper[0]
            model.model.layers[i].self_attn.config.tau2 = compress_config.hyper[1]
            model.model.layers[i].self_attn.config.gamma = compress_config.hyper[2]
            model.model.layers[i].self_attn.config.prefill_cake_evict = [
                CakeprefillKVCache(
                    cache_size=compress_config.cache_size,
                    window_size=compress_config.window_size,
                    k_seq_dim=2,
                    v_seq_dim=2,
                    num_heads=model.model.layers[i].self_attn.num_heads,
                    num_layers=layers,
                    use_cascading=compress_config.cascading,
                )
            ] * layers
        tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, mean_resizing=False, trust_remote_code=True
        )
        logger.info("Model and tokenizer initialized.", flush=True)
        time_cost = time.time() - time_start
        logger.info("Model_deploy time :{}".format(time_cost), flush=True)
        return model, tokenizer
