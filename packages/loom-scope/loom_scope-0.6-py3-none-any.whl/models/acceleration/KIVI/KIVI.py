import json
import sys
import time
from copy import deepcopy

import torch
import transformers
from accelerate import (
    infer_auto_device_map,
    init_empty_weights,
    load_checkpoint_and_dispatch,
)
from loguru import logger
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

from ...base.AccelerationModel import AccelerationModel

logger.remove()
logger.add(sys.stdout, colorize=True, format="<level>{message}</level>")


class KIVI(AccelerationModel):
    def __init__(self, model_path, torch_dtype, adapter_path, **kwargs):
        super().__init__()
        self.model_path = model_path
        self.torch_dtype = torch_dtype
        self.adapter_path = adapter_path
        self.params_dict = {}

    def deploy(self):
        if "llama" in self.model_path.lower():
            print("use kivi")
            from .models.llama_kivi import LlamaForCausalLM_KIVI
        elif "mistral" in self.model_path.lower():
            print("use kivi")
            from .models.mistral_kivi import MistralForCausalLM_KIVI
        else:
            raise (
                "Model not supported by snapkv.Please refer to the documentation at './models/acceleration/readme.md'."
            )
        time_start = time.time()
        # Load the model and tokenizer
        model_config = AutoConfig.from_pretrained(
            self.model_path, trust_remote_code=True
        )
        with open("./models/acceleration/KIVI/config/kivi_config.json", "r") as f:
            config_dict = json.load(f)
            model_config.k_bits = config_dict[
                "k_bits"
            ]  # KiVi currently support 2/4 K/V bits
            model_config.v_bits = config_dict["v_bits"]
            model_config.group_size = config_dict["group_size"]
            model_config.residual_length = config_dict[
                "residual_length"
            ]  # corresponding to the number of recent fp16 tokens
            model_config.use_flash = config_dict["use_flash"]
            model_config.use_flash = config_dict["use_sdqa"]
        if "long" in self.model_path.lower() or "mamba" in self.model_path.lower():
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                config=model_config,
                torch_dtype=eval(self.torch_dtype),
                device_map="auto",
                trust_remote_code=True,
            ).eval()
        elif "mistral" in self.model_path.lower():
            with init_empty_weights():
                model = MistralForCausalLM_KIVI.from_pretrained(
                    self.model_path,
                    config=model_config,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                ).eval()
        else:
            with init_empty_weights():
                model = LlamaForCausalLM_KIVI.from_pretrained(
                    self.model_path,
                    config=model_config,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                ).eval()
                if "llama" in self.model_path.lower():
                    from .models.llama_mlp_forward import llama_mlp_forward
                    import types
                    from transformers.models.llama.modeling_llama import (
                        LlamaMLP,
                    )

                    for idx, m in model.named_modules():
                        if isinstance(m, LlamaMLP):
                            m.forward = types.MethodType(llama_mlp_forward, m)
        if self.adapter_path:
            from peft import PeftModelForCausalLM

            model = PeftModelForCausalLM.from_pretrained(model, self.adapter_path)
        device_map = infer_auto_device_map(
            model, no_split_module_classes=["LlamaDecoderLayer_KIVI"]
        )
        model = load_checkpoint_and_dispatch(
            model,
            checkpoint=self.model_path,
            device_map=device_map,
            offload_folder=None,
            dtype=torch.float16,
        )
        tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, trust_remote_code=True
        )
        logger.info("Model and tokenizer initialized.", flush=True)
        time_cost = time.time() - time_start
        logger.info("Model_deploy time :{}".format(time_cost), flush=True)
        return model, tokenizer
