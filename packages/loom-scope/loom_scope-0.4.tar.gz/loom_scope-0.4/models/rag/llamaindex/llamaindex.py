import sys

import torch
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.readers.string_iterable import StringIterableReader
from loguru import logger

from models.rag.bass_class import Base

logger.remove();logger.add(sys.stdout, colorize=True, format="<level>{message}</level>")

class llamaindex(Base):
    def __init__(self,config,devices,index):
        super().__init__(config)
        self.retrieval_methods = "llamaindex"
        self.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5",device=f"cuda:{devices[index%len(devices)]}")

    def retrieve(self, context, question):
        return self.retrieve_relevant_chunks_for_question(context, question)

    def retrieve_relevant_chunks_for_question(self, context, question):
        document = StringIterableReader().load_data(texts=[context])
        parser = SimpleNodeParser.from_defaults(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap)
        nodes = parser.get_nodes_from_documents(document)
        index = VectorStoreIndex(nodes, embed_model=self.embed_model)
        
        retriever = index.as_retriever(
            similarity_top_k=self.num_chunks,
            include_metadata=False )
        retrieved_nodes = retriever.retrieve(question)
        return [node.get_content() for node in retrieved_nodes]