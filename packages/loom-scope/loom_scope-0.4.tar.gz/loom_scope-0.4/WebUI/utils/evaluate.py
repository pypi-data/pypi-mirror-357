import gradio as gr
import os,sys, json, shutil
import subprocess
from filelock import FileLock
# from watchfiles import watch

from ansi2html import Ansi2HTMLConverter
from copy import deepcopy

conv = Ansi2HTMLConverter(inline=True)

import time
from WebUI.benchmarks.constant import (
    BASE_CMD, ROOT_PATH, VLLM_LOG_END_CHAR, TMP_PATH,
    BENCHMARK_NAMES,
    CURRENT_CONSOLE_INDEX_PATH
)
from WebUI.benchmarks.pages import PAGES, PAGES_DICT
from WebUI.utils.status import save_all
from loguru import logger

from collections import defaultdict

class EmptyProcess:
    def poll(self):return 1

processes = defaultdict(EmptyProcess)
bars_dict = defaultdict(lambda : -1)

VLLM_LOG_PATH_DICT = defaultdict(str)
ALREADY_LOCK = FileLock(f"{TMP_PATH}/already.lock")

BAR_INDEX_LOCK = FileLock(f"{TMP_PATH}/locks/bar_index")
CONSOLE_INDEX_LOCK = FileLock(f"{TMP_PATH}/locks/console_index")
with open(f"{ROOT_PATH}/scripts/utils/vllm_log_config.json", "r") as f:
    VLLM_LOG_CONFIG = json.load(f)

logger.remove();logger.add(sys.stdout,colorize=True, format="<level>{message}</level>")



def get_current_bar():
    with BAR_INDEX_LOCK:
        with open(f"{TMP_PATH}/bar_index","r") as f:
            ret = f.read()
    return int(ret)


def set_current_bar(index):
    with BAR_INDEX_LOCK:
        with open(f"{TMP_PATH}/bar_index","w") as f:
            f.write(str(index))

def get_current_console():
    with CONSOLE_INDEX_LOCK:
        with open(f"{TMP_PATH}/console_index","r") as f:
            ret = f.read()
    return int(ret)


def set_current_console(index):
    with CONSOLE_INDEX_LOCK:
        with open(f"{TMP_PATH}/console_index","w") as f:
            f.write(str(index))

def active_consoles():
    if not os.path.exists(f"{TMP_PATH}/active_index"):
        with open(f"{TMP_PATH}/active_index","w") as f:
            f.write("0")
        return [0]
    

    with open(f"{TMP_PATH}/active_index","r") as f:
        ret = [int(k.strip()) for k in f.readlines() if k.strip()]

    if not ret:
        with open(f"{TMP_PATH}/active_index","w") as f:
            f.write("0")
        return [0]
    return ret

def add_console(index = None):
    consoles = active_consoles()
    print("ADD___: ",consoles)

    if index is None:
        for i in range(0, len(get_console_map())):
            if i not in consoles:
                index = i
                break
        else: return
    assert index not in consoles
    consoles.append(index)
    with open(f"{TMP_PATH}/active_index","w") as f:
        f.write("\n".join(map(str,consoles)))
    
    print("ADD:", index)
    return index

def del_console(index):
    consoles = set(active_consoles())
    assert index in consoles
    consoles.remove(index)
    with open(f"{TMP_PATH}/active_index","w") as f:
        ret = "\n".join(map(str,consoles))
    return ret


def get_console_map():
    if not os.path.exists(f"{TMP_PATH}/console_status"):
        with open(f"{TMP_PATH}/console_status","w") as f:
            json.dump(dict(), f)
    with open(f"{TMP_PATH}/console_status","r") as f:
        ret = json.load(f)
    return ret


def vllm_log_path(index):
    
    return f"./WebUI/tmp/progress_bar{index}/vllm.log"
    

def end_vllm_log():
    VLLM_LOG_PATH = VLLM_LOG_PATH_DICT[get_current_console()]
    if not os.path.exists(VLLM_LOG_PATH):
        return
    with open(VLLM_LOG_PATH,"a+") as f:
        f.write(VLLM_LOG_END_CHAR)
    time.sleep(0.5)


def reset_vllm_log():
    VLLM_LOG_PATH = VLLM_LOG_PATH_DICT[get_current_console()]
    if not os.path.exists(VLLM_LOG_PATH):
        return


def remove_vllm_log():
    if not VLLM_LOG_PATH_DICT[get_current_console()]:
        return
    end_vllm_log()
    reset_vllm_log()
    VLLM_LOG_PATH_DICT.pop(get_current_console(),
                                           None)




def remove_progress_bar(index):
    with FileLock(f"{TMP_PATH}/locks/progress_bar{index}"):
        with open(f"{TMP_PATH}/already.txt", "r") as f:
            folders = set([k.strip() for k in f.readlines() if k.strip()])
            if os.path.exists(f"{TMP_PATH}/progress_bar{index}"):
                shutil.rmtree(f"{TMP_PATH}/progress_bar{index}")
            if f"progress_bar{index}" in folders:
                folders.remove(f"progress_bar{index}")
        with open(f"{TMP_PATH}/already.txt", "w") as f:
            f.write("\n".join(sorted(folders)) + "\n")
    print(f"{TMP_PATH}/progress_bar{index} :" , 
          os.path.exists(f"{TMP_PATH}/progress_bar{index}"),
          folders)


def set_vllm_log(index):

    cur_config_pth = f"{TMP_PATH}/progress_bar{index}/vllm_config.json"
    with open(cur_config_pth, "w") as f:
        copyed = deepcopy(VLLM_LOG_CONFIG)
        path = vllm_log_path(index)
        copyed["handlers"]["file"]["filename"] = path
        VLLM_LOG_PATH_DICT[get_current_console()] = path
        json.dump(copyed, f, indent = 2)
        with open(path,"w"):
            pass
        os.chmod(path, 0o777)
    
    os.chmod(cur_config_pth, 0o777)
    print("WTF: ",os.path.exists(cur_config_pth), os.path.exists(path))

    return cur_config_pth, path

def end_progress_bar(index):
    folder = f"{TMP_PATH}/progress_bar{index}"
    cur_lock = FileLock(f"{TMP_PATH}/locks/progress_bar{index}")
    with ALREADY_LOCK:
        with cur_lock:
            if not os.path.exists(folder) or \
                not os.path.exists(f"{folder}/total.txt"):
                return
            with open(f"{folder}/total.txt", "r") as f:
                total = int(f.read().strip())
            with cur_lock:
                with open(f"{folder}/current.txt", "w") as f:
                    f.write("\n".join(["1\n"] * total))
            if os.path.exists(f"{TMP_PATH}/progress_bar{index}/pb_over"):
                os.remove(f"{TMP_PATH}/progress_bar{index}/pb_over")
            time.sleep(0.5)

def run_evaluation(
    model_path, cfg_path, server, device, limit,
    torch_dtype, gp_num, adapter_path, save_tag,
    max_length, rag_method, acceleration,
    max_model_len, gpu_memory_utilization,
    enable_thinking, use_hf_endpo, progress=gr.Progress()
):
    global processes, bars_dict
    CURRENT_CONSOLE_INDEX = get_current_console()
    CONSOLE_MAP = get_console_map()


    if not isinstance(processes[CURRENT_CONSOLE_INDEX], EmptyProcess):
        return

    left_update_pad = [gr.update()] * CURRENT_CONSOLE_INDEX

    right_update_pad = [gr.update()] * (len(CONSOLE_MAP) - CURRENT_CONSOLE_INDEX - 1)

    current_file_path = os.path.abspath(__file__)
    webui_path = os.path.dirname(os.path.dirname(current_file_path))
    index = 0
    while os.path.exists(f"{webui_path}/tmp/progress_bar{index}/"):
        index += 1

    os.makedirs(f"{webui_path}/tmp/progress_bar{index}", exist_ok=True)
    # bars_dict[CURRENT_CONSOLE_INDEX] = index
    set_current_bar(index)
    print(f"{webui_path}/tmp/progress_bar{index}",":",os.path.exists(f"{webui_path}/tmp/progress_bar{index}"))
    
    vllm_cfg_pth, vllm_log_pth = set_vllm_log(index)

    with ALREADY_LOCK:
        with open(f"{webui_path}/tmp/starting", "w") as f:...


    if rag_method== "/":
        rag_method=None
    if acceleration == "/":
        acceleration=None
    if not model_path.strip():
        return "Error: Model path cannot be empty"
    if not cfg_path.strip():
        return "Error: Benchmark configuration path cannot be empty"
    if use_hf_endpo:
        os.environ["HF_ENDPO"] = "https://hf-mirror.com"

    cmd = BASE_CMD.copy()
    params = [
        ("--model_path", model_path),
        ("--cfg_path", cfg_path),
        ("--server", server),
        ("--limit", str(limit) if limit is not None else None),
        ("--torch_dtype", torch_dtype),
        ("--gp_num", gp_num),
        ("--adapter_path", adapter_path if adapter_path else None),
        ("--save_tag", save_tag if save_tag else None),
        ("--max_length", str(max_length)),
        ("--rag_method", rag_method if rag_method else None),
        ("--acceleration", acceleration if acceleration else None),
        ("--max_model_len", str(max_model_len) if max_model_len is not None else None),
        ("--gpu_memory_utilization", str(gpu_memory_utilization) if gpu_memory_utilization is not None else None),
        ("--enable_thinking", None if enable_thinking else None),
        ("--web_index",index)
    ]

    if device:
        cmd.extend(["--device"] + [str(d) for d in device])

    for param, value in params:
        if value is not None and value != " " and value != "":
            cmd.append(f"{param}={value}" if param != "--enable_thinking" else param)
    
    cmd = list(dict.fromkeys(cmd))

    logger.info(f"Executing command: {' '.join(cmd)}")
    progress(0, desc="Starting evaluation...")


    env = os.environ.copy()
    env["VLLM_LOGGING_CONFIG_PATH"] = vllm_cfg_pth
    env["VLLM_CONFIGURE_LOGGING"] = "1"

    process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env = env,
            universal_newlines=True
        )
    
    processes[CURRENT_CONSOLE_INDEX] = process

    accumulated_output = ""
    try:
        for line in iter(process.stdout.readline, ''):
            if process.poll() is not None:
                break
            line = line.strip()
            if line:
                accumulated_output += f"{line}\n"
                logger.info(line)
                yield *left_update_pad, accumulated_output, *right_update_pad, \
                    *([gr.update()] * (len(BENCHMARK_NAMES)))
                time.sleep(0.01)
                
    except Exception as e:
        error_msg = f"An error occurred during execution: {str(e)}"
        logger.error(error_msg)
        accumulated_output += f"\n{error_msg}\n"
        yield *left_update_pad, accumulated_output, *right_update_pad, \
                    *([gr.update()] * (len(BENCHMARK_NAMES)))

    finally:
        end_progress_bar(index)
        end_vllm_log()
        

    return_code = process.wait()
    remove_vllm_log()
    if return_code != 0:
        accumulated_output += f"\n⚠️ Process ended with non-zero exit code: {return_code}\n"
    progress(1, desc="Evaluation completed")

    stop_evaluation()

    

    benchmark_name = cfg_path.split("/")[-1].split(".")[0]

    benchmark_path = os.path.join(cfg_path.split(benchmark_name)[0],
                                  benchmark_name)

    yield *left_update_pad, accumulated_output, *right_update_pad, \
                    *([gr.update(
                        **(dict() if i != BENCHMARK_NAMES.index(benchmark_name) else \
                            dict(value = PAGES_DICT[benchmark_name](f"{benchmark_path}/results/{save_tag}")))
                    ) for i in range(len(BENCHMARK_NAMES))])
    # , gr.update(
    #     value = PAGES[benchmark_name](f'{benchmark_path}/results/{save_tag}/results.xlsx'))

    CURRENT_STATUS = f"{TMP_PATH}/status/{save_tag[2:]}/UI_status.json"
    cate, bench, task = cfg_path.split("/")[-3:]
    save_all(
        CURRENT_STATUS,
        model_path = model_path,
        config_path = cfg_path,
        torch_dtype = torch_dtype,
        adapter_path = adapter_path,
        enable_thinking = enable_thinking,
        use_hf_endpo = use_hf_endpo,
        save_tag = save_tag,
        vllm_max_model_len = max_model_len,
        gpu_memory_utilization = gpu_memory_utilization,
        max_length = max_length,
        server = server,
        acceleration = acceleration,
        rag_method = rag_method,
        chunk_size = 510,
        num_chunks = 15,
        chunk_overlap = 128,
        num_processes = 4,
        openai_api = None,
        embedding_model = "text-embedding-3-small",
        base_url = "https://api.openai.com/v1",
        category = cate,
        benchmark = bench,
        task = task.split(".")[0],
        num_sample = limit,
        hardware_config = device,
        vllm_log = open(vllm_log_path(index), "r").read() \
            if os.path.exists(vllm_log_path(index)) else "",
        real_time_log = accumulated_output,
    )

def monitor_vllm_log(progress = gr.Progress()):
    global processes, bars_dict
    CURRENT_CONSOLE_INDEX = get_current_console()
    CONSOLE_MAP = get_console_map()
    # logger.info(processes[CURRENT_CONSOLE_INDEX])

    
    logger.info(f"Starting Moni: {isinstance(processes[CURRENT_CONSOLE_INDEX], EmptyProcess)} {CURRENT_CONSOLE_INDEX} {processes[CURRENT_CONSOLE_INDEX]}", )
    
    
    logger.info("Starting Monitor VLLM LOG ...")

    

    path = vllm_log_path(get_current_bar())

    seconds = 0
    while not os.path.exists(path): 
        path = vllm_log_path(get_current_bar())
        seconds += 0.1
        if seconds > 10: 
            logger.info("Too Much Time To Init VLLM")
            break
        time.sleep(0.1)

    logger.info("VLLM LOG PATH:", path)
    VLLM_LOG_PATH = path
        
    
    left_update_pad = [gr.update()] * CURRENT_CONSOLE_INDEX

    right_update_pad = [gr.update()] * (len(CONSOLE_MAP) - CURRENT_CONSOLE_INDEX - 1)
    # reset_vllm_log()

    accumulated_output = ""
    yield *left_update_pad, accumulated_output, *right_update_pad
    last_position = 0

    progress(0, "Init VLLM Log")
    try:
        vllm_not_end = True
        while vllm_not_end:
            # if not os.path.exists(VLLM_LOG_PATH):
            #     break
            with open(VLLM_LOG_PATH, 'r') as f:
                f.seek(2, 0)
                if f.tell() == last_position:continue
                f.seek(last_position, 0)

                lines = f.readlines()
                for line in lines:
                    if line.endswith(VLLM_LOG_END_CHAR):
                        vllm_not_end = False
                        break
                    accumulated_output += line
                    yield *left_update_pad, accumulated_output, *right_update_pad
                
                last_position = f.tell()
            time.sleep(0.01)
    except Exception as e:
        error_msg = f"An error occurred during execution: {str(e)}"
        logger.error(error_msg)
        accumulated_output += f"\n{error_msg}\n"
        yield *left_update_pad, accumulated_output, *right_update_pad
    

    progress(1, desc="Vllm Log Close")


def stop_evaluation() -> None:
    CURRENT_CONSOLE_INDEX = get_current_console()
    global processes, bars_dict
    process = processes[CURRENT_CONSOLE_INDEX]
    if isinstance(process, EmptyProcess):
        return
    if process and process.poll() is None:
        logger.warning("User requests to terminate the evaluation process, first attempting graceful termination (SIGTERM)")
        process.terminate()  
        
        for _ in range(10): 
            if process.poll() is not None:
                break
            time.sleep(0.1)  
        
        if process.poll() is None:
            logger.error("The process did not respond to SIGTERM, force termination (SIGKILL)")
            process.kill()  # 强制终止
        
        logger.info(f"The process has terminated, return code: {process.returncode}")

    processes.pop(CURRENT_CONSOLE_INDEX, None)
    
    end_progress_bar(get_current_bar())
        
    remove_vllm_log()
    remove_progress_bar(CURRENT_CONSOLE_INDEX)
    set_current_bar(-1)
    logger.info("Stop Evaluation !!!")