
import os

import torch
from transformers import AutoModel, AutoTokenizer

os.environ["TOKENIZERS_PARALLELISM"] = "false"
from models.rag.bass_class import Base


class contriever(Base):
    def __init__(self,config,devices,index):
        super().__init__(config)
        self.retrieval_methods = "contriever"
        self.model_name_contriever = "facebook/contriever"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_contriever)
        self.model = AutoModel.from_pretrained(self.model_name_contriever,device_map=f"cuda:{devices[index%len(devices)]}")
    def retrieve(self,context,question):
        chunks = self.retrieve_relevant_chunks_contriever(context, question)
        return chunks
    def retrieve_relevant_chunks_contriever(self, context, question):
        context = self.chunk_text(context)
        context_embeddings = []
        for doc in context:
            inputs = self.tokenizer(doc, truncation=True, max_length=512, return_tensors='pt').to(self.model.device)
            outputs = self.model(**inputs)
            embedding = outputs['last_hidden_state'].mean(dim=1)
            context_embeddings.append(embedding)
        inputs = self.tokenizer(question, truncation=True, max_length=512, return_tensors='pt').to(self.model.device)
        question_embedding = self.model(**inputs)['last_hidden_state'].mean(dim=1)
        scores = [torch.nn.functional.cosine_similarity(question_embedding, context_embedding).item() for context_embedding in context_embeddings]
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.num_chunks]
        relevant_chunks = [context[i] for i in top_indices]
        return relevant_chunks