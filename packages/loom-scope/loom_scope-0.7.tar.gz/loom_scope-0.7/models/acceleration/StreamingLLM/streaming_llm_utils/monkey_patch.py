import functools
import types

import transformers
from transformers.models.llama.modeling_llama import (
    LlamaMLP,
)
from .llama_model import (
    llama_attn_forward_StreamingLLM,
    llama_flash_attn2_forward_StreamingLLM,
    llama_sdpa_attn_forward_StreamingLLM,
    prepare_inputs_for_generation_llama_new,
)
from .mistral_model import mistral_attn_forward_StreamingLLM, mistral_flash_attn2_forward_StreamingLLM, mistral_sdpa_attn_forward_StreamingLLM, prepare_inputs_for_generation_mistral_new
def replace_llama():
    print("Using StreamingLLM!")
    transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation = prepare_inputs_for_generation_llama_new
    transformers.models.llama.modeling_llama.LlamaAttention.forward = llama_attn_forward_StreamingLLM
    transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward = llama_flash_attn2_forward_StreamingLLM
    transformers.models.llama.modeling_llama.LlamaSdpaAttention.forward = llama_sdpa_attn_forward_StreamingLLM
    
def replace_mistral():
    print("Using StreamingLLM!")
    transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation = prepare_inputs_for_generation_mistral_new
    transformers.models.mistral.modeling_mistral.MistralAttention.forward = mistral_attn_forward_StreamingLLM
    transformers.models.mistral.modeling_mistral.MistralFlashAttention2.forward = mistral_flash_attn2_forward_StreamingLLM
    transformers.models.mistral.modeling_mistral.MistralSdpaAttention.forward = mistral_sdpa_attn_forward_StreamingLLM