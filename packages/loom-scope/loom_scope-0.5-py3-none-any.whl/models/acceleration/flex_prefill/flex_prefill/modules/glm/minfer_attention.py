# Copyright 2024 GLM-4-9B Model Team @ Zhipu AI
# Copyright 2024 ByteDance and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch ChatGLM model."""

from flash_attn import flash_attn_func

from ...ops.minfer.minfer_attention import minfer_attention


def glm_minfer_attention_forward(
    self, query_states, key_states, value_states, attention_mask
):
    batch_size, query_length = query_states.shape[0], query_states.shape[2]
    gqa_groups = query_states.shape[1] // key_states.shape[1]

    if query_states.shape[2] > 1:
        attn_output = minfer_attention(
            query_states,
            key_states,
            value_states,
            self.config.minfer_config[self.layer_number - 1],
        ).transpose(1, 2)
    else:
        attn_output = flash_attn_func(
            query_states.transpose(1, 2),
            key_states.transpose(1, 2),
            value_states.transpose(1, 2),
        )

    attn_output = attn_output.reshape(
        batch_size, query_length, self.hidden_size_per_partition
    ).contiguous()
    return attn_output
