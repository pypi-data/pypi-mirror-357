import functools
import types

import transformers
import transformers.models
from transformers.models.llama.modeling_llama import (
    LlamaMLP,
)

from .llama_model import (
    llama_attn_forward_L2Compress,
    llama_flash_attn2_forward_L2Compress,
    llama_sdpa_attn_forward_L2Compress,
    prepare_inputs_for_generation_llama_new,
)
from .mistral_model import (
    mistral_attn_forward_L2Compress,
    mistral_flash_attn2_forward_L2Compress,
    mistral_sdpa_attn_forward_L2Compress,
    prepare_inputs_for_generation_mistral_new,
)


def replace_llama():
    print("Using L2Compress!")
    transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation = prepare_inputs_for_generation_llama_new
    transformers.models.llama.modeling_llama.LlamaAttention.forward = llama_attn_forward_L2Compress
    transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward = llama_flash_attn2_forward_L2Compress
    transformers.models.llama.modeling_llama.LlamaSdpaAttention.forward = llama_sdpa_attn_forward_L2Compress
    
def replace_mistral():
    print("Using L2Compress!")
    transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation = prepare_inputs_for_generation_mistral_new
    transformers.models.mistral.modeling_mistral.MistralAttention.forward = mistral_attn_forward_L2Compress
    transformers.models.mistral.modeling_mistral.MistralFlashAttention2.forward = mistral_flash_attn2_forward_L2Compress
    transformers.models.mistral.modeling_mistral.MistralSdpaAttention.forward = mistral_sdpa_attn_forward_L2Compress