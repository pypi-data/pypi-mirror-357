# ğŸš„ åŠ é€Ÿå·¥å…·åŒ…
[English Version](./README.md) | [ä¸­æ–‡ç‰ˆæœ¬](./README_zh.md) | [ä¸»é¡µ](../../README.md)

æ¬¢è¿ä½¿ç”¨**åŠ é€Ÿå·¥å…·åŒ…**ï¼æœ¬ä»“åº“æä¾›äº†ä¸€å¥—å…¨é¢çš„å‰æ²¿åŠ é€Ÿæ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å„ç±»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚æ— è®ºæ‚¨é€‰æ‹©æ¨ç†è¿˜æ˜¯è®­ç»ƒï¼Œæœ¬å·¥å…·åŒ…éƒ½èƒ½æä¾›ä¸€ç³»åˆ—å…ˆè¿›æŠ€æœ¯æ¥æ˜¾è‘—æå‡æ‚¨çš„å·¥ä½œæ•ˆç‡ã€‚

æœ¬å·¥å…·åŒ…ä¸­æ‰€æœ‰åŠ é€Ÿæ–¹æ³•éƒ½é’ˆå¯¹å¤šGPUæ¨ç†åœºæ™¯è¿›è¡Œäº†å…¨é¢ä¼˜åŒ–ï¼Œå¹¶å…¼å®¹å¤šç§é«˜æ€§èƒ½GPUï¼ŒåŒ…æ‹¬**NVIDIA 3090**ã€**Aç³»åˆ—**å’Œ**Hç³»åˆ—**æ˜¾å¡ã€‚è¿™ç¡®ä¿äº†åœ¨ä¸åŒç¡¬ä»¶é…ç½®ä¸‹éƒ½èƒ½å®ç°æ— ç¼æ‰©å±•å’Œé«˜æ•ˆè¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ**XAttention**ä¸“ä¸ºA100 GPUè¿›è¡Œäº†ç‰¹åˆ«ä¼˜åŒ–ã€‚

é€šè¿‡åŠ é€Ÿå·¥å…·åŒ…ï¼Œæ‚¨å¯ä»¥åœ¨é™ä½è®¡ç®—å¼€é”€å’Œèµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œå……åˆ†é‡Šæ”¾æ¨¡å‹çš„æ½œåŠ›ã€‚è®©æˆ‘ä»¬å…±åŒåŠ é€Ÿæ‚¨çš„AIä¹‹æ—…ï¼

## ğŸ” æ¦‚è¿°

å½“å‰åŠ é€Ÿå·¥å…·åŒ…æ”¯æŒä»¥ä¸‹åŠ é€Ÿæ–¹æ³•ï¼š


| **åŠ é€Ÿç±»å‹** | **åŠ é€Ÿæ–¹æ³•** | **å¤‡æ³¨** |
|:-------------|:---------|:---------|
| **Tokené©±é€** | [H2O](https://github.com/FMInference/H2O) | åŸºäºæ³¨æ„åŠ›å¾—åˆ†çš„é€‰æ‹© |
|              | [StreamingLLM](https://github.com/mit-han-lab/streaming-llm) | ä¿ç•™æœ€åˆçš„å‡ ä¸ªtoken |
|              | [SnapKV](https://github.com/FasterDecoding/SnapKV) | åœ¨é€‰æ‹©tokenå‰ä½¿ç”¨æ± æ“ä½œ |
|              | [L2Compress](https://github.com/alessiodevoto/l2compress) | L2 Normæ¯”æ³¨æ„åŠ›å¾—åˆ†ä½œä¸ºæŒ‡æ ‡æ›´å¥½ |
| **åˆ†å±‚**     | [PyramidKV](https://github.com/Zefan-Cai/KVCache-Factory) | åˆ†å±‚é¢„ç®—åˆ†é… |
|              | [CakeKV](https://github.com/antgroup/cakekv) | åˆ†å±‚ç‰¹å®šåå¥½åˆ†æ•° |
| **é‡åŒ–**     | [KIVI](https://github.com/jy-yuan/KIVI) | éå¯¹ç§°2bité‡åŒ– |
| **é‡åŒ–+é©±é€** | [ThinK](https://github.com/SalesforceAIResearch/ThinK) | é€šè¿‡æŸ¥è¯¢é©±åŠ¨çš„å‰ªæå®ç°æ›´è–„çš„Keyç¼“å­˜ |
| **Tokenåˆå¹¶** | [CaM](https://github.com/zyxxmu/cam) | é¢å‘å†…å­˜é«˜æ•ˆLLMæ¨ç†çš„ç¼“å­˜åˆå¹¶ |
| **ç¨€ç–æ³¨æ„åŠ›** | [FlexPrefill](https://github.com/bytedance/FlexPrefill) | åŠ¨æ€ä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ |
|             | [XAttention](https://github.com/mit-han-lab/x-attention) | åŸºäºå¯¹è§’çº¿è¯„åˆ†çš„åŠ¨æ€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ |


## ğŸ’» ç¯å¢ƒé…ç½®å’Œå®‰è£…æŒ‡å—

### > é€šç”¨ç¯å¢ƒå®‰è£…

é¦–å…ˆåˆ›å»ºæ–°çš„Condaç¯å¢ƒå¹¶å®‰è£…å¿…è¦ä¾èµ–ï¼š

```bash
conda create -n acceleration python=3.10.16
conda activate acceleration
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 -f https://mirrors.aliyun.com/pytorch-wheels/cu121
pip install transformers==4.44.2
pip install ninja
pip install flash-attn==2.5.6 --no-build-isolation
pip install -r requirements.txt
```

### > FlexPrefillä¸“é¡¹å®‰è£…

å¦‚éœ€ä½¿ç”¨**FlexPrefill**æ–¹æ³•ï¼Œé‡å»ºç¯å¢ƒå¹¶é¢å¤–å®‰è£…ä»¥ä¸‹ç»„ä»¶ï¼ˆflex_prefill/install.shï¼‰ï¼š

```bash
conda create -n FlexPrefill python=3.10.16
conda activate FlexPrefill
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -f https://mirrors.aliyun.com/pytorch-wheels/cu121
pip install triton==3.0.0 transformers==4.44.0  vllm==0.5.4
pip install ninja
pip install flash_attn==2.6.3 --no-build-isolation
pip install nemo-toolkit[all]==1.21 --no-deps -i https://pypi.tuna.tsinghua.edu.cn/simple
pip3 install Cython==3.0.11 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -r ./models/acceleration/flexprefill_requirements.txt
```

æ­¤å¤–ï¼Œè¯·ä»[flashinfer.ai/whl](https://flashinfer.ai/whl)ä¸‹è½½é€‚é…ç‰ˆæœ¬çš„`flashinfer`ï¼Œä¾‹å¦‚```pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/```ã€‚

### > XAttentionä¸“é¡¹å®‰è£…

å¦‚éœ€ä½¿ç”¨**XAttention**æ–¹æ³•ï¼š

```bash
conda create -yn xattn python=3.10
conda activate xattn

conda install -y git
conda install -y nvidia/label/cuda-12.4.0::cuda-toolkit
conda install -y nvidia::cuda-cudart-dev
conda install -y pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia

pip install transformers==4.46 accelerate sentencepiece minference datasets wandb zstandard matplotlib huggingface_hub==0.23.2 torch torchaudio torchvision xformers  vllm==0.6.3.post1 vllm-flash-attn==2.6.1
pip install tensor_parallel==2.0.0

pip install ninja packaging
pip install flash-attn==2.6.3 --no-build-isolation
pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/

# LongBenchè¯„ä¼°
pip install seaborn rouge_score einops pandas

# å®‰è£…xAttention
pip install -e .

# å®‰è£…Block Sparse Streaming Attention
git clone https://github.com/mit-han-lab/Block-Sparse-Attention.git
cd Block-Sparse-Attention
python setup.py install
cd ..

export PYTHONPATH="$PYTHONPATH:$(pwd)"

```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### > é€šç”¨åŠ é€Ÿæ–¹æ³•

ä½¿ç”¨**KIVI**è¿›è¡ŒåŠ é€Ÿæ¨ç†ï¼š

```bash
cd KIVI/quant
pip install -e .
```

ä½¿ç”¨**ThinK**è¿›è¡ŒåŠ é€Ÿæ¨ç†ï¼š

```bash
cd think/load_tiny_cuda_resource
make i
```

> Note: å…¶ä»–é€šç”¨åŠ é€Ÿæ–¹æ³•æ— éœ€é¢å¤–å®‰è£…

### > FlexPrefillä¸“é¡¹

è¿è¡Œä»¥ä¸‹å‘½ä»¤å³å¯ï¼š

```bash
cd flex_prefil
pip install -e .
```

### > å¿«é€Ÿå¼€å§‹

æ‚¨åªéœ€åœ¨è¿è¡Œå‘½ä»¤ä¸­æŒ‡å®š`--acceleration`å‚æ•°åŠ ä¸Šæ–¹æ³•åç§°å³å¯å¯ç”¨å¯¹åº”æ–¹æ³•ï¼Œå¦‚ï¼š`--acceleration L2Compress`ã€‚

| **ç±»åˆ«**          | **å‚æ•°**          | **ç±»å‹/æ ¼å¼**             | **æè¿°**                                                         | **æ˜¯å¦å¿…é¡»/é»˜è®¤å€¼**                                             |
|:------------------|:------------------|:--------------------------|:-----------------------------------------------------------------|:---------------------------------------------------------------|
| **æ ¸å¿ƒå‚æ•°**      |                   |                           |                                                                  |                                                                  |
|                   | `--model_path`    | `str`                     | æ¨¡å‹è·¯å¾„/æœåŠ¡æä¾›è€…ï¼ˆä¾‹å¦‚ï¼ŒMeta-Llama-3-70B-Instructï¼‰            | å¿…é¡»                                                             |
|                   | `--cfg_path`      | `str` (YAML)              | åŸºå‡†æµ‹è¯•é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œbenchmarks/General/LongBench.yamlï¼‰       | å¿…é¡»                                                             |
|                   | `--device`        | `List[int]`               | GPUç´¢å¼•ï¼ˆä¾‹å¦‚ï¼Œ`0 1 3`ï¼‰                                          | é»˜è®¤å€¼ï¼šæ‰€æœ‰å¯ç”¨GPU                                              |
|                   | `--torch_dtype`   | `str \| torch.dtype`      | ç²¾åº¦ï¼ˆä¾‹å¦‚ï¼Œ`float16`ï¼Œ`bfloat16`ï¼‰                                | é»˜è®¤å€¼ï¼š`torch.bfloat16`                                         |
| **GPUåˆ†é…**       |                   |                           |                                                                  |                                                                  |
|                   | `--gp_num`        | `int \| str`              | æ¯ä¸ªæ¨¡å‹çš„GPUæ•°é‡ï¼ˆä¾‹å¦‚ï¼Œ`2`ï¼‰*æˆ–*æ¯ä¸ªGPUçš„æ¨¡å‹æ•°é‡ï¼ˆä¾‹å¦‚ï¼Œ`'1/2'`ï¼‰ | é»˜è®¤å€¼ï¼š1                                                        |
| **ä¼˜åŒ–é€‰é¡¹**      |                   |                           |                                                                  |                                                                  |
|                   | `--acceleration`  | `str`                     | å®éªŒæ€§æ–¹æ³•ï¼ˆ`H2O`ï¼Œ`SnapKV`ï¼Œ`StreamingLLM`ï¼Œ...ï¼‰                | é€‰é¡¹ï¼š{â€œH2Oâ€ï¼Œâ€œStreamingLLMâ€ï¼Œâ€œSnapKVâ€ï¼Œâ€œL2Compressâ€ï¼Œâ€œFlexPrefillâ€ï¼Œâ€œPyramidKVâ€ï¼Œâ€œCakeKVâ€ï¼Œâ€œKIVIâ€ï¼Œâ€œThinKâ€ï¼Œâ€œCaMâ€} |

## ğŸ¤– æ”¯æŒæ¨¡å‹

æœ¬å·¥å…·åŒ…æ”¯æŒå¹¿æ³›çš„æ¨¡å‹ç³»åˆ—ï¼š

- **é™¤FlexPrefillå¤–æ‰€æœ‰æ–¹æ³•**ï¼šæ”¯æŒ`llama`å’Œ`mistral`ç³»åˆ—
- **FlexPrefill**ï¼šæ”¯æŒ`llama`ã€`glm`å’Œ`qwen2`ç³»åˆ—
- **CakeKV**ï¼šæ”¯æŒ`llama`ã€`mistral`å’Œ`qwen2`ç³»åˆ—
- **XAttention**ï¼šæ”¯æŒ`llama`ç³»åˆ—

### âš ï¸ æ³¨æ„äº‹é¡¹

åœ¨è¿è¡Œæ¨ç†ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨è‡ªå®šä¹‰äº† `acceleration/xxx/config` ç›®å½•ä¸‹çš„ç›¸å…³é…ç½®æ–‡ä»¶ï¼Œä»¥æ»¡è¶³æ‚¨çš„ç‰¹å®šéœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½éœ€è¦è°ƒæ•´æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼Œå¦‚ `.models/acceleration/cake/config/cake_config.json `ã€‚

### âš ï¸ é‡è¦æç¤º

é™¤äº†L2Compressä¹‹å¤–çš„æ‰€æœ‰åŠ é€Ÿæ–¹æ³•éƒ½å¯ä»¥åœ¨å•å¼ 40GB A100æ˜¾å¡ä¸Šç”¨äºé•¿åº¦ä¸º128Kçš„æ¨ç†ä»»åŠ¡ã€‚L2Compressæ¨èç”¨äºä¸‰å¡æ¨ç†ï¼Œå› ä¸ºåœ¨KVç¼“å­˜å‹ç¼©è¿‡ç¨‹ä¸­ï¼Œè¿™ç§æ–¹æ³•ä¼šè·³è¿‡æµ…å±‚å¹¶ä¿ç•™å®ƒä»¬çš„å®Œæ•´KVç¼“å­˜ï¼Œè¿™ä½¿å¾—å®ƒåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸­ç›¸æ¯”å…¶ä»–æ–¹æ³•ä¼šå ç”¨æ›´é«˜çš„æ˜¾å­˜ã€‚å¦‚æœæ˜¾å­˜å¤§å°ä»…æœ‰24GBï¼Œåˆ™åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ¨èä½¿ç”¨å¤šå¡æ¨ç†ã€‚

## ğŸ“Š å®éªŒç»“æœ

æˆ‘ä»¬ä½¿ç”¨ Llama-3.1-8B-Instruct æ¨¡å‹åœ¨ RULER æµ‹è¯•åŸºå‡†ä¸Šè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†ä¸åŒ GPU ç¡¬ä»¶ï¼ˆNVIDIA 3090ã€40GB A100 å’Œ H20ï¼‰ä¸‹çš„åŠ é€Ÿæ–¹æ³•æµ‹è¯•ç»“æœã€‚

### > æ¨ç†æ—¶é—´
å®éªŒé…ç½®å¦‚ä¸‹ï¼š
```ymal
benchmark_name: RULER
task_names: ["cwe","niah_multikey_1", "niah_multikey_2", "niah_multikey_3", "niah_multiquery", "niah_multivalue", "niah_single_1", "niah_single_2", "niah_single_3", "qa_1", "qa_2", "vt"]
length: [4096,8192,16384,32768,65536,131072] 

num_samples: 10 # Set the number of samples for each task to num_samples
no_template_tasks: ["fwe"]  # task_names or "all" 
template: NULL  # choose from ./models/utils/build_chat.py
```
ç ´æŠ˜å·â€œ-â€è¡¨ç¤ºå†…å­˜ä¸è¶³ï¼ˆOOMï¼‰é”™è¯¯ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼ŒCAMæ— æ³•æœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬æ¨ç†ä»»åŠ¡ï¼Œè€ŒFlexPrefillåœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­å‡è¡¨ç°å‡ºæœ€ä½³çš„æ€§èƒ½å’Œé€Ÿåº¦ã€‚

#### NVIDIA 3090è¡¨ç°

| åºåˆ—é•¿åº¦ | KIVI | H2O | StreamingLLM | L2Compress | CaM | CakeKV | PyramidKV | FlexPrefill | SnapKV | ThinK | Transformers | vLLM |
|------------|------|-----|--------------|--------|-----|--------|-----------|-------------|--------|-------|--------------|------|
| 4K         | 94.83 | 46.13 | 27.79 | 36.42 | 67.39 | 91.88 | 77.42 | 93.33 | 81.54 | 81.83 | 94.83 | 93.67 |
| 8K         | 94.29 | 34.42 | 22.21 | 27.75 | 59.60 | 82.42 | 71.25 | 87.92 | 73.29 | 72.88 | 94.42 | 93.04 |
| 16K        | 91.08 | 21.63 | 11.46 | 27.58 | 48.53 | 78.96 | 68.75 | 83.38 | 67.88 | 67.25 | 92.50 | 91.50 |
| 32K             | 89.50   | 19.67   | 10.79        | 22.17      | 43.42    | 79.54   | 64.33     | 83.38       | 66.29   | 66.21   | 92.75        | 88.17   |
| 64K        | - | 10.54 | 7.67 | 18.25 | 33.25 | 62.88 | 57.29 | 65.04 | 55.58 | 56.92 | 83.71 | 78.71 |
| 128K       | - | 6.38 | 9.79 | 12.17 | 28.10 | 53.17 | 48.29 | 46.58 | 47.25 | 48.71 | - | 16.67 |
| æ€»è€—æ—¶ | 1:29:58 | 6:11:21 | 6:07:27 | 6:08:31 | 50:31:13 | 6:13:01 | 6:14:12 | 3:58:17 | 6:08:31 | 6:18:08 | 3:11:15 | 4:47:37 |

#### 40GB A100è¡¨ç°

| åºåˆ—é•¿åº¦ | KIVI | H2O | StreamingLLM | L2Compress | CaM | CakeKV | PyramidKV | FlexPrefill | SnapKV | ThinK | Transformers | vLLM |
|------------|------|-----|--------------|--------|-----|--------|-----------|-------------|--------|-------|--------------|------|
| 4K         | 95.46 | 46.38 | 28.83 | 33.96 | 67.29 | 89.67 | 77.00 | 93.58 | 79.75 | 79.33 | 95.04 | 90.71 |
| 8K         | 94.42 | 31.96 | 21.58 | 26.83 | 59.67 | 82.46 | 70.79 | 89.04 | 72.67 | 71.92 | 94.46 | 91.63 |
| 16K        | 90.04 | 23.00 | 10.63 | 27.92 | 48.53 | 77.29 | 67.54 | 84.83 | 66.63 | 65.25 | 92.50 | 91.75 |
| 32K             | 89.00   | 19.86   | 10.79        | 20.83      | 43.21    | 76.42   | 65.25     | 79.63       | 63.88   | 66.46   | 92.08        | 86.25   |
| 64K             | 90.25   | 10.50   | 7.67         | 17.25      | 33.35    | 65.79   | 55.54     | 62.50       | 54.75   | 55.75   | 83.71        | 78.83   |
| 128K            | 67.92   | 6.17    | 10.63        | 12.17      | 28.03    | 55.21   | 49.54     | 49.42       | 50.79   | 51.00   | 75.21        | 17.50   |
| æ€»è€—æ—¶ | 3:00:05 | 2:50:26 | 2:44:12      | 2:56:36    | 25:41:21 | 2:58:21 | 2:57:54   | 1:52:41     | 2:52:51 | 3:09:58 | 3:23:10      | 2:15:38 |
| Batch=8æ—¶ | 0:32:57 | 0:27:00 | 0:26:35      | 0:27:26    | - | 0:28:26 | 0:26:39   | 0:15:58     | 0:26:44 | 0:35:46 | -            | 0:27:20 |

#### NVIDIA H20è¡¨ç°

| åºåˆ—é•¿åº¦ | KIVI | H2O | StreamingLLM | L2Compress | CaM | CakeKV | PyramidKV | FlexPrefill | SnapKV | ThinK | Transformers | vLLM |
|------------|------|-----|--------------|--------|-----|--------|-----------|-------------|--------|-------|--------------|------|
| 4K         | 93.79 | 54.33 | 28.00 | 34.21 | 66.29 | 89.67 | 77.13 | 92.63 | 82.22 | 80.83 | 94.00 | 91.75 |
| 8K         | 94.33 | 41.25 | 21.58 | 26.75 | 59.67 | 82.63 | 71.50 | 87.88 | 70.46 | 72.33 | 94.46 | 88.79 |
| 16K        | 90.71 | 24.17 | 10.63 | 27.00 | 48.33 | 77.21 | 65.33 | 84.54 | 64.57 | 65.92 | 92.50 | 84.58 |
| 32K        | 88.08 | 19.42 | 10.79 | 22.58 | 43.42 | 76.42 | 66.21 | 80.71 | 69.27 | 66.46 | 91.88 | 79.50 |
| 64K        | 80.91 | 12.33 | 6.83 | 16.58 | 34.25 | 65.71 | 58.25 | 62.96 | 57.73 | 56.25 | 83.71 | 77.79 |
| 128K       | 69.13 | 10.92 | 10.63 | 12.25 | 28.13 | 54.21 | 47.58 | 48.08 | 45.94 | 47.25 | 75.21 | 57.88 |
| æ€»è€—æ—¶ | 3:31:35 | 3:20:18 | 3:21:25 | 3:29:34 | 26:11:23 | 3:22:06 | 3:22:05 | 1:49:35 | 3:26:09 | 3:35:32 | 3:57:44 | 2:22:45 |
| Batch=8æ—¶ | 0:33:44 | 0:33:08 | 0:32:35 | 00:33:48 | - | 0:32:37 | 0:32:41 | 0:13:10 | 0:32:41 | 0:34:25 | - | 0:38:56 |