
import math

import torch
import torch.nn as nn

import sys
from loguru import logger

logger.remove()
logger.add(sys.stdout, colorize=True, format="<level>{message}</level>")
class H2OKVCluster():
    def __init__(self, window_size = 64, max_capacity_prompt = 256 + 64):
        self.window_size = window_size
        self.max_capacity_prompt = max_capacity_prompt
        assert self.max_capacity_prompt - self.window_size > 0

    def reset(self, window_size = 64, max_capacity_prompt = 256 + 64):
        self.window_size = window_size
        self.max_capacity_prompt = max_capacity_prompt
        assert self.max_capacity_prompt - self.window_size > 0

    def update_kv(self, key_states, query_states, value_states, attention_mask, num_key_value_groups):
        # check if prefix phase
        assert key_states.shape[-2] == query_states.shape[-2]
        bsz, num_heads, q_len, head_dim = query_states.shape
        
        if q_len < self.max_capacity_prompt:
            return key_states, value_states
        else:
            # Calculate how many tokens we need to compress
            past_len = q_len - self.window_size
            compress_len = self.max_capacity_prompt - self.window_size

            # Process in chunks to avoid OOM with large matrices
            chunk_size = 512  # Adjustable chunk size based on memory constraints
            indices_list = []

            for chunk_start in range(0, past_len, chunk_size):
                chunk_end = min(chunk_start + chunk_size, past_len)
                chunk_len = chunk_end - chunk_start
                
                # Calculate attention for this chunk
                q_chunk = query_states[:, :, -self.window_size:, :]
                k_chunk = key_states[:, :, chunk_start:chunk_end, :]
                
                # Compute attention weights for this chunk
                chunk_attn = torch.matmul(q_chunk, k_chunk.transpose(2, 3)) / math.sqrt(head_dim)
                chunk_attn = nn.functional.softmax(chunk_attn, dim=-1, dtype=torch.float32).to(query_states.dtype)
                
                # Sum across query dimension to get importance scores
                chunk_attn_sum = chunk_attn.sum(dim=2)
                
                # Store this chunk's indices separately
                if chunk_start == 0:
                    attn_cache = chunk_attn_sum
                else:
                    attn_cache = torch.cat([attn_cache, chunk_attn_sum], dim=-1)

            # Get top-k indices from the complete attention cache
            topk_indices = attn_cache.topk(compress_len, dim=-1).indices
            topk_indices = topk_indices.unsqueeze(-1).expand(-1, -1, -1, head_dim)

            # Gather the most important past keys and values
            k_past_compress = key_states[:, :, :-self.window_size, :].gather(dim=2, index=topk_indices)
            v_past_compress = value_states[:, :, :-self.window_size, :].gather(dim=2, index=topk_indices)

            # Get current window keys and values
            k_cur = key_states[:, :, -self.window_size:, :]
            v_cur = value_states[:, :, -self.window_size:, :]

            # Concatenate compressed past with current window
            key_states = torch.cat([k_past_compress, k_cur], dim=2)
            value_states = torch.cat([v_past_compress, v_cur], dim=2)

            return key_states, value_states


def init_H2O(self):
    if not hasattr(self, "kv_cluster"):
        if not hasattr(self.config, 'window_size'):
            self.config.window_size = 32
        if not hasattr(self.config, 'max_capacity_prompt'):
            self.config.max_capacity_prompt = 2048
    
    self.kv_cluster = H2OKVCluster(
        window_size = self.config.window_size, 
        max_capacity_prompt = self.config.max_capacity_prompt, 
        )