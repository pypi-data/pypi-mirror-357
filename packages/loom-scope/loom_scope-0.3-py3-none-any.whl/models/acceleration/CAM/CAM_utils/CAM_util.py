
import math

import torch
import torch.nn as nn


class CAMKVCluster:
    def __init__(
        self,
        start_budget_ratio=0.1,
        window_size=64,
        max_capacity_prompt=256 + 64,
    ):
        self.window_size = window_size
        self.max_capacity_prompt = max_capacity_prompt
        assert self.max_capacity_prompt - self.window_size > 0
        self.start_budget_ratio = start_budget_ratio

    def reset(
        self,
        start_budget_ratio=0.1,
        window_size=64,
        max_capacity_prompt=256 + 64,
    ):
        self.window_size = window_size
        self.max_capacity_prompt = max_capacity_prompt
        assert self.max_capacity_prompt - self.window_size > 0
        self.start_budget_ratio = start_budget_ratio

    def update_kv(
        self,
        key_states,
        query_states,
        value_states,
        attention_mask,
        num_key_value_groups,
    ):
        # check if prefix phase
        assert key_states.shape[-2] == query_states.shape[-2]
        bsz, num_heads, q_len, head_dim = query_states.shape

        if q_len < self.max_capacity_prompt:
            return key_states, value_states
        else:
            
            # Compute attention weights
            attn_weights = torch.matmul(
                query_states[..., -self.window_size :, :], key_states.transpose(2, 3)
            ) / math.sqrt(head_dim)

            # Create mask
            mask = torch.full(
                (self.window_size, self.window_size),
                torch.finfo(attn_weights.dtype).min,
                device=attn_weights.device,
            )
            mask_cond = torch.arange(mask.size(-1), device=attn_weights.device)
            mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
            mask = mask.to(attn_weights.device)
            attention_mask = mask[None, None, :, :]

            # Apply mask to attention weights
            attn_weights[:, :, -self.window_size :, -self.window_size :] += attention_mask

            # Softmax normalization
            attn_weights = nn.functional.softmax(
                attn_weights, dim=-1, dtype=torch.float32
            ).to(query_states.dtype)

            # Compute attention cache
            attn_weights_sum = attn_weights[:, :, :, : -self.window_size].sum(dim=-2)
            attn_cache = attn_weights_sum

            # Merge recent tokens
            start_budget = math.ceil(self.start_budget_ratio * q_len)
            recent_budget = self.window_size

            # CAM merge optimization
            seq_length = attn_weights.shape[-1]
            padding_length = 0
            merge_budget = recent_budget

            # Precompute attn_score and mean_attn to avoid redundant calculations
            attn_score = torch.mean(attn_weights[:, :, :seq_length, :seq_length], dim=-2)
            mean_attn = torch.max(
                torch.cat(
                    (
                        attn_score[0, :, :start_budget],
                        attn_score[0, :, -recent_budget:],
                    ),
                    dim=-1,
                ),
                dim=-1,
            )[0]

            for token_index in range(start_budget + padding_length + recent_budget, seq_length):
                if token_index - recent_budget < 0 or token_index - recent_budget >= value_states.shape[2]:
                    continue

                merge_prob = attn_score[0, :, token_index - recent_budget] / mean_attn
                merge_prob = torch.nan_to_num(merge_prob, nan=0.0, posinf=1.0, neginf=0.0)
                merge_mask = torch.bernoulli(merge_prob.clamp(min=0, max=1))

                score1 = (
                    value_states[:, :, token_index - recent_budget, ...].clone()
                    * merge_mask.unsqueeze(-1)
                    / merge_budget
                )
                value_states[
                    :,
                    :,
                    token_index - recent_budget + 1 : token_index - recent_budget + merge_budget + 1,
                    :,
                ] += score1.unsqueeze(2)

            # Compress past key-value states
            indices = attn_cache.topk(self.max_capacity_prompt - self.window_size, dim=-1).indices
            indices = indices.unsqueeze(-1).expand(-1, -1, -1, head_dim)

            k_past_compress = key_states[:, :, : -self.window_size, :].gather(dim=2, index=indices)
            v_past_compress = value_states[:, :, : -self.window_size, :].gather(dim=2, index=indices)

            k_cur = key_states[:, :, -self.window_size :, :]
            v_cur = value_states[:, :, -self.window_size :, :]

            key_states = torch.cat([k_past_compress, k_cur], dim=2)
            value_states = torch.cat([v_past_compress, v_cur], dim=2)

            return key_states, value_states



def init_CAM(self):
    if not hasattr(self, "kv_cluster"):
        if not hasattr(self.config, "window_size"):
            self.config.window_size = 32
        if not hasattr(self.config, "max_capacity_prompt"):
            self.config.max_capacity_prompt = 2048

    self.kv_cluster = CAMKVCluster(
        window_size=self.config.window_size,
        max_capacity_prompt=self.config.max_capacity_prompt,
    )
