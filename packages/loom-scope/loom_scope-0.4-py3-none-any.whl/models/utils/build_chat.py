def Convert_To_Input(messages):
    input = []
    for d in messages:
        input.append(d["content"])
    return " ".join(input)

def Llama_4_Maverick(tokenizer,input,model_path,**kwargs):
    with open(f"{model_path}/chat_template.jinja", "r") as f:
        chat_template = f.read()
    tokenizer.chat_template = chat_template 
    if isinstance(input,list):
        return tokenizer.apply_chat_template(
        input,
        tokenize=False,
        add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template(
        [{"role": "user", "content": input}],
        tokenize=False,
        add_generation_prompt=True)

def Mistral_Small_3_1(tokenizer,input,model_path,**kwargs):
    with open(f"{model_path}/chat_template.json", "r") as f:
        chat_template_data = json.load(f)
    tokenizer.chat_template = chat_template_data["chat_template"]
    if isinstance(input,list):
        return tokenizer.apply_chat_template(
        input,
        tokenize=False,
        add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template(
        [{"role": "user", "content": input}],
        tokenize=False,
        add_generation_prompt=True)
        
def messages_to_longchat_v2_format(messages):
    conversation = ""
    system_message = ""
    for message in messages:
        if message["role"] == "system":
            system_message = message["content"]
            break
    if system_message:
        conversation += f"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\nSYSTEM: {system_message}\n\n"
    else:
        conversation += "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\n"
    for message in messages:
        role = message["role"]
        content = message["content"]
        if role == "user":
            conversation += f"USER: {content}\n"
        elif role == "assistant":
            conversation += f"ASSISTANT: {content}\n"
    if not conversation.endswith("ASSISTANT: "):
        conversation += "ASSISTANT:"
    return conversation
    
def chatglm3(tokenizer, input,**kwargs):
    input = tokenizer.build_chat_input(input)
    return input

def chatglm(tokenizer, input,**kwargs):
    input = tokenizer.build_prompt(input)
    return input


def longchat(tokenizer,input,**kwargs):
    if isinstance(input,list):
        input = Convert_To_Input(input)
    from fastchat.model import get_conversation_template
    conv = get_conversation_template("vicuna")
    conv.append_message(conv.roles[0], input)
    conv.append_message(conv.roles[1], None)
    input = conv.get_prompt()
    return input

def llama2(input,**kwargs):
    if isinstance(input,list):
        input = Convert_To_Input(input)
    return f"[INST]{input}[/INST]"

def xgen(tokenizer,input,**kwargs):
    if isinstance(input,list):
        input = Convert_To_Input(input)
    header = ("A chat between a curious human and an artificial intelligence assistant. "
            "The assistant gives helpful, detailed, and polite answers to the human's questions.\n\n")
    return  header + f" ### Human: {input}\n###"

def internlm(input,**kwargs):
    if isinstance(input,list):
        input = Convert_To_Input(input)

    return f"<|User|>:{input}<eoh>\n<|Bot|>:"

def deepseek(tokenizer,input,**kwargs):
    if isinstance(input,list):
        return tokenizer.apply_chat_template(input,tokenize=False, add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template([{"role": "user", "content": input}],tokenize=False, add_generation_prompt=True)

def Mistral(tokenizer,input,**kwargs):
    if isinstance(input,list):
        return tokenizer.apply_chat_template(input,tokenize=False, add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template([{"role": "user", "content": input}],tokenize=False, add_generation_prompt=True)

def llama3(tokenizer,input,**kwargs):
    if isinstance(input,list):
        return tokenizer.apply_chat_template(input,tokenize=False, add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template([{"role": "user", "content": input}],tokenize=False, add_generation_prompt=True)

def phi(tokenizer,input,**kwargs):
    if isinstance(input,list):
        return tokenizer.apply_chat_template(input,tokenize=False, add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template([{"role": "user", "content": input}],tokenize=False, add_generation_prompt=True)


def glm4(tokenizer,input,enable_thinking,**kwargs):
    if isinstance(input,list):
        return tokenizer.apply_chat_template(input,tokenize=False, add_generation_prompt=True)
    else:
        return tokenizer.apply_chat_template([{"role": "user", "content": input}],tokenize=False, add_generation_prompt=True)


def qwen(tokenizer,input,enable_thinking,**kwargs):
    if isinstance(input,list):
        return tokenizer.apply_chat_template(input,tokenize=False, add_generation_prompt=True,enable_thinking=enable_thinking)
    else:
        return tokenizer.apply_chat_template([{"role": "user", "content": input}],tokenize=False, add_generation_prompt=True,enable_thinking=enable_thinking)
        
def longchat(tokenizer, input, **kwargs):
    if isinstance(input,list):
        input = messages_to_longchat_v2_format(input)
    return input
    
def default(input,**kwargs):
    if isinstance(input,list):
        input = Convert_To_Input(input)
    return input
import importlib



def RetrieveModelBuildChat(model_name):
    if not model_name:model_name="default"
    module_path = "models.utils.build_chat" 
    module = importlib.import_module(module_path)
    generate_func = getattr(module, model_name)
    return generate_func
