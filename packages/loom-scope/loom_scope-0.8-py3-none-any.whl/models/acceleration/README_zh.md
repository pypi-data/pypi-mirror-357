# 🚄 加速工具包
[English Version](./README.md) | [中文版本](./README_zh.md) | [主页](../../README.md)

欢迎使用**加速工具包**！本仓库提供了一套全面的前沿加速方法，旨在优化各类大语言模型（LLM）的性能。无论您选择推理还是训练，本工具包都能提供一系列先进技术来显著提升您的工作效率。

本工具包中所有加速方法都针对多GPU推理场景进行了全面优化，并兼容多种高性能GPU，包括**NVIDIA 3090**、**A系列**和**H系列**显卡。这确保了在不同硬件配置下都能实现无缝扩展和高效运行。需要注意的是，**XAttention**专为A100 GPU进行了特别优化。

通过加速工具包，您可以在降低计算开销和资源消耗的同时，充分释放模型的潜力。让我们共同加速您的AI之旅！

## 🔍 概述

当前加速工具包支持以下加速方法：


| **加速类型** | **加速方法** | **备注** |
|:-------------|:---------|:---------|
| **Token驱逐** | [H2O](https://github.com/FMInference/H2O) | 基于注意力得分的选择 |
|              | [StreamingLLM](https://github.com/mit-han-lab/streaming-llm) | 保留最初的几个token |
|              | [SnapKV](https://github.com/FasterDecoding/SnapKV) | 在选择token前使用池操作 |
|              | [L2Compress](https://github.com/alessiodevoto/l2compress) | L2 Norm比注意力得分作为指标更好 |
| **分层**     | [PyramidKV](https://github.com/Zefan-Cai/KVCache-Factory) | 分层预算分配 |
|              | [CakeKV](https://github.com/antgroup/cakekv) | 分层特定偏好分数 |
| **量化**     | [KIVI](https://github.com/jy-yuan/KIVI) | 非对称2bit量化 |
| **量化+驱逐** | [ThinK](https://github.com/SalesforceAIResearch/ThinK) | 通过查询驱动的剪枝实现更薄的Key缓存 |
| **Token合并** | [CaM](https://github.com/zyxxmu/cam) | 面向内存高效LLM推理的缓存合并 |
| **稀疏注意力** | [FlexPrefill](https://github.com/bytedance/FlexPrefill) | 动态且上下文感知的稀疏注意力机制 |
|             | [XAttention](https://github.com/mit-han-lab/x-attention) | 基于对角线评分的动态上下文感知稀疏注意力机制 |


## 💻 环境配置和安装指南

### > 通用环境安装

首先创建新的Conda环境并安装必要依赖：

```bash
conda create -n acceleration python=3.10.16
conda activate acceleration
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 -f https://mirrors.aliyun.com/pytorch-wheels/cu121
pip install transformers==4.44.2
pip install ninja
pip install flash-attn==2.5.6 --no-build-isolation
pip install -r requirements.txt
```

### > FlexPrefill专项安装

如需使用**FlexPrefill**方法，重建环境并额外安装以下组件（flex_prefill/install.sh）：

```bash
conda create -n FlexPrefill python=3.10.16
conda activate FlexPrefill
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -f https://mirrors.aliyun.com/pytorch-wheels/cu121
pip install triton==3.0.0 transformers==4.44.0  vllm==0.5.4
pip install ninja
pip install flash_attn==2.6.3 --no-build-isolation
pip install nemo-toolkit[all]==1.21 --no-deps -i https://pypi.tuna.tsinghua.edu.cn/simple
pip3 install Cython==3.0.11 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -r ./models/acceleration/flexprefill_requirements.txt
```

此外，请从[flashinfer.ai/whl](https://flashinfer.ai/whl)下载适配版本的`flashinfer`，例如```pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/```。

### > XAttention专项安装

如需使用**XAttention**方法：

```bash
conda create -yn xattn python=3.10
conda activate xattn

conda install -y git
conda install -y nvidia/label/cuda-12.4.0::cuda-toolkit
conda install -y nvidia::cuda-cudart-dev
conda install -y pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia

pip install transformers==4.46 accelerate sentencepiece minference datasets wandb zstandard matplotlib huggingface_hub==0.23.2 torch torchaudio torchvision xformers  vllm==0.6.3.post1 vllm-flash-attn==2.6.1
pip install tensor_parallel==2.0.0

pip install ninja packaging
pip install flash-attn==2.6.3 --no-build-isolation
pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/

# LongBench评估
pip install seaborn rouge_score einops pandas

# 安装xAttention
pip install -e .

# 安装Block Sparse Streaming Attention
git clone https://github.com/mit-han-lab/Block-Sparse-Attention.git
cd Block-Sparse-Attention
python setup.py install
cd ..

export PYTHONPATH="$PYTHONPATH:$(pwd)"

```

## 🚀 快速开始

### > 通用加速方法

使用**KIVI**进行加速推理：

```bash
cd KIVI/quant
pip install -e .
```

使用**ThinK**进行加速推理：

```bash
cd think/load_tiny_cuda_resource
make i
```

> Note: 其他通用加速方法无需额外安装

### > FlexPrefill专项

运行以下命令即可：

```bash
cd flex_prefil
pip install -e .
```

### > 快速开始

您只需在运行命令中指定`--acceleration`参数加上方法名称即可启用对应方法，如：`--acceleration L2Compress`。

| **类别**          | **参数**          | **类型/格式**             | **描述**                                                         | **是否必须/默认值**                                             |
|:------------------|:------------------|:--------------------------|:-----------------------------------------------------------------|:---------------------------------------------------------------|
| **核心参数**      |                   |                           |                                                                  |                                                                  |
|                   | `--model_path`    | `str`                     | 模型路径/服务提供者（例如，Meta-Llama-3-70B-Instruct）            | 必须                                                             |
|                   | `--cfg_path`      | `str` (YAML)              | 基准测试配置文件（例如，benchmarks/General/LongBench.yaml）       | 必须                                                             |
|                   | `--device`        | `List[int]`               | GPU索引（例如，`0 1 3`）                                          | 默认值：所有可用GPU                                              |
|                   | `--torch_dtype`   | `str \| torch.dtype`      | 精度（例如，`float16`，`bfloat16`）                                | 默认值：`torch.bfloat16`                                         |
| **GPU分配**       |                   |                           |                                                                  |                                                                  |
|                   | `--gp_num`        | `int \| str`              | 每个模型的GPU数量（例如，`2`）*或*每个GPU的模型数量（例如，`'1/2'`） | 默认值：1                                                        |
| **优化选项**      |                   |                           |                                                                  |                                                                  |
|                   | `--acceleration`  | `str`                     | 实验性方法（`H2O`，`SnapKV`，`StreamingLLM`，...）                | 选项：{“H2O”，“StreamingLLM”，“SnapKV”，“L2Compress”，“FlexPrefill”，“PyramidKV”，“CakeKV”，“KIVI”，“ThinK”，“CaM”} |

## 🤖 支持模型

本工具包支持广泛的模型系列：

- **除FlexPrefill外所有方法**：支持`llama`和`mistral`系列
- **FlexPrefill**：支持`llama`、`glm`和`qwen2`系列
- **CakeKV**：支持`llama`、`mistral`和`qwen2`系列
- **XAttention**：支持`llama`系列

### ⚠️ 注意事项

在运行推理之前，请确保您自定义了 `acceleration/xxx/config` 目录下的相关配置文件，以满足您的特定需求。例如，你可能需要调整文件中的设置，如 `.models/acceleration/cake/config/cake_config.json `。

### ⚠️ 重要提示

除了L2Compress之外的所有加速方法都可以在单张40GB A100显卡上用于长度为128K的推理任务。L2Compress推荐用于三卡推理，因为在KV缓存压缩过程中，这种方法会跳过浅层并保留它们的完整KV缓存，这使得它在长文本任务中相比其他方法会占用更高的显存。如果显存大小仅有24GB，则在所有情况下都推荐使用多卡推理。

## 📊 实验结果

我们使用 Llama-3.1-8B-Instruct 模型在 RULER 测试基准上进行了性能评估，并展示了不同 GPU 硬件（NVIDIA 3090、40GB A100 和 H20）下的加速方法测试结果。

### > 推理时间
实验配置如下：
```ymal
benchmark_name: RULER
task_names: ["cwe","niah_multikey_1", "niah_multikey_2", "niah_multikey_3", "niah_multiquery", "niah_multivalue", "niah_single_1", "niah_single_2", "niah_single_3", "qa_1", "qa_2", "vt"]
length: [4096,8192,16384,32768,65536,131072] 

num_samples: 10 # Set the number of samples for each task to num_samples
no_template_tasks: ["fwe"]  # task_names or "all" 
template: NULL  # choose from ./models/utils/build_chat.py
```
破折号“-”表示内存不足（OOM）错误。在这些方法中，CAM无法有效处理长文本推理任务，而FlexPrefill在所有测试场景中均表现出最佳的性能和速度。

#### NVIDIA 3090表现

| 序列长度 | KIVI | H2O | StreamingLLM | L2Compress | CaM | CakeKV | PyramidKV | FlexPrefill | SnapKV | ThinK | Transformers | vLLM |
|------------|------|-----|--------------|--------|-----|--------|-----------|-------------|--------|-------|--------------|------|
| 4K         | 94.83 | 46.13 | 27.79 | 36.42 | 67.39 | 91.88 | 77.42 | 93.33 | 81.54 | 81.83 | 94.83 | 93.67 |
| 8K         | 94.29 | 34.42 | 22.21 | 27.75 | 59.60 | 82.42 | 71.25 | 87.92 | 73.29 | 72.88 | 94.42 | 93.04 |
| 16K        | 91.08 | 21.63 | 11.46 | 27.58 | 48.53 | 78.96 | 68.75 | 83.38 | 67.88 | 67.25 | 92.50 | 91.50 |
| 32K             | 89.50   | 19.67   | 10.79        | 22.17      | 43.42    | 79.54   | 64.33     | 83.38       | 66.29   | 66.21   | 92.75        | 88.17   |
| 64K        | - | 10.54 | 7.67 | 18.25 | 33.25 | 62.88 | 57.29 | 65.04 | 55.58 | 56.92 | 83.71 | 78.71 |
| 128K       | - | 6.38 | 9.79 | 12.17 | 28.10 | 53.17 | 48.29 | 46.58 | 47.25 | 48.71 | - | 16.67 |
| 总耗时 | 1:29:58 | 6:11:21 | 6:07:27 | 6:08:31 | 50:31:13 | 6:13:01 | 6:14:12 | 3:58:17 | 6:08:31 | 6:18:08 | 3:11:15 | 4:47:37 |

#### 40GB A100表现

| 序列长度 | KIVI | H2O | StreamingLLM | L2Compress | CaM | CakeKV | PyramidKV | FlexPrefill | SnapKV | ThinK | Transformers | vLLM |
|------------|------|-----|--------------|--------|-----|--------|-----------|-------------|--------|-------|--------------|------|
| 4K         | 95.46 | 46.38 | 28.83 | 33.96 | 67.29 | 89.67 | 77.00 | 93.58 | 79.75 | 79.33 | 95.04 | 90.71 |
| 8K         | 94.42 | 31.96 | 21.58 | 26.83 | 59.67 | 82.46 | 70.79 | 89.04 | 72.67 | 71.92 | 94.46 | 91.63 |
| 16K        | 90.04 | 23.00 | 10.63 | 27.92 | 48.53 | 77.29 | 67.54 | 84.83 | 66.63 | 65.25 | 92.50 | 91.75 |
| 32K             | 89.00   | 19.86   | 10.79        | 20.83      | 43.21    | 76.42   | 65.25     | 79.63       | 63.88   | 66.46   | 92.08        | 86.25   |
| 64K             | 90.25   | 10.50   | 7.67         | 17.25      | 33.35    | 65.79   | 55.54     | 62.50       | 54.75   | 55.75   | 83.71        | 78.83   |
| 128K            | 67.92   | 6.17    | 10.63        | 12.17      | 28.03    | 55.21   | 49.54     | 49.42       | 50.79   | 51.00   | 75.21        | 17.50   |
| 总耗时 | 3:00:05 | 2:50:26 | 2:44:12      | 2:56:36    | 25:41:21 | 2:58:21 | 2:57:54   | 1:52:41     | 2:52:51 | 3:09:58 | 3:23:10      | 2:15:38 |
| Batch=8时 | 0:32:57 | 0:27:00 | 0:26:35      | 0:27:26    | - | 0:28:26 | 0:26:39   | 0:15:58     | 0:26:44 | 0:35:46 | -            | 0:27:20 |

#### NVIDIA H20表现

| 序列长度 | KIVI | H2O | StreamingLLM | L2Compress | CaM | CakeKV | PyramidKV | FlexPrefill | SnapKV | ThinK | Transformers | vLLM |
|------------|------|-----|--------------|--------|-----|--------|-----------|-------------|--------|-------|--------------|------|
| 4K         | 93.79 | 54.33 | 28.00 | 34.21 | 66.29 | 89.67 | 77.13 | 92.63 | 82.22 | 80.83 | 94.00 | 91.75 |
| 8K         | 94.33 | 41.25 | 21.58 | 26.75 | 59.67 | 82.63 | 71.50 | 87.88 | 70.46 | 72.33 | 94.46 | 88.79 |
| 16K        | 90.71 | 24.17 | 10.63 | 27.00 | 48.33 | 77.21 | 65.33 | 84.54 | 64.57 | 65.92 | 92.50 | 84.58 |
| 32K        | 88.08 | 19.42 | 10.79 | 22.58 | 43.42 | 76.42 | 66.21 | 80.71 | 69.27 | 66.46 | 91.88 | 79.50 |
| 64K        | 80.91 | 12.33 | 6.83 | 16.58 | 34.25 | 65.71 | 58.25 | 62.96 | 57.73 | 56.25 | 83.71 | 77.79 |
| 128K       | 69.13 | 10.92 | 10.63 | 12.25 | 28.13 | 54.21 | 47.58 | 48.08 | 45.94 | 47.25 | 75.21 | 57.88 |
| 总耗时 | 3:31:35 | 3:20:18 | 3:21:25 | 3:29:34 | 26:11:23 | 3:22:06 | 3:22:05 | 1:49:35 | 3:26:09 | 3:35:32 | 3:57:44 | 2:22:45 |
| Batch=8时 | 0:33:44 | 0:33:08 | 0:32:35 | 00:33:48 | - | 0:32:37 | 0:32:41 | 0:13:10 | 0:32:41 | 0:34:25 | - | 0:38:56 |