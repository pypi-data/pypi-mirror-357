# Copyright 2024 GLM-4-9B Model Team @ Zhipu AI
# Copyright 2024 ByteDance and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch ChatGLM model."""

from flash_attn import flash_attn_func

from ...ops.vertical_slash_attention import vertical_slash_attention


def glm_vertical_slash_attention_forward(
    self, query_states, key_states, value_states, attention_mask
):
    query_states = query_states.transpose(1, 2)
    key_states = key_states.transpose(1, 2)
    value_states = value_states.transpose(1, 2)
    batch_size, query_length = query_states.shape[:2]

    if query_states.shape[1] > 1:
        attn_output = vertical_slash_attention(
            query_states,
            key_states,
            value_states,
            block_size=self.config.block_size,
            vertical_size=self.config.vertical_size,
            slash_size=self.config.slash_size,
        )
    else:
        attn_output = flash_attn_func(query_states, key_states, value_states)

    attn_output = attn_output.reshape(
        batch_size, query_length, self.hidden_size_per_partition
    ).contiguous()
    return attn_output
