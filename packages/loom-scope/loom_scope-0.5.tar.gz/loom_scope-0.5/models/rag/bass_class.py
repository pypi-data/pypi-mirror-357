import json
import os
import re
import sys
import shutil
from loguru import logger
from tqdm import tqdm

logger.remove();logger.add(sys.stdout,colorize=True, format="<level>{message}</level>")
class Base:
    def __init__(self,config,**kwargs):
        self.chunk_size = config["chunk_size"]
        self.num_chunks = config["num_chunks"]
        self.chunk_overlap = config["chunk_overlap"]
    def traverse_rawdata(self,RawDataSet,rag_data_path,**kwargs):
        rag_data_path = f"{rag_data_path}/{self.retrieval_methods}"
        for RawDataSet in tqdm(RawDataSet):
            raw_input,task_name,benchmark = RawDataSet
            context,question = benchmark.extract_contextAndquestion(raw_input=raw_input,task_name=task_name)
            if not question:question = " "
            chunks = self.retrieve(context,question)
            raw_input = benchmark.update_context(raw_input=raw_input,chunks=chunks,task_name=task_name)
            save_path = f"{rag_data_path}/{task_name}.jsonl"
            os.makedirs(f"{rag_data_path}",exist_ok=True)
            with open(save_path,"a") as f:
                f.write(json.dumps(raw_input,ensure_ascii=False)+"\n")
        return RawDataSet
    def retrieve(self,context,question,**kwargs):
        chunks = self.retrieve_relevant_chunks_for_question(context,question)
        return chunks

    def chunk_text(self, context):
        max_length = self.chunk_size
        if not hasattr(self, 'chunk_size') or max_length <= 0:
            raise ValueError("chunk_size must be a positive integer")
        if isinstance(context, str):
            texts = [context] if context.strip() else []
        else:
            texts = [text for text in (context or []) if isinstance(text, str) and text.strip()]
        
        if not texts:
            return []
    
        def count_tokens(text):
            chinese_count = len(re.findall(r'[\u4e00-\u9fa5]', text))
            english_count = len(re.findall(r'[a-zA-Z]+', text))
            number_count = len(re.findall(r'\d+', text))
            return chinese_count + english_count + number_count
        
        def normalize_spaces(text):
            text = re.sub(r'([\u4e00-\u9fa5])([a-zA-Z0-9])', r'\1 \2', text)
            text = re.sub(r'([a-zA-Z0-9])([\u4e00-\u9fa5])', r'\1 \2', text)
            text = re.sub(r'\s+', ' ', text)
            return text.strip()
        
        def split_long_text(text, max_len):
            if count_tokens(text) <= max_len:
                return [text]
            tokens = re.findall(r'[\u4e00-\u9fa5]|[a-zA-Z]+|\d+|[^\u4e00-\u9fa5a-zA-Z\d]+', text)
            chunks = []
            current_chunk = []
            current_count = 0
            for token in tokens:
                if re.match(r'[\u4e00-\u9fa5]|[a-zA-Z]+|\d+', token):
                    token_count = 1
                else:
                    token_count = 0
                if current_count + token_count <= max_len:
                    current_chunk.append(token)
                    current_count += token_count
                else:
                    if current_chunk:
                        chunks.append(normalize_spaces(''.join(current_chunk)))
                    current_chunk = [token]
                    current_count = token_count
            
            if current_chunk:
                chunks.append(normalize_spaces(''.join(current_chunk)))
            
            return chunks
        
        all_chunks = []
        
        for text in texts:
            sentence_pattern = r'(?<=[.!?。！？；;：:\n])\s*(?=[^\s])'
            sentences = re.split(sentence_pattern, text.strip())
            sentences = [s.strip() for s in sentences if s.strip()]
            
            current_sentences = []
            current_length = 0
            
            for sentence in sentences:
                sentence_length = count_tokens(sentence)
                
                if sentence_length > max_length:
                    if current_sentences:
                        chunk = normalize_spaces(' '.join(current_sentences))
                        all_chunks.append(chunk)
                        current_sentences = []
                        current_length = 0
                    sub_chunks = split_long_text(sentence, max_length)
                    all_chunks.extend(sub_chunks)
                    
                elif current_length + sentence_length <= max_length:
                    current_sentences.append(sentence)
                    current_length += sentence_length
                    
                else:
                    if current_sentences:
                        chunk = normalize_spaces(' '.join(current_sentences))
                        all_chunks.append(chunk)
                    
                    current_sentences = [sentence]
                    current_length = sentence_length
            
            if current_sentences:
                chunk = normalize_spaces(' '.join(current_sentences))
                all_chunks.append(chunk)
        
        return [chunk for chunk in all_chunks if chunk]