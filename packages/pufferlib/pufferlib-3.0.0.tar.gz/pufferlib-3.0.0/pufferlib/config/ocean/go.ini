[base]
package = ocean
env_name = puffer_go
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024
reward_move_pass = -0.6026362603175613
reward_move_valid = 0
reward_move_invalid = -0.5393516480382454
reward_opponent_capture = -0.3152783593705354
reward_player_capture = 0.42122681325442923
grid_size = 7

[vec]
num_envs = 8

[train]
total_timesteps = 100_000_000
adam_beta1 = 0.5686370767889766
adam_beta2 = 0.9999454817221638
adam_eps = 2.007252656207671e-12
bptt_horizon = 64
clip_coef = 0.17930104885238807
ent_coef = 0.0018946598458748304
gae_lambda = 0.9831319174802507
gamma = 0.9480351741863737
learning_rate = 0.031603809039284864
max_grad_norm = 1.320177349287771
minibatch_size = 8192
prio_alpha = 0.6979639079178326
prio_beta0 = 0.5614257332458639
vf_clip_coef = 1.1755607092687304
vf_coef = 1.6195967557187005
vtrace_c_clip = 0
vtrace_rho_clip = 4.060318960532289

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 5e8
mean = 2e8
scale = 0.25

[sweep.env.reward_move_invalid]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5

[sweep.env.reward_move_pass]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5

[sweep.env.reward_player_capture]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = 0.5

[sweep.env.reward_opponent_capture]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5
