[base]
package = metta
env_name = metta 
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 512
num_workers = 16
batch_size = 256

[env]
render_mode = auto

[train]
total_timesteps = 150_000_000

adam_beta1 = 0.9564306102799757
adam_beta2 = 0.99999
adam_eps = 9.783476537284348e-13
batch_size = 786432
clip_coef = 0.4892960366937307
ent_coef = 0.0021495271282999063
gae_lambda = 0.9548445797172682
gamma = 0.9970473490110006
learning_rate = 0.0034144261695800942
max_grad_norm = 4.131048552978143
max_minibatch_size = 32768
minibatch_size = 32768
prio_alpha = 0.09999999999999998
prio_beta0 = 0.6414824826502135
vf_clip_coef = 0.7479924868175252
vf_coef = 4.596909413885139
vtrace_c_clip = 2.8594046341079227
vtrace_rho_clip = 1.0478534705178881

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 5e8
mean = 1e8
scale = auto

[sweep.env.ore_reward]
distribution = uniform
min = 0.0
mean = 0.25
max = 1.0
scale = auto

[sweep.env.heart_reward]
distribution = uniform
min = 0.0
mean = 0.5
max = 1.0
scale = auto

[sweep.env.battery_reward]
distribution = uniform
min = 0.0
mean = 0.25
max = 1.0
scale = auto


