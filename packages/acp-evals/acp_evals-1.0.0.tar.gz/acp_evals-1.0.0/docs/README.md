# ACP Evals Documentation

Evaluation framework for AI agents with three core evaluators.

## Overview

ACP Evals provides a streamlined API for evaluating AI agent performance across three key dimensions:

- **AccuracyEval** - Evaluates response quality and correctness using LLM-as-judge
- **PerformanceEval** - Measures latency and memory usage
- **ReliabilityEval** - Validates tool usage and consistency

## Documentation

- [Setup Guide](./setup.md) - Installation and configuration
- [API Reference](./api-reference.md) - Complete API documentation
- [Architecture](./architecture.md) - System design and implementation
- [LLM Evaluators Guide](./llm-evaluators-guide.md) - Understanding LLM-as-judge
- [Providers](./providers.md) - Configuring LLM providers
- [Troubleshooting](./troubleshooting.md) - Common issues and solutions