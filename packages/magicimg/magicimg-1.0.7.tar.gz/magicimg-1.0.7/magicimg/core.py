"""
X·ª≠ l√Ω v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh cho h·ªá th·ªëng OCR
"""

import cv2
import numpy as np
import os
import logging
import time
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union
from dataclasses import dataclass
import json
import gc
import torch
import contextlib

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@contextlib.contextmanager
def gpu_memory_manager():
    """Context manager ƒë·ªÉ qu·∫£n l√Ω b·ªô nh·ªõ GPU"""
    try:
        yield
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

@dataclass
class ImageQualityMetrics:
    """L∆∞u tr·ªØ c√°c ch·ªâ s·ªë ch·∫•t l∆∞·ª£ng ·∫£nh"""
    blur_index: float
    dark_ratio: float
    brightness: float
    contrast: float
    resolution: Tuple[int, int]
    quality_score: float = 0.0
    warnings: List[str] = None
    
    def __post_init__(self):
        self.warnings = self.warnings or []
        
    def to_dict(self) -> Dict:
        return {
            "blur_index": self.blur_index,
            "dark_ratio": self.dark_ratio,
            "brightness": self.brightness,
            "contrast": self.contrast,
            "resolution": self.resolution,
            "quality_score": self.quality_score,
            "warnings": self.warnings
        }

@dataclass
class ProcessingResult:
    """K·∫øt qu·∫£ x·ª≠ l√Ω ·∫£nh"""
    success: bool
    image: Optional[np.ndarray]
    quality_metrics: ImageQualityMetrics
    processing_steps: List[str]
    output_path: Optional[str] = None
    error_message: Optional[str] = None
    debug_images: Dict[str, str] = None
    rotation_angle: float = 0.0  # Th√™m g√≥c xoay
    
    def __post_init__(self):
        self.debug_images = self.debug_images or {}
        
    def to_dict(self) -> Dict:
        return {
            "success": self.success,
            "quality_metrics": self.quality_metrics.to_dict(),
            "processing_steps": self.processing_steps,
            "output_path": self.output_path,
            "error_message": self.error_message,
            "debug_images": self.debug_images,
            "rotation_angle": self.rotation_angle
        }

class ImageProcessor:
    """X·ª≠ l√Ω v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh"""
    
    def __init__(self, debug_dir: Optional[str] = None, config: Optional[Dict[str, Any]] = None):
        """Kh·ªüi t·∫°o v·ªõi c·∫•u h√¨nh t√πy ch·ªânh
        
        Args:
            debug_dir: Th∆∞ m·ª•c l∆∞u ·∫£nh debug, n·∫øu None th√¨ t·∫Øt ch·∫ø ƒë·ªô debug
            config: C·∫•u h√¨nh t√πy ch·ªânh cho x·ª≠ l√Ω ·∫£nh
        """
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        self.config = {
            # C·∫•u h√¨nh GPU
            "use_gpu": True,              # B·∫≠t/t·∫Øt GPU
            "gpu_id": 0,                  # ID c·ªßa GPU mu·ªën s·ª≠ d·ª•ng
            "gpu_memory_limit": 0.7,      # Gi·ªõi h·∫°n b·ªô nh·ªõ GPU (70%)
            "batch_size": 4,              # S·ªë ·∫£nh x·ª≠ l√Ω c√πng l√∫c
            "parallel_jobs": 2,           # S·ªë lu·ªìng x·ª≠ l√Ω song song
            "optimize_for_gpu": True,     # T·ªëi ∆∞u c√°c ph√©p t√≠nh cho GPU
            
            # Ng∆∞·ª°ng ch·∫•t l∆∞·ª£ng
            "min_blur_index": 80.0,       # Gi·∫£m y√™u c·∫ßu ƒë·ªô n√©t v√¨ ·∫£nh scan th∆∞·ªùng h∆°i m·ªù
            "max_dark_ratio": 0.2,        # Gi·∫£m ng∆∞·ª°ng pixel t·ªëi v√¨ ·∫£nh phi·∫øu th∆∞·ªùng s√°ng
            "min_brightness": 180.0,      # TƒÉng y√™u c·∫ßu ƒë·ªô s√°ng v√¨ c·∫ßn ƒë·ªçc text r√µ r√†ng
            "min_contrast": 50.0,         # TƒÉng y√™u c·∫ßu ƒë·ªô t∆∞∆°ng ph·∫£n ƒë·ªÉ ph√¢n bi·ªát text v√† n·ªÅn
            "min_resolution": (1000, 1400), # TƒÉng ƒë·ªô ph√¢n gi·∫£i t·ªëi thi·ªÉu cho ·∫£nh phi·∫øu
            "min_quality_score": 0.7,     # TƒÉng y√™u c·∫ßu ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ
            
            # Ng∆∞·ª°ng x·ª≠ l√Ω
            "min_skew_angle": 0.3,        # Gi·∫£m ng∆∞·ª°ng ph√°t hi·ªán g√≥c nghi√™ng
            "max_skew_angle": 30.0,       # Gi·∫£m g√≥c nghi√™ng t·ªëi ƒëa v√¨ phi·∫øu th∆∞·ªùng kh√¥ng b·ªã nghi√™ng nhi·ªÅu
            "min_rotation_confidence": 0.8,# 
            
            # C·∫•u h√¨nh debug
            "debug": debug_dir is not None,
            "debug_dir": debug_dir,
            
            # C·∫•u h√¨nh x·ª≠ l√Ω
            "skip_rotation": False,        # Th√™m flag ƒë·ªÉ b·ªè qua xoay ·∫£nh
            "reuse_rotation": None,        # G√≥c xoay ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng
            
            # T·ª´ kh√≥a b·ªï sung cho phi·∫øu b·∫ßu
            "ballot_keywords": [
                "phieu bau cu", "doan dai bieu", "dai hoi", "dang bo",
                "nhiem ky", "stt", "ho va ten", "ha noi", "ngay", "thang", "nam"
            ],

            # Tham s·ªë ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª ngang
            "line_detection": {
                "min_line_length_ratio": 0.6,     # Gi·∫£m y√™u c·∫ßu chi·ªÅu d√†i ƒë∆∞·ªùng k·∫ª
                "min_length_threshold_ratio": 0.7, # Gi·∫£m ng∆∞·ª°ng l·ªçc
                "max_line_height": 3,            # Gi·ªØ nguy√™n
                "morph_iterations": 1,           # Gi·ªØ nguy√™n
                "histogram_threshold_ratio": 0.5, # Gi·∫£m ng∆∞·ª°ng histogram
                "min_line_distance": 20,         # Gi·∫£m kho·∫£ng c√°ch t·ªëi thi·ªÉu
                "dilate_kernel_div": 30,         # Gi·∫£m v·ªÅ gi√° tr·ªã c≈©
                "horizontal_kernel_div": 5,       # Gi·∫£m v·ªÅ gi√° tr·ªã c≈©
                "projection_threshold_div": 4     # Gi·∫£m v·ªÅ gi√° tr·ªã c≈©
            },

            # Tham s·ªë c·∫Øt h√†ng
            "row_extraction": {
                "top_margin": 8,        # L·ªÅ tr√™n khi c·∫Øt h√†ng (pixel)
                "bottom_margin": 8,     # L·ªÅ d∆∞·ªõi khi c·∫Øt h√†ng (pixel)
                "safe_zone": 5,         # V√πng an to√†n xung quanh ƒë∆∞·ªùng k·∫ª (pixel)
                "min_row_height": 20,   # Chi·ªÅu cao t·ªëi thi·ªÉu c·ªßa h√†ng
                "check_text": True,     # C√≥ ki·ªÉm tra text trong h√†ng kh√¥ng
                "text_margin": 3,       # L·ªÅ th√™m n·∫øu ph√°t hi·ªán text g·∫ßn bi√™n
                "min_text_area": 0.003  # T·ª∑ l·ªá di·ªán t√≠ch text t·ªëi thi·ªÉu ƒë·ªÉ coi l√† c√≥ text
            }
        }
        
        # C·∫≠p nh·∫≠t c·∫•u h√¨nh t·ª´ tham s·ªë
        if config:
            self.config.update(config)
            
        # T·∫°o th∆∞ m·ª•c debug n·∫øu c·∫ßn
        if self.config["debug"] and self.config["debug_dir"]:
            os.makedirs(self.config["debug_dir"], exist_ok=True)
            
        # Kh·ªüi t·∫°o GPU n·∫øu c√≥ th·ªÉ
        self.has_gpu = False
        self.gpu_info = None
        if self.config["use_gpu"]:
            self._setup_gpu()
            
        # Ghi log c·∫•u h√¨nh (ch·ªâ th√¥ng tin c·∫ßn thi·∫øt)
        logger.debug("Kh·ªüi t·∫°o ImageProcessor v·ªõi c·∫•u h√¨nh:")
        for key, value in self.config.items():
            logger.debug(f"  - {key}: {value}")
        
        self.debug_dir = self.config["debug_dir"]
        self.debug = self.config["debug"]
        
        # C·∫≠p nh·∫≠t t·ª´ kh√≥a ti·∫øng Vi·ªát
        self.keywords = self.config.get("ballot_keywords", []) + [
            # T·ª´ kh√≥a h√†nh ch√≠nh
            "bau cu", "dang", "cong hoa", "xa hoi", "viet nam",
            "phieu", "ho ten", "stt", "so thu tu", "chu ky",
            
            # T·ª´ v√† h·ªç ph·ªï bi·∫øn trong phi·∫øu b·∫ßu
            "nguyen", "tran", "le", "pham", "hoang", "do",
            "thi", "van", "duc", "anh", "quang", "thanh"
        ]
        
        # Ch·ªØ c√°i ƒë·∫∑c bi·ªát ƒë·ªÉ x√°c ƒë·ªãnh h∆∞·ªõng d·ª±a tr√™n contour
        self.orientation_chars = ['c', 'C', 'a', 'A', 'v', 'V', 'n', 'N', '9', '6', 'p', 'q', 'g']
        
    def _setup_gpu(self):
        """Thi·∫øt l·∫≠p v√† ki·ªÉm tra GPU"""
        try:
            if torch.cuda.is_available():
                # Ch·ªçn GPU
                torch.cuda.set_device(self.config["gpu_id"])
                
                # L·∫•y th√¥ng tin GPU
                gpu_name = torch.cuda.get_device_name()
                total_memory = torch.cuda.get_device_properties(self.config["gpu_id"]).total_memory
                memory_limit = int(total_memory * self.config["gpu_memory_limit"])
                
                # Thi·∫øt l·∫≠p gi·ªõi h·∫°n b·ªô nh·ªõ
                torch.cuda.set_per_process_memory_fraction(self.config["gpu_memory_limit"])
                
                self.has_gpu = True
                self.gpu_info = {
                    "name": gpu_name,
                    "total_memory": total_memory,
                    "memory_limit": memory_limit,
                    "device_id": self.config["gpu_id"]
                }
                
                logger.debug(f"S·ª≠ d·ª•ng GPU: {gpu_name}")
                logger.debug(f"B·ªô nh·ªõ GPU: {total_memory/1024/1024:.1f}MB")
                logger.debug(f"Gi·ªõi h·∫°n b·ªô nh·ªõ: {memory_limit/1024/1024:.1f}MB")
            else:
                logger.warning("Kh√¥ng t√¨m th·∫•y GPU, s·ª≠ d·ª•ng CPU")
                self.has_gpu = False
                
        except Exception as e:
            logger.error(f"L·ªói kh·ªüi t·∫°o GPU: {str(e)}")
            self.has_gpu = False
            
    def _to_gpu(self, image: np.ndarray) -> torch.Tensor:
        """Chuy·ªÉn ·∫£nh l√™n GPU"""
        if self.has_gpu:
            with gpu_memory_manager():
                tensor = torch.from_numpy(image).cuda()
                return tensor
        return torch.from_numpy(image)
        
    def _to_cpu(self, tensor: torch.Tensor) -> np.ndarray:
        """Chuy·ªÉn tensor v·ªÅ CPU"""
        if tensor.is_cuda:
            with gpu_memory_manager():
                return tensor.cpu().numpy()
        return tensor.numpy()
        
    def _process_batch(self, images: List[np.ndarray], process_fn) -> List[np.ndarray]:
        """X·ª≠ l√Ω m·ªôt batch ·∫£nh tr√™n GPU"""
        if not self.has_gpu:
            return [process_fn(img) for img in images]
            
        with gpu_memory_manager():
            # Chuy·ªÉn batch l√™n GPU
            batch = torch.stack([self._to_gpu(img) for img in images])
            
            # X·ª≠ l√Ω batch
            processed = process_fn(batch)
            
            # Chuy·ªÉn k·∫øt qu·∫£ v·ªÅ CPU
            results = [self._to_cpu(img) for img in processed]
            
            return results
            
    def __del__(self):
        """Gi·∫£i ph√≥ng t√†i nguy√™n khi h·ªßy ƒë·ªëi t∆∞·ª£ng"""
        if hasattr(self, 'has_gpu') and self.has_gpu:
            with gpu_memory_manager():
                torch.cuda.empty_cache()
                gc.collect()
                
    def _check_image_quality(self, image) -> ImageQualityMetrics:
        """Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ·∫£nh v√† tr·∫£ v·ªÅ metrics
        
        Args:
            image: ·∫¢nh c·∫ßn ki·ªÉm tra (numpy array)
            
        Returns:
            ImageQualityMetrics: Th√¥ng tin ch·∫•t l∆∞·ª£ng ·∫£nh
        """
        # Chuy·ªÉn ƒë·ªïi sang ·∫£nh x√°m n·∫øu c·∫ßn
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
            
        # T√≠nh c√°c ch·ªâ s·ªë ch·∫•t l∆∞·ª£ng
        blur_index = cv2.Laplacian(gray, cv2.CV_64F).var()
        dark_pixels = np.sum(gray < 50)
        dark_ratio = dark_pixels / (gray.shape[0] * gray.shape[1])
        brightness = np.mean(gray)
        contrast = np.std(gray.astype(np.float32))
        resolution = image.shape[:2][::-1]  # width, height
        
        # Thu th·∫≠p c·∫£nh b√°o
        warnings = []
        if blur_index < self.config["min_blur_index"]:
            warnings.append("·∫¢nh qu√° m·ªù")
        if dark_ratio > self.config["max_dark_ratio"]:
            warnings.append("·∫¢nh qu√° t·ªëi")
        if brightness < self.config["min_brightness"]:
            warnings.append("ƒê·ªô s√°ng kh√¥ng ƒë·ªß")
        if contrast < self.config["min_contrast"]:
            warnings.append("ƒê·ªô t∆∞∆°ng ph·∫£n kh√¥ng ƒë·ªß")
        if resolution[0] < self.config["min_resolution"][0] or resolution[1] < self.config["min_resolution"][1]:
            warnings.append("ƒê·ªô ph√¢n gi·∫£i kh√¥ng ƒë·ªß")
            
        # T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng t·ªïng h·ª£p
        quality_score = (
            (min(blur_index / self.config["min_blur_index"], 2.0) * 0.3) +
            ((1 - dark_ratio / self.config["max_dark_ratio"]) * 0.2) +
            (min(brightness / self.config["min_brightness"], 1.5) * 0.25) +
            (min(contrast / self.config["min_contrast"], 1.5) * 0.25)
        )
        quality_score = min(1.0, max(0.0, quality_score))
        
        return ImageQualityMetrics(
            blur_index=blur_index,
            dark_ratio=dark_ratio,
            brightness=brightness,
            contrast=contrast,
            resolution=resolution,
            quality_score=quality_score,
            warnings=warnings
        )
    
    def check_quality(self, image, prefix=None):
        """Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ·∫£nh
        
        Args:
            image: ·∫¢nh c·∫ßn ki·ªÉm tra
            prefix: Ti·ªÅn t·ªë cho t√™n file debug
            
        Returns:
            tuple: (is_good, quality_info, enhanced_image)
        """
        # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng
        quality_metrics = self._check_image_quality(image)
        
        # Log th√¥ng tin ch·∫•t l∆∞·ª£ng
        logger.debug("Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ·∫£nh:")
        logger.debug(f"  - Ch·ªâ s·ªë m·ªù (blur index): {quality_metrics.blur_index:.2f} (t·ªëi thi·ªÉu: {self.config['min_blur_index']})")
        logger.debug(f"  - T·ª∑ l·ªá pixel t·ªëi: {quality_metrics.dark_ratio:.4f} (t·ªëi ƒëa: {self.config['max_dark_ratio']})")
        logger.debug(f"  - ƒê·ªô s√°ng trung b√¨nh: {quality_metrics.brightness:.2f} (t·ªëi thi·ªÉu: {self.config['min_brightness']})")
        logger.debug(f"  - ƒê·ªô t∆∞∆°ng ph·∫£n: {quality_metrics.contrast:.2f} (t·ªëi thi·ªÉu: {self.config['min_contrast']})")
        logger.debug(f"  - ƒê·ªô ph√¢n gi·∫£i: {quality_metrics.resolution[0]}x{quality_metrics.resolution[1]} (t·ªëi thi·ªÉu: {self.config['min_resolution'][0]}x{self.config['min_resolution'][1]})")
        
        # Ki·ªÉm tra t·ª´ng ti√™u ch√≠
        blur_ok = quality_metrics.blur_index >= self.config["min_blur_index"]
        dark_ok = quality_metrics.dark_ratio <= self.config["max_dark_ratio"]
        brightness_ok = quality_metrics.brightness >= self.config["min_brightness"]
        contrast_ok = quality_metrics.contrast >= self.config["min_contrast"]
        resolution_ok = (quality_metrics.resolution[0] >= self.config["min_resolution"][0] and 
                       quality_metrics.resolution[1] >= self.config["min_resolution"][1])
        
        is_good = all([blur_ok, dark_ok, brightness_ok, contrast_ok, resolution_ok])
        
        if not is_good:
            logger.warning("·∫¢nh kh√¥ng ƒë·∫°t ch·∫•t l∆∞·ª£ng y√™u c·∫ßu!")
            if not blur_ok:
                logger.warning("  - ·∫¢nh qu√° m·ªù")
            if not dark_ok:
                logger.warning("  - ·∫¢nh qu√° t·ªëi")
            if not brightness_ok:
                logger.warning("  - ƒê·ªô s√°ng kh√¥ng ƒë·ªß")
            if not contrast_ok:
                logger.warning("  - ƒê·ªô t∆∞∆°ng ph·∫£n kh√¥ng ƒë·ªß")
            if not resolution_ok:
                logger.warning("  - ƒê·ªô ph√¢n gi·∫£i kh√¥ng ƒë·ªß")
        
        # N√¢ng cao ch·∫•t l∆∞·ª£ng ·∫£nh n·∫øu c·∫ßn
        enhanced_image = self.enhance_image(image, quality_metrics)
        
        return is_good, quality_metrics, enhanced_image
    
    def enhance_image(self, image: np.ndarray, quality_info: Union[Dict, ImageQualityMetrics]) -> np.ndarray:
        """TƒÉng c∆∞·ªùng ch·∫•t l∆∞·ª£ng ·∫£nh m·ªôt c√°ch nh·∫π nh√†ng
        
        Args:
            image: ·∫¢nh c·∫ßn x·ª≠ l√Ω
            quality_info: Th√¥ng tin ch·∫•t l∆∞·ª£ng ·∫£nh (dict ho·∫∑c ImageQualityMetrics)
            
        Returns:
            np.ndarray: ·∫¢nh ƒë√£ tƒÉng c∆∞·ªùng
        """
        # Chuy·ªÉn dict th√†nh ImageQualityMetrics n·∫øu c·∫ßn
        if isinstance(quality_info, dict):
            quality_info = ImageQualityMetrics(**quality_info)
            
        result = image.copy()
        needs_enhancement = False
        
        # Ch·ªâ tƒÉng c∆∞·ªùng khi th·∫≠t s·ª± c·∫ßn thi·∫øt v·ªõi m·ª©c ƒë·ªô nh·∫π
        if quality_info.brightness < self.config["min_brightness"] * 0.8:  # Ch·ªâ khi qu√° t·ªëi
            alpha = min(1.2, self.config["min_brightness"] / quality_info.brightness)  # Gi·ªõi h·∫°n 20%
            result = cv2.convertScaleAbs(result, alpha=alpha, beta=0)
            needs_enhancement = True
            
        if quality_info.contrast < self.config["min_contrast"] * 0.7:  # Ch·ªâ khi qu√° nh·∫°t
            # TƒÉng contrast nh·∫π nh√†ng
            lab = cv2.cvtColor(result, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            l = clahe.apply(l)
            result = cv2.merge([l, a, b])
            result = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)
            needs_enhancement = True
            
        # N·∫øu kh√¥ng c·∫ßn tƒÉng c∆∞·ªùng, tr·∫£ v·ªÅ ·∫£nh g·ªëc
        if not needs_enhancement:
            return image
            
        return result
            
    def enhance_image_for_ocr(self, image: np.ndarray) -> np.ndarray:
        """T·ªëi ∆∞u h√≥a ·∫£nh cho OCR
        
        Args:
            image: ·∫¢nh c·∫ßn x·ª≠ l√Ω
            
        Returns:
            np.ndarray: ·∫¢nh ƒë√£ t·ªëi ∆∞u
        """
        if self.has_gpu:
            with gpu_memory_manager():
                # Chuy·ªÉn ·∫£nh l√™n GPU
                img_tensor = self._to_gpu(image)
                
                # Chuy·ªÉn sang ·∫£nh x√°m
                if len(image.shape) == 3:
                    weights = torch.tensor([0.299, 0.587, 0.114], device=img_tensor.device)
                    img_tensor = torch.sum(img_tensor * weights, dim=2)
                
                # √Åp d·ª•ng ng∆∞·ª°ng th√≠ch ·ª©ng
                block_size = 11
                C = 2
                mean = torch.nn.functional.avg_pool2d(
                    img_tensor.unsqueeze(0).unsqueeze(0),
                    block_size,
                    stride=1,
                    padding=block_size//2
                ).squeeze()
                
                thresh = mean - C
                binary = (img_tensor > thresh).float() * 255
                
                # Chuy·ªÉn v·ªÅ CPU
                return self._to_cpu(binary).astype(np.uint8)
        else:
            # X·ª≠ l√Ω tr√™n CPU
            # Chuy·ªÉn sang ·∫£nh x√°m
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy()
                
            # √Åp d·ª•ng ng∆∞·ª°ng th√≠ch ·ª©ng
            binary = cv2.adaptiveThreshold(
                gray,
                255,
                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                cv2.THRESH_BINARY,
                11,
                2
            )
            
            return binary
            
    def rotate_image(self, image: np.ndarray, angle: float) -> np.ndarray:
        """Xoay ·∫£nh
        
        Args:
            image: ·∫¢nh c·∫ßn xoay
            angle: G√≥c xoay (ƒë·ªô)
            
        Returns:
            np.ndarray: ·∫¢nh ƒë√£ xoay
        """
        if self.has_gpu and self.config["optimize_for_gpu"]:
            with gpu_memory_manager():
                # Chuy·ªÉn ·∫£nh l√™n GPU
                img_tensor = self._to_gpu(image)
                
                # T√≠nh ma tr·∫≠n xoay
                height, width = image.shape[:2]
                center = torch.tensor([width/2, height/2], device=img_tensor.device)
                scale = 1.0
                
                angle_rad = torch.tensor(angle * np.pi / 180)
                alpha = torch.cos(angle_rad) * scale
                beta = torch.sin(angle_rad) * scale
                
                # Ma tr·∫≠n affine
                affine_matrix = torch.tensor([
                    [alpha, beta, (1-alpha)*center[0] - beta*center[1]],
                    [-beta, alpha, beta*center[0] + (1-alpha)*center[1]]
                ], device=img_tensor.device)
                
                # √Åp d·ª•ng bi·∫øn ƒë·ªïi affine
                grid = torch.nn.functional.affine_grid(
                    affine_matrix.unsqueeze(0),
                    img_tensor.unsqueeze(0).size(),
                    align_corners=True
                )
                
                rotated = torch.nn.functional.grid_sample(
                    img_tensor.unsqueeze(0),
                    grid,
                    align_corners=True
                ).squeeze(0)
                
                # Chuy·ªÉn v·ªÅ CPU
                return self._to_cpu(rotated).astype(np.uint8)
        else:
            # X·ª≠ l√Ω tr√™n CPU
            height, width = image.shape[:2]
            center = (width/2, height/2)
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            rotated = cv2.warpAffine(image, rotation_matrix, (width, height))
            return rotated
            
    def process_image(self, input_path: Union[str, np.ndarray], output_path: str = None, auto_rotate: bool = True, 
                     preserve_color: bool = True) -> ProcessingResult:
        """X·ª≠ l√Ω ·∫£nh ho√†n ch·ªânh
        
        Args:
            input_path: ƒê∆∞·ªùng d·∫´n ·∫£nh ƒë·∫ßu v√†o (str) ho·∫∑c numpy array
            output_path: ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh k·∫øt qu·∫£
            auto_rotate: C√≥ t·ª± ƒë·ªông xoay ·∫£nh kh√¥ng
            preserve_color: C√≥ gi·ªØ nguy√™n m√†u s·∫Øc kh√¥ng (kh√¥ng chuy·ªÉn binary)
            
        Returns:
            ProcessingResult: K·∫øt qu·∫£ x·ª≠ l√Ω
        """
        try:
            # ƒê·ªçc ·∫£nh
            if isinstance(input_path, str):
                logger.info(f"üîÑ ƒêang x·ª≠ l√Ω: {input_path}")
                image = cv2.imread(input_path)
                if image is None:
                    raise ValueError(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {input_path}")
            else:
                logger.info("üîÑ ƒêang x·ª≠ l√Ω ·∫£nh t·ª´ numpy array")
                image = input_path
                
            # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng
            quality_info = self._check_image_quality(image)
            
            # X·ª≠ l√Ω ·∫£nh
            with gpu_memory_manager():
                # TƒÉng c∆∞·ªùng ch·∫•t l∆∞·ª£ng
                enhanced = self.enhance_image(image, quality_info)
                
                # Ph√°t hi·ªán v√† s·ª≠a g√≥c nghi√™ng
                angle = 0
                if auto_rotate and not self.config["skip_rotation"]:
                    angle = self.detect_skew(enhanced)
                    if abs(angle) > self.config["min_skew_angle"]:
                        enhanced = self.rotate_image(enhanced, angle)
                
                # Quy·∫øt ƒë·ªãnh c√≥ chuy·ªÉn binary kh√¥ng
                if preserve_color:
                    # Gi·ªØ nguy√™n ·∫£nh m√†u ƒë√£ tƒÉng c∆∞·ªùng
                    result = enhanced
                    processing_steps = ["check_quality", "enhance_image"]
                    if angle != 0:
                        processing_steps.append("rotate_image")
                else:
                    # T·ªëi ∆∞u cho OCR (chuy·ªÉn binary)
                    result = self.enhance_image_for_ocr(enhanced)
                    processing_steps = ["check_quality", "enhance_image"]
                    if angle != 0:
                        processing_steps.append("rotate_image")
                    processing_steps.append("enhance_for_ocr")
                
            # L∆∞u k·∫øt qu·∫£
            if output_path:
                cv2.imwrite(output_path, result)
                logger.info(f"‚úÖ Ho√†n th√†nh: {output_path}")
            else:
                logger.info("‚úÖ Ho√†n th√†nh x·ª≠ l√Ω ·∫£nh")
                
            # T·∫°o k·∫øt qu·∫£
            processing_result = ProcessingResult(
                success=True,
                image=result,
                quality_metrics=quality_info,
                processing_steps=processing_steps,
                output_path=output_path,
                rotation_angle=angle
            )
            
            return processing_result
            
        except Exception as e:
            logger.error(f"L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}")
            return ProcessingResult(
                success=False,
                image=None,
                quality_metrics=None,
                processing_steps=[],
                error_message=str(e)
            )
            
    def detect_skew(self, image, prefix=None):
        """Ph√°t hi·ªán g√≥c nghi√™ng c·ªßa ·∫£nh s·ª≠ d·ª•ng minAreaRect
        Args:
            image: ·∫¢nh ƒë·∫ßu v√†o (numpy array)
            prefix: Ti·ªÅn t·ªë cho t√™n file debug
        Returns:
            float: G√≥c nghi√™ng (ƒë·ªô)
        """
        # Chuy·ªÉn sang grayscale v√† nh·ªã ph√¢n h√≥a
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
            
        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
        
        # T√¨m c√°c contour
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # L·ªçc c√°c contour qu√° nh·ªè v√† t√¨m contour l·ªõn nh·∫•t (c√≥ th·ªÉ l√† b·∫£ng)
        min_area = 1000
        max_area = 0
        largest_contour = None
        
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area > min_area and area > max_area:
                max_area = area
                largest_contour = cnt
                
        if largest_contour is None:
            return 0.0
            
        # S·ª≠ d·ª•ng minAreaRect ƒë·ªÉ t√¨m g√≥c nghi√™ng
        rect = cv2.minAreaRect(largest_contour)
        angle = rect[-1]
        
        # Chu·∫©n h√≥a g√≥c v·ªÅ kho·∫£ng [-45, 45]
        if angle < -45:
            angle = 90 + angle
        elif angle > 45:
            angle = angle - 90
            
        # Debug: v·∫Ω h√¨nh ch·ªØ nh·∫≠t bao quanh contour l·ªõn nh·∫•t
        if self.debug_dir:
            debug_img = image.copy()
            box = cv2.boxPoints(rect)
            box = np.int32(box)
            cv2.drawContours(debug_img, [box], 0, (0, 0, 255), 2)
            
            # L∆∞u ·∫£nh debug
            debug_path = os.path.join(self.debug_dir, "skew_detection.jpg")
            if prefix:
                debug_path = os.path.join(self.debug_dir, f"{prefix}_skew_detection.jpg")
            cv2.imwrite(debug_path, debug_img)
            logger.info(f"ƒê√£ l∆∞u ·∫£nh debug ph√°t hi·ªán g√≥c nghi√™ng: {debug_path}")
        
        # L√†m tr√≤n g√≥c v·ªÅ b∆∞·ªõc 0.2 ƒë·ªô
        angle = round(angle / 0.2) * 0.2
        
        logger.debug(f"Ph√°t hi·ªán g√≥c nghi√™ng: {angle:.2f} ƒë·ªô")
        return angle
        
    def correct_skew(self, image, angle):
        """Ch·ªânh s·ª≠a g√≥c nghi√™ng c·ªßa ·∫£nh
        Args:
            image: ·∫¢nh ƒë·∫ßu v√†o (numpy array)
            angle: G√≥c nghi√™ng c·∫ßn ch·ªânh s·ª≠a (ƒë·ªô)
        Returns:
            numpy array: ·∫¢nh ƒë√£ ƒë∆∞·ª£c ch·ªânh s·ª≠a
        """
        if abs(angle) < self.config["min_skew_angle"]:  # B·ªè qua c√°c g√≥c nghi√™ng nh·ªè
            return image
            
        # L·∫•y k√≠ch th∆∞·ªõc ·∫£nh
        height, width = image.shape[:2]
        
        # T√≠nh t√¢m xoay
        center = (width // 2, height // 2)
        
        # T·∫°o ma tr·∫≠n xoay
        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
        
        # Xoay ·∫£nh
        rotated = cv2.warpAffine(image, rotation_matrix, (width, height), 
                                flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
        
        return rotated

    def detect_horizontal_lines(self, image: np.ndarray, min_line_length_ratio: Optional[float] = None) -> List[int]:
        """Ph√°t hi·ªán c√°c ƒë∆∞·ªùng k·∫ª ngang trong ·∫£nh
        
        Args:
            image: ·∫¢nh ƒë√£ ti·ªÅn x·ª≠ l√Ω
            min_line_length_ratio: T·ª∑ l·ªá t·ªëi thi·ªÉu c·ªßa chi·ªÅu d√†i ƒë∆∞·ªùng k·∫ª so v·ªõi chi·ªÅu r·ªông ·∫£nh
            
        Returns:
            List[int]: Danh s√°ch c√°c t·ªça ƒë·ªô y c·ªßa ƒë∆∞·ªùng k·∫ª ngang
        """
        # L·∫•y c√°c tham s·ªë t·ª´ config
        line_config = self.config["line_detection"]
        min_line_length_ratio = min_line_length_ratio or line_config["min_line_length_ratio"]
        min_length_threshold_ratio = line_config["min_length_threshold_ratio"]
        max_line_height = line_config["max_line_height"]
        morph_iterations = line_config["morph_iterations"]
        histogram_threshold_ratio = line_config["histogram_threshold_ratio"]
        min_line_distance = line_config["min_line_distance"]
        dilate_kernel_div = line_config["dilate_kernel_div"]
        horizontal_kernel_div = line_config["horizontal_kernel_div"]
        projection_threshold_div = line_config["projection_threshold_div"]

        # K√≠ch th∆∞·ªõc ·∫£nh
        height, width = image.shape[:2]
        
        # T√≠nh chi·ªÅu d√†i t·ªëi thi·ªÉu c·ªßa ƒë∆∞·ªùng k·∫ª
        min_line_length = int(width * min_line_length_ratio)
        logger.debug(f"Chi·ªÅu d√†i t·ªëi thi·ªÉu c·ªßa ƒë∆∞·ªùng k·∫ª ngang: {min_line_length}px (={min_line_length_ratio:.2f} √ó {width}px)")
        
        # T·∫°o kernel ngang ƒë·ªÉ ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª ngang
        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (min_line_length // horizontal_kernel_div, 1))
        
        # √Åp d·ª•ng ph√©p to√°n m·ªü ƒë·ªÉ ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª ngang
        horizontal_lines = cv2.morphologyEx(image, cv2.MORPH_OPEN, horizontal_kernel, iterations=morph_iterations)
        
        # L·ªçc c√°c ƒë∆∞·ªùng ngang c√≥ chi·ªÅu d√†i nh·ªè
        filtered_horizontal_lines = np.zeros_like(horizontal_lines)
        contours, _ = cv2.findContours(horizontal_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # T√¨m chi·ªÅu d√†i l·ªõn nh·∫•t c·ªßa ƒë∆∞·ªùng k·∫ª
        max_length = 0
        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)
            max_length = max(max_length, w)
        
        # T√≠nh ng∆∞·ª°ng chi·ªÅu d√†i t·ªëi thi·ªÉu
        min_length_threshold = int(max_length * min_length_threshold_ratio)
        logger.debug(f"Chi·ªÅu d√†i t·ªëi thi·ªÉu c·ªßa ƒë∆∞·ªùng k·∫ª ({min_length_threshold_ratio*100}% ƒë∆∞·ªùng d√†i nh·∫•t): {min_length_threshold}px")
        
        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)
            if w >= min_length_threshold:
                # Ki·ªÉm tra th√™m chi·ªÅu cao ƒë·ªÉ lo·∫°i b·ªè g·∫°ch ngang c·ªßa ch·ªØ
                if h <= max_line_height:
                    cv2.drawContours(filtered_horizontal_lines, [cnt], -1, 255, -1)
        
        # L√†m d√†y ƒë∆∞·ªùng k·∫ª
        dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (min_line_length // dilate_kernel_div, 2))
        filtered_horizontal_lines = cv2.dilate(filtered_horizontal_lines, dilate_kernel, iterations=1)
        
        # L∆∞u ·∫£nh ƒë∆∞·ªùng k·∫ª ngang ƒë·ªÉ debug
        if self.debug:
            debug_path_original = os.path.join(self.debug_dir, "horizontal_lines_original.jpg")
            cv2.imwrite(debug_path_original, horizontal_lines)
            
            debug_path_filtered = os.path.join(self.debug_dir, "horizontal_lines_filtered.jpg")
            cv2.imwrite(debug_path_filtered, filtered_horizontal_lines)
        
        # Ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª ngang b·∫±ng histogram
        h_projection = cv2.reduce(filtered_horizontal_lines, 1, cv2.REDUCE_SUM, dtype=cv2.CV_32S)
        h_projection = h_projection / 255
        
        # √Åp d·ª•ng ng∆∞·ª°ng histogram
        max_projection_value = np.max(h_projection)
        threshold_value = max_projection_value * histogram_threshold_ratio
        logger.debug(f"Gi√° tr·ªã ng∆∞·ª°ng l·ªçc histogram ({histogram_threshold_ratio*100}% gi√° tr·ªã l·ªõn nh·∫•t): {threshold_value:.2f}")
        
        filtered_projection = np.copy(h_projection)
        filtered_projection[filtered_projection < threshold_value] = 0
        
        # T√¨m v·ªã tr√≠ c√°c ƒë·ªânh trong histogram
        line_positions = []
        threshold = width / projection_threshold_div
        
        for y in range(1, height - 1):
            if filtered_projection[y] > threshold:
                # Ki·ªÉm tra xem c√≥ ph·∫£i ƒë·ªânh c·ª•c b·ªô kh√¥ng
                if filtered_projection[y] >= filtered_projection[y-1] and filtered_projection[y] >= filtered_projection[y+1]:
                    line_positions.append(y)
                    continue
                    
                # Ho·∫∑c v·∫´n l√† ph·∫ßn c·ªßa ƒë∆∞·ªùng k·∫ª
                is_peak = True
                for i in range(1, 3):  # Ki·ªÉm tra 3 pixel l√¢n c·∫≠n
                    if y + i < height and filtered_projection[y] < filtered_projection[y+i]:
                        is_peak = False
                        break
                    if y - i >= 0 and filtered_projection[y] < filtered_projection[y-i]:
                        is_peak = False
                        break
                
                if is_peak:
                    line_positions.append(y)
        
        # L·ªçc c√°c ƒë∆∞·ªùng k·∫ª qu√° g·∫ßn nhau
        filtered_positions = self._filter_close_lines(line_positions, min_distance=min_line_distance)
        
        # Th√™m v·ªã tr√≠ ƒë·∫ßu v√† cu·ªëi ·∫£nh n·∫øu c·∫ßn
        if len(filtered_positions) > 0 and filtered_positions[0] > 20:
            filtered_positions.insert(0, 0)
        if len(filtered_positions) > 0 and filtered_positions[-1] < height - 20:
            filtered_positions.append(height)
        
        # S·∫Øp x·∫øp l·∫°i c√°c v·ªã tr√≠
        filtered_positions.sort()
        
        # V·∫Ω c√°c ƒë∆∞·ªùng k·∫ª ngang l√™n ·∫£nh ƒë·ªÉ debug
        if self.debug:
            debug_image = cv2.cvtColor(image.copy(), cv2.COLOR_GRAY2BGR)
            for y in filtered_positions:
                cv2.line(debug_image, (0, y), (width, y), (0, 0, 255), 2)
            
            debug_lines_path = os.path.join(self.debug_dir, "detected_lines.jpg")
            cv2.imwrite(debug_lines_path, debug_image)
        
        logger.debug(f"ƒê√£ ph√°t hi·ªán {len(filtered_positions)} ƒë∆∞·ªùng k·∫ª ngang sau khi l·ªçc")
        return filtered_positions

    @staticmethod
    def preprocess_image_for_api(
        image_path: str, 
        provider: str = "google", 
        output_dir: Optional[str] = None,
        debug_dir: Optional[str] = None,
        min_quality_score: float = 0.4,
        auto_rotate: bool = False
    ) -> Tuple[bool, Dict[str, Any], Optional[str]]:
        """
        Ti·ªÅn x·ª≠ l√Ω ·∫£nh tr∆∞·ªõc khi g·ª≠i ƒë·∫øn API c·ªßa c√°c nh√† cung c·∫•p kh√°c nhau
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ·∫£nh c·∫ßn x·ª≠ l√Ω
            provider: Nh√† cung c·∫•p API ("google", "anthropic", "local")
            output_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u ·∫£nh ƒë√£ x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh l√† th∆∞ m·ª•c c·ªßa ·∫£nh g·ªëc)
            debug_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u c√°c ·∫£nh debug (n·∫øu None s·∫Ω kh√¥ng l∆∞u)
            min_quality_score: ƒêi·ªÉm ch·∫•t l∆∞·ª£ng t·ªëi thi·ªÉu ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω (0-1)
            auto_rotate: C√≥ t·ª± ƒë·ªông ph√°t hi·ªán v√† xoay ·∫£nh hay kh√¥ng
            
        Returns:
            Tuple[bool, Dict[str, Any], Optional[str]]:
                - bool: True n·∫øu ti·ªÅn X·ª≠ l√Ω xong, False n·∫øu th·∫•t b·∫°i
                - Dict[str, Any]: Th√¥ng tin v·ªÅ qu√° tr√¨nh x·ª≠ l√Ω
                - Optional[str]: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh ƒë√£ x·ª≠ l√Ω (None n·∫øu th·∫•t b·∫°i nghi√™m tr·ªçng)
        """
        start_time = time.time()
        logger.info(f"üîÑ ƒêang ti·ªÅn x·ª≠ l√Ω cho {provider.upper()} API: {image_path}")
        
        # X√°c th·ª±c ƒë∆∞·ªùng d·∫´n ·∫£nh
        if not os.path.exists(image_path):
            return False, {"message": f"File kh√¥ng t·ªìn t·∫°i: {image_path}"}, None
        
        # Chu·∫©n b·ªã th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if output_dir is None:
            output_dir = os.path.dirname(image_path)
        
        # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra v√† debug n·∫øu c·∫ßn
        os.makedirs(output_dir, exist_ok=True)
        if debug_dir:
            os.makedirs(debug_dir, exist_ok=True)
        
        # Th√¥ng tin x·ª≠ l√Ω
        processing_info = {
            "original_path": image_path,
            "processing_steps": [],
            "warnings": [],
            "metrics": {
                "start_time": start_time,
                "original_size": os.path.getsize(image_path) / (1024 * 1024)  # MB
            },
            "provider": provider,
            "auto_rotate": auto_rotate
        }
        
        try:
            # ƒê·ªçc ·∫£nh
            img = cv2.imread(image_path)
            if img is None:
                # Th·ª≠ ƒë·ªçc v·ªõi PIL n·∫øu OpenCV kh√¥ng ƒë·ªçc ƒë∆∞·ª£c
                try:
                    from PIL import Image
                    pil_img = Image.open(image_path)
                    img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
                    processing_info["warnings"].append("OpenCV kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh, s·ª≠ d·ª•ng PIL")
                    processing_info["processing_steps"].append("fallback_to_pil")
                except Exception as e:
                    return False, {"message": f"Kh√¥ng th·ªÉ ƒë·ªçc file ·∫£nh: {str(e)}"}, None
            
            # L∆∞u k√≠ch th∆∞·ªõc g·ªëc
            original_height, original_width = img.shape[:2]
            processing_info["metrics"]["original_dimensions"] = (original_width, original_height)
            
            # L∆∞u ·∫£nh g·ªëc ƒë·ªÉ debug
            if debug_dir:
                original_debug_path = os.path.join(debug_dir, f"original_{os.path.basename(image_path)}")
                cv2.imwrite(original_debug_path, img)
            
            # Kh·ªüi t·∫°o b·ªô ki·ªÉm tra ch·∫•t l∆∞·ª£ng
            processor = ImageProcessor(debug_dir)
            filename, ext = os.path.splitext(os.path.basename(image_path))
            
            # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ·∫£nh
            is_good_quality, quality_info, enhanced_img = processor.check_quality(img, prefix=filename)
            
            # T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng t·ªïng h·ª£p (n·∫øu ch∆∞a ƒë∆∞·ª£c t√≠nh trong check_quality)
            if hasattr(quality_info, 'quality_score') and quality_info.quality_score > 0:
                quality_score = quality_info.quality_score
            else:
                # T√≠nh to√°n quality_score t·ª´ c√°c metrics
                quality_score = (
                    (min(quality_info.blur_index / processor.config["min_blur_index"], 2.0) * 0.3) +
                    ((1 - quality_info.dark_ratio / processor.config["max_dark_ratio"]) * 0.2) +
                    (min(quality_info.brightness / processor.config["min_brightness"], 1.5) * 0.25) +
                    (min(quality_info.contrast / processor.config["min_contrast"], 1.5) * 0.25)
                )
                quality_score = min(1.0, max(0.0, quality_score))
                quality_info.quality_score = quality_score
            
            # Th√™m th√¥ng tin ch·∫•t l∆∞·ª£ng v√†o processing_info
            processing_info["metrics"]["quality"] = quality_info.to_dict() if hasattr(quality_info, 'to_dict') else quality_info
            processing_info["metrics"]["quality_passed"] = is_good_quality
            
            # N·∫øu ·∫£nh ch·∫•t l∆∞·ª£ng qu√° k√©m, d·ª´ng x·ª≠ l√Ω
            if quality_score < min_quality_score:
                error_msg = f"·∫¢nh c√≥ ch·∫•t l∆∞·ª£ng qu√° k√©m (ƒëi·ªÉm: {quality_score:.2f}), kh√¥ng th·ªÉ x·ª≠ l√Ω"
                processing_info["warnings"].append(error_msg)
                processing_info["message"] = error_msg
                logger.error(error_msg)
                return False, processing_info, None
            
            # S·ª≠ d·ª•ng ·∫£nh ƒë√£ n√¢ng cao ch·∫•t l∆∞·ª£ng n·∫øu c√≥
            if enhanced_img is not None:
                img = enhanced_img
                processing_info["processing_steps"].append("quality_enhancement")
                
                # L∆∞u ·∫£nh ƒë√£ n√¢ng cao ch·∫•t l∆∞·ª£ng ƒë·ªÉ debug
                if debug_dir:
                    enhanced_debug_path = os.path.join(debug_dir, f"enhanced_{os.path.basename(image_path)}")
                    cv2.imwrite(enhanced_debug_path, img)
            
            # X·ª≠ l√Ω nghi√™ng/xoay ·∫£nh
            result = processor.process_image(img, output_path=None, auto_rotate=auto_rotate)
            if result.success and result.image is not None:
                img = result.image
                processing_info["processing_steps"].append("skew_correction")
                if auto_rotate:
                    processing_info["processing_steps"].append("auto_rotation")
            
            # ƒêi·ªÅu ch·ªânh h√¨nh ·∫£nh theo nh√† cung c·∫•p API
            if provider.lower() == "google":
                # Google y√™u c·∫ßu h√¨nh ·∫£nh r√µ r√†ng, ƒë·ªô ph√¢n gi·∫£i cao
                # Kh√¥ng c·∫ßn thay ƒë·ªïi nhi·ªÅu
                pass
            elif provider.lower() == "anthropic":
                # Anthropic c√≥ th·ªÉ x·ª≠ l√Ω h√¨nh ·∫£nh h∆°i m·ªù nh∆∞ng c·∫ßn t∆∞∆°ng ph·∫£n t·ªët
                # TƒÉng c∆∞·ªùng t∆∞∆°ng ph·∫£n
                brightness = 1.0
                contrast = 1.3  # TƒÉng c∆∞·ªùng t∆∞∆°ng ph·∫£n cho Claude
                alpha = contrast
                beta = 10
                img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)
                processing_info["processing_steps"].append("contrast_enhancement_for_anthropic")
            elif provider.lower() == "local":
                # X·ª≠ l√Ω local th∆∞·ªùng c·∫ßn ·∫£nh r√µ r√†ng h∆°n
                # TƒÉng c∆∞·ªùng ƒë·ªô s·∫Øc n√©t
                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
                img = cv2.filter2D(img, -1, kernel)
                processing_info["processing_steps"].append("sharpening_for_local")
            
            # Chu·∫©n b·ªã ƒë∆∞·ªùng d·∫´n ƒë·∫ßu ra
            filename, ext = os.path.splitext(os.path.basename(image_path))
            output_path = os.path.join(output_dir, f"{filename}_processed_{provider.lower()}{ext}")
            
            # L∆∞u ·∫£nh ƒë√£ x·ª≠ l√Ω
            cv2.imwrite(output_path, img)
            processing_info["output_path"] = output_path
            
            # C·∫≠p nh·∫≠t metrics
            processing_info["metrics"]["processed_size"] = os.path.getsize(output_path) / (1024 * 1024)  # MB
            processing_info["metrics"]["processed_dimensions"] = img.shape[:2][::-1]  # width, height
            processing_info["metrics"]["processing_time"] = time.time() - start_time
            
            # L∆∞u ·∫£nh ƒë√£ x·ª≠ l√Ω ƒë·ªÉ debug
            if debug_dir:
                debug_path = os.path.join(debug_dir, f"processed_{provider.lower()}_{os.path.basename(image_path)}")
                cv2.imwrite(debug_path, img)
                
                # T·∫°o ·∫£nh so s√°nh tr∆∞·ªõc-sau
                h_orig, w_orig = original_height, original_width
                h_proc, w_proc = img.shape[:2]
                
                # ƒê·∫£m b·∫£o c·∫£ hai ·∫£nh c√≥ c√πng k√≠ch th∆∞·ªõc ƒë·ªÉ gh√©p
                if h_orig != h_proc or w_orig != w_proc:
                    orig_img = cv2.imread(image_path)
                    orig_img = cv2.resize(orig_img, (w_proc, h_proc))
                else:
                    orig_img = cv2.imread(image_path)
                
                # Gh√©p ·∫£nh g·ªëc v√† ·∫£nh ƒë√£ x·ª≠ l√Ω ƒë·ªÉ so s√°nh
                comparison = np.zeros((h_proc, w_proc*2, 3), dtype=np.uint8)
                comparison[:, :w_proc] = orig_img
                comparison[:, w_proc:] = img
                
                # Th√™m nh√£n
                cv2.putText(comparison, "Original", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.putText(comparison, f"Processed ({provider})", (w_proc+10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
                # L∆∞u ·∫£nh so s√°nh
                comparison_path = os.path.join(debug_dir, f"comparison_{provider.lower()}_{os.path.basename(image_path)}")
                cv2.imwrite(comparison_path, comparison)
                logger.info(f"ƒê√£ l∆∞u ·∫£nh so s√°nh: {comparison_path}")
            
            processing_info["message"] = "X·ª≠ l√Ω xong"
            logger.info(f"‚úÖ Ho√†n th√†nh ti·ªÅn x·ª≠ l√Ω {provider.upper()}: {output_path}")
            return True, processing_info, output_path
            
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            processing_info["error"] = str(e)
            processing_info["traceback"] = traceback.format_exc()
            processing_info["message"] = f"L·ªói: {str(e)}"
            
            return False, processing_info, None

    def extract_rows_from_table(self, table_image: np.ndarray, table_id: int) -> List[np.ndarray]:
        """C·∫Øt c√°c h√†ng t·ª´ ·∫£nh b·∫£ng d·ª±a tr√™n ƒë∆∞·ªùng k·∫ª ngang
        
        Args:
            table_image: ·∫¢nh b·∫£ng
            table_id: ID c·ªßa b·∫£ng
            
        Returns:
            List[np.ndarray]: Danh s√°ch c√°c ·∫£nh h√†ng ƒë√£ c·∫Øt
        """
        # L·∫•y c√°c tham s·ªë t·ª´ config
        row_config = self.config["row_extraction"]
        top_margin = row_config["top_margin"]
        bottom_margin = row_config["bottom_margin"]
        safe_zone = row_config["safe_zone"]
        min_row_height = row_config["min_row_height"]
        check_text = row_config["check_text"]
        text_margin = row_config["text_margin"]
        min_text_area = row_config["min_text_area"]

        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
        processed = self.preprocess_image(table_image)
        
        # Ph√°t hi·ªán c√°c ƒë∆∞·ªùng k·∫ª ngang
        line_positions = self.detect_horizontal_lines(processed)
        
        # N·∫øu kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ƒë∆∞·ªùng k·∫ª n√†o
        if len(line_positions) <= 1:
            logger.warning("Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ƒë·ªß ƒë∆∞·ªùng k·∫ª ngang, d√πng ph∆∞∆°ng ph√°p d·ª± ph√≤ng")
            return self._extract_rows_fallback(table_image, table_id)
        
        # L·∫•y k√≠ch th∆∞·ªõc ·∫£nh
        height, width = table_image.shape[:2]
        
        # T·∫°o ·∫£nh debug
        debug_image = table_image.copy()

        def check_text_in_region(img: np.ndarray, region: Tuple[int, int, int, int]) -> bool:
            """Ki·ªÉm tra c√≥ text trong v√πng ·∫£nh kh√¥ng"""
            x1, y1, x2, y2 = region
            region_img = img[y1:y2, x1:x2]
            if len(region_img.shape) == 3:
                gray = cv2.cvtColor(region_img, cv2.COLOR_BGR2GRAY)
            else:
                gray = region_img
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            text_pixels = np.sum(binary == 255)
            total_pixels = binary.size
            return text_pixels / total_pixels > min_text_area

        def calculate_iou(box1, box2):
            """T√≠nh to√°n IoU gi·ªØa hai box"""
            y1_1, y2_1 = box1
            y1_2, y2_2 = box2
            
            # T√≠nh to√°n v√πng giao
            intersection_y1 = max(y1_1, y1_2)
            intersection_y2 = min(y2_1, y2_2)
            
            if intersection_y2 <= intersection_y1:
                return 0.0
                
            intersection = intersection_y2 - intersection_y1
            
            # T√≠nh to√°n v√πng h·ª£p
            box1_height = y2_1 - y1_1
            box2_height = y2_2 - y1_2
            union = box1_height + box2_height - intersection
            
            return intersection / union if union > 0 else 0.0

        # B∆∞·ªõc 1: T√≠nh to√°n chi·ªÅu cao trung b√¨nh c·ªßa c√°c h√†ng
        row_heights = []
        for i in range(len(line_positions) - 1):
            line_top = line_positions[i]
            line_bottom = line_positions[i + 1]
            safe_top = line_top + safe_zone
            safe_bottom = line_bottom - safe_zone
            y_top = max(0, safe_top - top_margin)
            y_bottom = min(height, safe_bottom + bottom_margin)
            row_height = y_bottom - y_top
            row_heights.append(row_height)
            
        avg_row_height = sum(row_heights) / len(row_heights) if row_heights else min_row_height
        logger.info(f"Chi·ªÅu cao trung b√¨nh c·ªßa h√†ng: {avg_row_height:.2f}px")
        
        # B∆∞·ªõc 2: C·∫Øt v√† x·ª≠ l√Ω c√°c h√†ng
        rows = []
        row_boxes = []  # L∆∞u t·ªça ƒë·ªô c√°c h√†ng ƒë·ªÉ t√≠nh IoU
        empty_rows_count = 0
        skipped_small_rows_count = 0
        
        for i in range(len(line_positions) - 1):
            line_top = line_positions[i]
            line_bottom = line_positions[i + 1]
            
            # Ki·ªÉm tra v√πng an to√†n xung quanh ƒë∆∞·ªùng k·∫ª
            safe_top = line_top + safe_zone
            safe_bottom = line_bottom - safe_zone
            
            # Th√™m l·ªÅ c∆° b·∫£n
            y_top = max(0, safe_top - top_margin)
            y_bottom = min(height, safe_bottom + bottom_margin)
            
            # Ki·ªÉm tra text trong v√πng l·ªÅ ƒë·ªÉ ƒëi·ªÅu ch·ªânh bi√™n
            if check_text:
                top_margin_region = (0, y_top, width, safe_top)
                if check_text_in_region(table_image, top_margin_region):
                    y_top = max(0, y_top - text_margin)
                    logger.info(f"ƒêi·ªÅu ch·ªânh l·ªÅ tr√™n cho h√†ng {i+1} do ph√°t hi·ªán text")
                
                bottom_margin_region = (0, safe_bottom, width, y_bottom)
                if check_text_in_region(table_image, bottom_margin_region):
                    y_bottom = min(height, y_bottom + text_margin)
                    logger.info(f"ƒêi·ªÅu ch·ªânh l·ªÅ d∆∞·ªõi cho h√†ng {i+1} do ph√°t hi·ªán text")
            
            # Ki·ªÉm tra chi·ªÅu cao h√†ng
            row_height = y_bottom - y_top
            current_box = (y_top, y_bottom)
            
            # X·ª≠ l√Ω h√†ng nh·ªè
            if row_height < avg_row_height * 0.1:  # H√†ng nh·ªè h∆°n 10% chi·ªÅu cao trung b√¨nh
                logger.info(f"Ph√°t hi·ªán h√†ng nh·ªè {i+1}: {row_height}px < {avg_row_height * 0.1:.2f}px")
                
                # Ki·ªÉm tra IoU v·ªõi h√†ng tr∆∞·ªõc v√† sau
                prev_box = row_boxes[-1] if row_boxes else None
                next_box = (line_positions[i+1], line_positions[i+2]) if i < len(line_positions) - 2 else None
                
                prev_iou = calculate_iou(current_box, prev_box) if prev_box else 0
                next_iou = calculate_iou(current_box, next_box) if next_box else 0
                
                if (prev_iou > 0.5 or next_iou > 0.5):  # IoU > 50%
                    # T√≠nh kho·∫£ng c√°ch ƒë·∫øn h√†ng tr∆∞·ªõc v√† sau
                    dist_to_prev = abs(y_top - prev_box[1]) if prev_box else float('inf')
                    dist_to_next = abs(y_bottom - next_box[0]) if next_box else float('inf')
                    
                    # M·ªü r·ªông v·ªÅ ph√≠a c√≥ kho·∫£ng c√°ch l·ªõn h∆°n
                    if dist_to_prev > dist_to_next and prev_box:
                        # M·ªü r·ªông l√™n tr√™n
                        y_top = max(0, y_top - (avg_row_height - row_height))
                        logger.info(f"M·ªü r·ªông h√†ng {i+1} l√™n tr√™n")
                    elif next_box:
                        # M·ªü r·ªông xu·ªëng d∆∞·ªõi
                        y_bottom = min(height, y_bottom + (avg_row_height - row_height))
                        logger.info(f"M·ªü r·ªông h√†ng {i+1} xu·ªëng d∆∞·ªõi")
                    
                    row_height = y_bottom - y_top
                    current_box = (y_top, y_bottom)
            
            # Ki·ªÉm tra l·∫°i chi·ªÅu cao sau khi x·ª≠ l√Ω
            if row_height < min_row_height:
                logger.warning(f"B·ªè qua h√†ng {i+1} do chi·ªÅu cao qu√° nh·ªè: {row_height}px < {min_row_height}px")
                skipped_small_rows_count += 1
                cv2.rectangle(debug_image, (0, y_top), (width, y_bottom), (255, 0, 255), 2)
                cv2.putText(debug_image, f"Small Row {i+1} ({row_height}px)", (10, y_top+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
                continue
            
            # C·∫Øt h√†ng t·ª´ ·∫£nh g·ªëc
            row = table_image[y_top:y_bottom, 0:width]
            
            # Ki·ªÉm tra text trong h√†ng
            has_text = True
            if check_text:
                has_text = check_text_in_region(row, (0, 0, width, row_height))
            
            if not has_text:
                empty_rows_count += 1
                logger.warning(f"C·∫£nh b√°o: H√†ng {i+1} kh√¥ng c√≥ text")
                cv2.rectangle(debug_image, (0, y_top), (width, y_bottom), (0, 0, 255), 2)
                cv2.putText(debug_image, f"Empty Row {i+1}", (10, y_top+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            else:
                cv2.rectangle(debug_image, (0, y_top), (width, y_bottom), (0, 255, 0), 2)
                cv2.putText(debug_image, f"Row {i+1} ({row_height}px)", (10, y_top+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.line(debug_image, (0, safe_top), (width, safe_top), (255, 255, 0), 1)
                cv2.line(debug_image, (0, safe_bottom), (width, safe_bottom), (255, 255, 0), 1)
            
            rows.append(row)
            row_boxes.append(current_box)
        
        # L∆∞u ·∫£nh debug
        if self.debug:
            debug_path = os.path.join(self.debug_dir, f"table_{table_id}_rows.jpg")
            cv2.imwrite(debug_path, debug_image)
            
            # L∆∞u t·ª´ng h√†ng ri√™ng bi·ªát
            rows_dir = os.path.join(self.debug_dir, f"table_{table_id}_rows")
            os.makedirs(rows_dir, exist_ok=True)
            for i, row in enumerate(rows):
                row_path = os.path.join(rows_dir, f"row_{i+1}.jpg")
                cv2.imwrite(row_path, row)
        
        if empty_rows_count > 0:
            logger.warning(f"C·∫£nh b√°o: C√≥ {empty_rows_count}/{len(rows)} h√†ng kh√¥ng c√≥ text")
        
        if skipped_small_rows_count > 0:
            logger.warning(f"C·∫£nh b√°o: ƒê√£ b·ªè qua {skipped_small_rows_count} h√†ng c√≥ chi·ªÅu cao < {min_row_height}px")
        
        logger.info(f"ƒê√£ c·∫Øt {len(rows)} h√†ng t·ª´ b·∫£ng {table_id}")
        return rows

    def test_extract_rows_comparison(self, image_path: str) -> None:
        """So s√°nh k·∫øt qu·∫£ extract rows gi·ªØa ph∆∞∆°ng ph√°p hi·ªán t·∫°i v√† ph∆∞∆°ng ph√°p OCR
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh c·∫ßn test
        """
        try:
            import pytesseract
            from PIL import Image
            OCR_AVAILABLE = True
        except ImportError:
            logger.error("Kh√¥ng t√¨m th·∫•y pytesseract. Vui l√≤ng c√†i ƒë·∫∑t: pip install pytesseract")
            return

        # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh
        image = cv2.imread(image_path)
        if image is None:
            logger.error(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·ª´ {image_path}")
            return

        # T·∫°o th∆∞ m·ª•c debug cho test n√†y
        test_debug_dir = os.path.join(self.debug_dir, "extract_rows_test")
        os.makedirs(test_debug_dir, exist_ok=True)

        # 1. Ph∆∞∆°ng ph√°p hi·ªán t·∫°i
        logger.info("1. ƒêang th·ª≠ nghi·ªám ph∆∞∆°ng ph√°p hi·ªán t·∫°i...")
        current_rows = self.extract_rows_from_table(image, table_id=1)
        
        # L∆∞u c√°c h√†ng ƒë∆∞·ª£c ph√°t hi·ªán b·ªüi ph∆∞∆°ng ph√°p hi·ªán t·∫°i
        current_debug_dir = os.path.join(test_debug_dir, "current_method")
        os.makedirs(current_debug_dir, exist_ok=True)
        for i, row in enumerate(current_rows):
            row_path = os.path.join(current_debug_dir, f"row_{i+1}.jpg")
            cv2.imwrite(row_path, row)
        logger.info(f"Ph∆∞∆°ng ph√°p hi·ªán t·∫°i ph√°t hi·ªán ƒë∆∞·ª£c {len(current_rows)} h√†ng")

        # 2. Ph∆∞∆°ng ph√°p OCR
        logger.info("2. ƒêang th·ª≠ nghi·ªám ph∆∞∆°ng ph√°p OCR...")
        
        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh cho OCR
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # L∆∞u ·∫£nh ƒë√£ ti·ªÅn x·ª≠ l√Ω
        preprocessed_path = os.path.join(test_debug_dir, "preprocessed.jpg")
        cv2.imwrite(preprocessed_path, binary)

        # Th·ª±c hi·ªán OCR v·ªõi bounding boxes
        ocr_debug_image = image.copy()
        height, width = image.shape[:2]

        # L·∫•y th√¥ng tin v·ªÅ boxes v√† text
        ocr_data = pytesseract.image_to_data(binary, output_type=pytesseract.Output.DICT)
        
        # L·ªçc v√† gom nh√≥m c√°c boxes theo h√†ng
        boxes = []
        for i in range(len(ocr_data['text'])):
            # Ch·ªâ x·ª≠ l√Ω c√°c box c√≥ text v√† ƒë·ªô tin c·∫≠y cao
            if not ocr_data['text'][i].strip() or int(ocr_data['conf'][i]) < 50:  # TƒÉng ng∆∞·ª°ng tin c·∫≠y
                continue
                
            x = ocr_data['left'][i]
            y = ocr_data['top'][i]
            w = ocr_data['width'][i]
            h = ocr_data['height'][i]
            text = ocr_data['text'][i].strip()
            boxes.append((x, y, w, h, text))
        
        # S·∫Øp x·∫øp boxes theo y
        boxes.sort(key=lambda box: box[1])
        
        # Gom nh√≥m boxes th√†nh c√°c h√†ng
        row_boxes = []
        current_row = []
        min_row_gap = 40  # TƒÉng kho·∫£ng c√°ch t·ªëi thi·ªÉu gi·ªØa c√°c h√†ng
        min_row_height = 25  # TƒÉng chi·ªÅu cao t·ªëi thi·ªÉu c·ªßa h√†ng
        min_text_ratio = 0.1  # T·ª∑ l·ªá di·ªán t√≠ch text t·ªëi thi·ªÉu
        
        def get_row_bounds(row_boxes):
            """T√≠nh to√°n bi√™n c·ªßa m·ªôt h√†ng"""
            min_x = min(box[0] for box in row_boxes)
            min_y = min(box[1] for box in row_boxes)
            max_x = max(box[0] + box[2] for box in row_boxes)
            max_y = max(box[1] + box[3] for box in row_boxes)
            return min_x, min_y, max_x, max_y
        
        def check_horizontal_alignment(box1, box2, tolerance=0.5):
            """Ki·ªÉm tra cƒÉn ch·ªânh ngang gi·ªØa hai box"""
            _, y1, _, y1_max = get_row_bounds([box1])
            _, y2, _, y2_max = get_row_bounds([box2])
            h1 = y1_max - y1
            h2 = y2_max - y2
            overlap = min(y1_max, y2_max) - max(y1, y2)
            return overlap >= min(h1, h2) * tolerance
        
        for box in boxes:
            x, y, w, h, text = box
            
            # N·∫øu l√† box ƒë·∫ßu ti√™n ho·∫∑c g·∫ßn v·ªõi h√†ng hi·ªán t·∫°i v√† cƒÉn ch·ªânh ngang
            if not current_row or (
                abs(y - current_row[-1][1]) < min_row_gap and 
                check_horizontal_alignment(box, current_row[-1])
            ):
                current_row.append(box)
            else:
                # Ki·ªÉm tra chi·ªÅu cao v√† t·ª∑ l·ªá text c·ªßa h√†ng hi·ªán t·∫°i
                if current_row:
                    min_x, min_y, max_x, max_y = get_row_bounds(current_row)
                    row_height = max_y - min_y
                    row_width = max_x - min_x
                    text_area = sum(b[2] * b[3] for b in current_row)
                    row_area = row_height * row_width
                    
                    if (row_height >= min_row_height and 
                        text_area / row_area >= min_text_ratio):
                        row_boxes.append(current_row)
                
                current_row = [box]
        
        # Th√™m h√†ng cu·ªëi c√πng n·∫øu th·ªèa m√£n ƒëi·ªÅu ki·ªán
        if current_row:
            min_x, min_y, max_x, max_y = get_row_bounds(current_row)
            row_height = max_y - min_y
            row_width = max_x - min_x
            text_area = sum(b[2] * b[3] for b in current_row)
            row_area = row_height * row_width
            
            if (row_height >= min_row_height and 
                text_area / row_area >= min_text_ratio):
                row_boxes.append(current_row)

        # C·∫Øt v√† l∆∞u c√°c h√†ng t·ª´ ph∆∞∆°ng ph√°p OCR
        ocr_rows = []
        ocr_debug_dir = os.path.join(test_debug_dir, "ocr_method")
        os.makedirs(ocr_debug_dir, exist_ok=True)

        for i, row_box_list in enumerate(row_boxes):
            if not row_box_list:
                continue

            # T√≠nh to√°n bounding box cho c·∫£ h√†ng
            min_x, min_y, max_x, max_y = get_row_bounds(row_box_list)

            # Th√™m padding
            padding = 10
            min_x = max(0, min_x - padding)
            min_y = max(0, min_y - padding)
            max_x = min(width, max_x + padding)
            max_y = min(height, max_y + padding)

            # C·∫Øt h√†ng
            row_img = image[min_y:max_y, min_x:max_x]
            
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu v√† t·ª∑ l·ªá chi·ªÅu r·ªông
            if (row_img.shape[0] >= min_row_height and 
                row_img.shape[1] >= width * 0.3):
                ocr_rows.append(row_img)

                # L∆∞u ·∫£nh h√†ng
                row_path = os.path.join(ocr_debug_dir, f"row_{len(ocr_rows)}.jpg")
                cv2.imwrite(row_path, row_img)

                # V·∫Ω bounding box v√† text tr√™n ·∫£nh debug
                cv2.rectangle(ocr_debug_image, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)
                cv2.putText(ocr_debug_image, f"Row {len(ocr_rows)}", (min_x, min_y-5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # V·∫Ω text ƒë∆∞·ª£c ph√°t hi·ªán
                text_list = [box[4] for box in row_box_list]
                text = " | ".join(text_list)
                cv2.putText(ocr_debug_image, text[:50], (min_x, max_y+15),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)

        # L∆∞u ·∫£nh debug v·ªõi c√°c bounding box
        cv2.imwrite(os.path.join(test_debug_dir, "ocr_detection.jpg"), ocr_debug_image)

        # So s√°nh k·∫øt qu·∫£
        logger.info("\nK·∫øt qu·∫£ so s√°nh:")
        logger.info(f"1. Ph∆∞∆°ng ph√°p hi·ªán t·∫°i: {len(current_rows)} h√†ng")
        logger.info(f"2. Ph∆∞∆°ng ph√°p OCR: {len(ocr_rows)} h√†ng")
        logger.info(f"Ch√™nh l·ªách: {abs(len(current_rows) - len(ocr_rows))} h√†ng")
        logger.info(f"\nƒê√£ l∆∞u k·∫øt qu·∫£ debug trong th∆∞ m·ª•c: {test_debug_dir}")
        logger.info("- current_method/: C√°c h√†ng ƒë∆∞·ª£c ph√°t hi·ªán b·ªüi ph∆∞∆°ng ph√°p hi·ªán t·∫°i")
        logger.info("- ocr_method/: C√°c h√†ng ƒë∆∞·ª£c ph√°t hi·ªán b·ªüi ph∆∞∆°ng ph√°p OCR")
        logger.info("- ocr_detection.jpg: ·∫¢nh v·ªõi c√°c bounding box t·ª´ OCR")
        logger.info("- preprocessed.jpg: ·∫¢nh ƒë√£ ti·ªÅn x·ª≠ l√Ω cho OCR")

    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """Ti·ªÅn x·ª≠ l√Ω ·∫£nh tr∆∞·ªõc khi ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª
        
        Args:
            image: ·∫¢nh ƒë·∫ßu v√†o
            
        Returns:
            np.ndarray: ·∫¢nh ƒë√£ ti·ªÅn x·ª≠ l√Ω
        """
        # Chuy·ªÉn sang ·∫£nh x√°m n·∫øu c·∫ßn
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
            
        # L√†m m·ªãn ·∫£nh ƒë·ªÉ gi·∫£m nhi·ªÖu
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Nh·ªã ph√¢n h√≥a v·ªõi Otsu's thresholding
        _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # L∆∞u ·∫£nh ƒë√£ ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ debug
        if self.debug:
            debug_path = os.path.join(self.debug_dir, "preprocessed.jpg")
            cv2.imwrite(debug_path, binary)
            logger.info(f"ƒê√£ l∆∞u ·∫£nh ƒë√£ ti·ªÅn x·ª≠ l√Ω: {debug_path}")
        
        return binary

    def _filter_close_lines(self, line_positions: List[int], min_distance: int = 15) -> List[int]:
        """L·ªçc c√°c ƒë∆∞·ªùng k·∫ª qu√° g·∫ßn nhau
        
        Args:
            line_positions: Danh s√°ch v·ªã tr√≠ c√°c ƒë∆∞·ªùng k·∫ª
            min_distance: Kho·∫£ng c√°ch t·ªëi thi·ªÉu gi·ªØa c√°c ƒë∆∞·ªùng k·∫ª
            
        Returns:
            List[int]: Danh s√°ch v·ªã tr√≠ ƒë∆∞·ªùng k·∫ª sau khi l·ªçc
        """
        if not line_positions:
            return []
            
        # S·∫Øp x·∫øp c√°c v·ªã tr√≠
        sorted_positions = sorted(line_positions)
        
        # L·ªçc c√°c ƒë∆∞·ªùng k·∫ª qu√° g·∫ßn nhau
        filtered = [sorted_positions[0]]  # Gi·ªØ l·∫°i ƒë∆∞·ªùng k·∫ª ƒë·∫ßu ti√™n
        
        for pos in sorted_positions[1:]:
            # So s√°nh v·ªõi ƒë∆∞·ªùng k·∫ª g·∫ßn nh·∫•t ƒë√£ ƒë∆∞·ª£c gi·ªØ l·∫°i
            if pos - filtered[-1] >= min_distance:
                filtered.append(pos)
                
        return filtered

    def _extract_rows_fallback(self, table_image: np.ndarray, table_id: int) -> List[np.ndarray]:
        """Ph∆∞∆°ng ph√°p d·ª± ph√≤ng ƒë·ªÉ c·∫Øt c√°c h√†ng khi kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ƒë∆∞·ªùng k·∫ª
        
        Args:
            table_image: ·∫¢nh b·∫£ng
            table_id: ID c·ªßa b·∫£ng
            
        Returns:
            List[np.ndarray]: Danh s√°ch c√°c ·∫£nh h√†ng ƒë√£ c·∫Øt
        """
        logger.info("S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng ƒë·ªÉ c·∫Øt h√†ng...")
        
        # Chuy·ªÉn sang ·∫£nh x√°m v√† nh·ªã ph√¢n h√≥a
        if len(table_image.shape) == 3:
            gray = cv2.cvtColor(table_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = table_image.copy()
        
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # T√≠nh histogram theo chi·ªÅu d·ªçc
        v_projection = np.sum(binary, axis=1)
        
        # Chu·∫©n h√≥a histogram
        v_projection = v_projection / np.max(v_projection)
        
        # T√¨m c√°c v√πng tr·ªëng (kho·∫£ng tr·∫Øng gi·ªØa c√°c h√†ng)
        height = table_image.shape[0]
        min_gap = 10  # Kho·∫£ng tr·ªëng t·ªëi thi·ªÉu
        gap_threshold = 0.1  # Ng∆∞·ª°ng ƒë·ªÉ x√°c ƒë·ªãnh kho·∫£ng tr·ªëng
        
        # T√¨m c√°c ƒëi·ªÉm b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c·ªßa kho·∫£ng tr·ªëng
        gaps = []
        start_gap = None
        
        for y in range(height):
            if v_projection[y] < gap_threshold:
                if start_gap is None:
                    start_gap = y
            elif start_gap is not None:
                if y - start_gap >= min_gap:
                    gaps.append((start_gap, y))
                start_gap = None
        
        # Th√™m ƒëi·ªÉm cu·ªëi n·∫øu ƒëang trong kho·∫£ng tr·ªëng
        if start_gap is not None and height - start_gap >= min_gap:
            gaps.append((start_gap, height))
        
        # C·∫Øt c√°c h√†ng d·ª±a tr√™n kho·∫£ng tr·ªëng
        rows = []
        prev_end = 0
        
        for gap_start, gap_end in gaps:
            if gap_start > prev_end:
                # C·∫Øt h√†ng t·ª´ ƒëi·ªÉm k·∫øt th√∫c tr∆∞·ªõc ƒë·∫øn ƒëi·ªÉm b·∫Øt ƒë·∫ßu kho·∫£ng tr·ªëng
                row = table_image[prev_end:gap_start]
                if row.shape[0] >= 20:  # Ch·ªâ l·∫•y c√°c h√†ng ƒë·ªß cao
                    rows.append(row)
            prev_end = gap_end
        
        # Th√™m h√†ng cu·ªëi c√πng n·∫øu c·∫ßn
        if prev_end < height:
            row = table_image[prev_end:height]
            if row.shape[0] >= 20:
                rows.append(row)
        
        logger.info(f"ƒê√£ c·∫Øt ƒë∆∞·ª£c {len(rows)} h√†ng b·∫±ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng")
        
        # L∆∞u ·∫£nh debug
        if self.debug:
            debug_image = table_image.copy()
            for i, row in enumerate(rows):
                y_start = sum(r.shape[0] for r in rows[:i])
                y_end = y_start + row.shape[0]
                cv2.rectangle(debug_image, (0, y_start), (table_image.shape[1], y_end), (0, 255, 0), 2)
                cv2.putText(debug_image, f"Row {i+1}", (10, y_start+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            debug_path = os.path.join(self.debug_dir, f"table_{table_id}_rows_fallback.jpg")
            cv2.imwrite(debug_path, debug_image)
        
        return rows

    def extract_rows_using_ocr(self, image_path: str, output_dir: Optional[str] = None,
                                min_row_height: int = 30) -> List[str]:
        """
        Ph√°t hi·ªán v√† tr√≠ch xu·∫•t c√°c h√†ng t·ª´ ·∫£nh s·ª≠ d·ª•ng OCR.
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh
            output_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u ·∫£nh c√°c h√†ng (n·∫øu None, s·ª≠ d·ª•ng th∆∞ m·ª•c c·ªßa ·∫£nh g·ªëc)
            min_row_height: Chi·ªÅu cao t·ªëi thi·ªÉu c·ªßa m·ªói h√†ng
            
        Returns:
            List: Danh s√°ch ƒë∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh c√°c h√†ng
        """
        try:
            # Ki·ªÉm tra pytesseract
            import pytesseract
            
            # ƒê·ªçc ·∫£nh
            img = cv2.imread(image_path)
            if img is None:
                logger.error(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {image_path}")
                return []
                
            # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            blur = cv2.GaussianBlur(gray, (3, 3), 0)
            thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                         cv2.THRESH_BINARY, 11, 2)
                                         
            # Th·ª±c hi·ªán OCR
            config = r'--oem 3 --psm 6'
            boxes = pytesseract.image_to_boxes(thresh, config=config)
            
            if not boxes:
                logger.warning("Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c text trong ·∫£nh")
                return []
                
            # Chuy·ªÉn ƒë·ªïi boxes th√†nh danh s√°ch t·ªça ƒë·ªô
            height = img.shape[0]
            boxes_list = []
            for b in boxes.splitlines():
                b = b.split(' ')
                boxes_list.append({
                    'x': int(b[1]),
                    'y': height - int(b[2]),  # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô y
                    'w': int(b[3]) - int(b[1]),
                    'h': int(b[4]) - int(b[2]),
                    'char': b[0]
                })
                
            # L·ªçc c√°c box c√≥ ƒë·ªô tin c·∫≠y th·∫•p
            boxes_list = [b for b in boxes_list if b['h'] >= min_row_height * 0.3]
            
            # Nh√≥m c√°c box th√†nh h√†ng
            rows = []
            current_row = [boxes_list[0]]
            min_gap = min_row_height * 0.5  # Gi·∫£m kho·∫£ng c√°ch t·ªëi thi·ªÉu
            
            for box in boxes_list[1:]:
                # Ki·ªÉm tra overlap v·ªõi h√†ng hi·ªán t·∫°i
                current_y = sum(b['y'] for b in current_row) / len(current_row)
                if abs(box['y'] - current_y) < min_gap:
                    current_row.append(box)
                else:
                    if len(current_row) > 0:
                        rows.append(current_row)
                    current_row = [box]
                    
            if len(current_row) > 0:
                rows.append(current_row)
                
            # S·∫Øp x·∫øp c√°c h√†ng theo y
            rows.sort(key=lambda r: sum(b['y'] for b in r) / len(r))
            
            # C·∫Øt v√† l∆∞u ·∫£nh c√°c h√†ng
            if output_dir is None:
                output_dir = os.path.dirname(image_path)
                
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                
            row_paths = []
            for i, row in enumerate(rows):
                # T√≠nh to√°n v√πng c·∫Øt
                min_x = min(b['x'] for b in row)
                max_x = max(b['x'] + b['w'] for b in row)
                min_y = min(b['y'] - b['h'] for b in row)
                max_y = max(b['y'] for b in row)
                
                # Th√™m margin
                margin = min_row_height // 4
                min_x = max(0, min_x - margin)
                max_x = min(img.shape[1], max_x + margin)
                min_y = max(0, min_y - margin)
                max_y = min(img.shape[0], max_y + margin)
                
                # C·∫Øt h√†ng
                row_img = img[min_y:max_y, min_x:max_x]
                
                # L∆∞u ·∫£nh h√†ng
                row_filename = f"row_{i+1}.jpg"
                row_path = os.path.join(output_dir, row_filename)
                cv2.imwrite(row_path, row_img)
                row_paths.append(row_path)
                
            logger.info(f"ƒê√£ tr√≠ch xu·∫•t {len(row_paths)} h√†ng s·ª≠ d·ª•ng OCR")
            return row_paths
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t h√†ng b·∫±ng OCR: {str(e)}")
            return []

 