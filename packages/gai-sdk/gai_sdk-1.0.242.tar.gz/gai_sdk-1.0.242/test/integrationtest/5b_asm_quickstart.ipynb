{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Agent State Machine (ASM) - Quick Start Guide\n",
                "\n",
                "An **agent** is a wrapper around a finite state machine designed to accomplish a specific task and will be referred to as ASM.\n",
                "\n",
                "A **StateMachineBuilder** is used to build the ASM from a manifest and a state diagram.\n",
                "\n",
                "```mermaid\n",
                "flowchart TD\n",
                "    A[\"state_diagram\"] -->|Input| B[\"StateMachineBuilder\"]\n",
                "    A2[\"state_manifest\"] -->|Input| C[\"StateModel\"]\n",
                "    C -->|Build| B\n",
                "    B --> D[\"fsm\"]\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: Standard Assistant\n",
                "\n",
                "In this example, we will demonstrate how to create a simple **Assistant** agent using a state diagram and state manifest.\n",
                "\n",
                "### a) Define State Diagram\n",
                "\n",
                "The following is a simple example of an **Assistant** agent using 3 states:\n",
                "\n",
                "* **INIT:** Collect initial input.\n",
                "* **GENERATE:** The state where the LLM generates a response.\n",
                "* **FINAL:** Return final output.\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> GENERATE: next / action\n",
                "GENERATE --> FINAL: next / action\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_DIAGRAM = \"\"\"\n",
                "    INIT --> GENERATE\n",
                "    GENERATE --> FINAL\n",
                "    \"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Define State Manifest\n",
                "\n",
                "The state manifest is a dictionary. \n",
                "Each state in the manifest corresponds to a state in the state diagram.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_MANIFEST_V1 = {\n",
                "    \"INIT\": {\n",
                "        \"input_data\": {\n",
                "            \"user_message\": \"Write a one sentence story\",\n",
                "            \"llm_config\": {\"type\": \"getter\", \"dependency\": \"get_llm_config\"}\n",
                "            }\n",
                "    },\n",
                "    \"GENERATE\": {\n",
                "        \"module_path\": \"gai.asm.states\",\n",
                "        \"class_name\": \"PureActionState\",\n",
                "        \"title\": \"GENERATE\",\n",
                "        \"action\": \"generate\",\n",
                "        \"input_data\": {\n",
                "            \"user_message\": {\"type\":\"state_bag\",\"dependency\":\"user_message\"},\n",
                "            \"llm_config\": {\"type\": \"state_bag\", \"dependency\": \"llm_config\"},\n",
                "        },\n",
                "        \"output_data\": [\"streamer\", \"get_assistant_message\"],\n",
                "    },\n",
                "    \"FINAL\": {\"output_data\": [\"monologue\"]},\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Create state action"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.dialogue.chat import AsyncOpenAI\n",
                "\n",
                "async def generate_action(state):\n",
                "    \n",
                "    llm_config = state.input[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Get message from input\n",
                "    user_message = state.input[\"user_message\"]\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=[{\n",
                "            \"role\":\"user\",\n",
                "            \"content\":user_message\n",
                "            }],\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Build State Machine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.asm import AsyncStateMachine\n",
                "\n",
                "with AsyncStateMachine.StateMachineBuilder(STATE_DIAGRAM) as builder:\n",
                "    fsm = builder.build(\n",
                "        STATE_MANIFEST_V1,\n",
                "        get_llm_config=lambda state: {\n",
                "            \"client_type\": \"anthropic\",\n",
                "            \"model\": \"claude-opus-4-20250514\",\n",
                "        },\n",
                "        # get_llm_config=lambda state: {\n",
                "        #     \"client_type\": \"gai\",\n",
                "        #     \"model\": \"ttt\",\n",
                "        #     \"url\": \"http://gai-llm-svr:12031/gen/v1/chat/completions\"\n",
                "        # },\n",
                "        agent_name=\"Sara\",\n",
                "        generate=generate_action,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Run State Machine (INIT->GENERATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The old lighthouse keeper discovered that the ships he'd been guiding to safety for forty years were actually ghosts, doomed to repeat their final voyage unless someone lit the beacon each night.\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Continue (GENERATE->FINAL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State History:\n",
                        "State: INIT\n",
                        "- input: {'user_message': 'Write a one sentence story', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17c7fd6c0>, 'step': 0, 'time': datetime.datetime(2025, 6, 14, 4, 26, 14, 916184), 'name': 'Sara', 'llm_config': {'client_type': 'anthropic', 'model': 'claude-opus-4-20250514'}}\n",
                        "- output: {'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17de709d0>, 'step': 1, 'time': datetime.datetime(2025, 6, 14, 4, 26, 14, 916283)}\n",
                        "--------------------\n",
                        "State: GENERATE\n",
                        "- input: {'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17c7fe4d0>, 'step': 1, 'time': datetime.datetime(2025, 6, 14, 4, 26, 14, 916488), 'user_message': 'Write a one sentence story', 'llm_config': {'client_type': 'anthropic', 'model': 'claude-opus-4-20250514'}}\n",
                        "- output: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x71a16b3102c0>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x71a16b309bd0>, 'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17c7fe410>, 'step': 2, 'time': datetime.datetime(2025, 6, 14, 4, 26, 18, 32300)}\n",
                        "--------------------\n",
                        "State: FINAL\n",
                        "- input: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x71a16b3102c0>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x71a16b309bd0>, 'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17c7fe410>, 'step': 2, 'time': datetime.datetime(2025, 6, 14, 4, 26, 18, 32300)}\n",
                        "- output: {'monologue': <gai.asm.monologue.Monologue object at 0x71a17c7fe4d0>}\n",
                        "--------------------\n",
                        "Assistant Message:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\"The old lighthouse keeper discovered that the ships he'd been guiding to safety for forty years were actually ghosts, doomed to repeat their final voyage unless someone lit the beacon each night.\""
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "print(\"State History:\")\n",
                "for state in fsm.state_history:\n",
                "    print(f\"State: {state['state']}\")\n",
                "    print(f\"- input: {state['input']}\")\n",
                "    print(f\"- output: {state['output']}\")\n",
                "    print(\"-\" * 20)\n",
                "print(\"Assistant Message:\")\n",
                "fsm.state_bag[\"get_assistant_message\"]()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: Standard Assistant (Part 2)\n",
                "\n",
                "Same example but with an additional state to demonstrate context management by monologue messages.\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> GENERATE\n",
                "GENERATE --> CONTINUE\n",
                "CONTINUE --> FINAL\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_DIAGRAM = \"\"\"\n",
                "    INIT --> GENERATE\n",
                "    GENERATE --> CONTINUE\n",
                "    CONTINUE --> FINAL\n",
                "    \"\"\"\n",
                "    \n",
                "STATE_MANIFEST_V1 = {\n",
                "    \"INIT\": {\n",
                "        \"input_data\": {\n",
                "            \"user_message\": \"Write a one sentence story\",\n",
                "            \"llm_config\": {\"type\": \"getter\", \"dependency\": \"get_llm_config\"},\n",
                "        }\n",
                "    },\n",
                "    \"GENERATE\": {\n",
                "        \"module_path\": \"gai.asm.states\",\n",
                "        \"class_name\": \"PureActionState\",\n",
                "        \"title\": \"GENERATE\",\n",
                "        \"action\": \"generate_action\",\n",
                "        \"input_data\": {\n",
                "            \"user_message\": {\"type\": \"state_bag\", \"dependency\": \"user_message\"},\n",
                "            \"llm_config\": {\"type\": \"state_bag\", \"dependency\": \"llm_config\"},\n",
                "        },\n",
                "        \"output_data\": [\"streamer\", \"get_assistant_message\"],\n",
                "    },\n",
                "    \"CONTINUE\": {\n",
                "        \"module_path\": \"gai.asm.states\",\n",
                "        \"class_name\": \"PureActionState\",\n",
                "        \"title\": \"CONTINUE\",\n",
                "        \"action\": \"continue_action\",\n",
                "        \"input_data\": {\n",
                "            \"llm_config\": {\"type\": \"state_bag\", \"dependency\": \"llm_config\"},\n",
                "        },\n",
                "        \"output_data\": [\"streamer\", \"get_assistant_message\"],\n",
                "    },\n",
                "    \"FINAL\": {\"output_data\": [\"monologue\"]},\n",
                "}\n",
                "\n",
                "from gai.dialogue.chat import AsyncOpenAI\n",
                "\n",
                "async def generate_action(state):\n",
                "\n",
                "    llm_config = state.input[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "\n",
                "    # Import data from state_bag\n",
                "    user_message = state.input[\"user_message\"]\n",
                "    state.machine.monologue.add_user_message(state=state,content=user_message)\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=state.machine.monologue.list_chat_messages(),\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "        state.machine.monologue.add_assistant_message(state=state,content=assistant_message)\n",
                "        # Need to update the stale history due to delayed output\n",
                "        state.machine.state_history[-1][\"output\"][\"monologue\"]=state.machine.monologue.copy()\n",
                "    \n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()\n",
                "\n",
                "async def continue_action(state):\n",
                "    \n",
                "    llm_config = state.input[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    state.machine.monologue.add_user_message(state=state,content=\"Please continue.\")\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=state.machine.monologue.list_chat_messages(),\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "        state.machine.monologue.add_assistant_message(\n",
                "            state=state, content=assistant_message\n",
                "        )\n",
                "        # Need to update the stale history due to delayed output\n",
                "        state.machine.state_history[-1][\"output\"][\"monologue\"] = (\n",
                "            state.machine.monologue.copy()\n",
                "        )\n",
                "    \n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()\n",
                "    \n",
                "from gai.asm import AsyncStateMachine\n",
                "\n",
                "with AsyncStateMachine.StateMachineBuilder(STATE_DIAGRAM) as builder:\n",
                "    fsm = builder.build(\n",
                "        STATE_MANIFEST_V1,\n",
                "        get_llm_config=lambda state: {\n",
                "            \"client_type\": \"anthropic\",\n",
                "            \"model\": \"claude-opus-4-20250514\",\n",
                "        },\n",
                "        # get_llm_config=lambda state: {\n",
                "        #     \"client_type\": \"gai\",\n",
                "        #     \"model\": \"ttt\",\n",
                "        #     \"url\": \"http://gai-llm-svr:12031/gen/v1/chat/completions\"\n",
                "        # },\n",
                "        agent_name=\"Sara\",\n",
                "        generate_action=generate_action,\n",
                "        continue_action=continue_action,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Run State Machine (INIT->GENERATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The old lighthouse keeper discovered that the mysterious flashes he'd been answering for forty years were not from ships, but from another lighthouse keeper on a distant shore, equally alone, speaking in light.\n",
                        "\n",
                        "\n",
                        "Step 1:\n",
                        "Before:\n",
                        "After:\n",
                        "1749875293.8218691 User > Write a one sentence story\n",
                        "1749875299.4398446 Sara > The old lighthouse keeper discovered that the mysterious flashes he'd been answering for forty years were not from ships, but from another lighthouse keeper on a distant shore, equally alone, speaking in light.\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")\n",
                "\n",
                "print(\"Step 1:\")\n",
                "print(\"Before:\")\n",
                "for message in fsm.state_history[1][\"input\"][\"monologue\"].list_messages():\n",
                "    print(\n",
                "        f\"{message.header.timestamp} {message.header.sender} > {message.body.content}\"\n",
                "    )\n",
                "\n",
                "print(\"After:\")\n",
                "for message in fsm.state_history[1][\"output\"][\"monologue\"].list_messages():\n",
                "    print(\n",
                "        f\"{message.header.timestamp} {message.header.sender} > {message.body.content}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Run State Machine (GENERATE->CONTINUE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "When the coast guard arrived that morning to tell him the lighthouse was being decommissioned, he frantically flashed his final message across the dark waters—\"I am here, I have always been here\"—and for the first time in\n",
                        "\n",
                        "\n",
                        "Step 2:\n",
                        "Before:\n",
                        "1749875293.8218691 User > Write a one sentence story\n",
                        "1749875299.4398446 Sara > The old lighthouse keeper discovered that the mysterious flashes he'd been answering for forty years were not from ships, but from another lighthouse keeper on a distant shore, equally alone, speaking in light.\n",
                        "After:\n",
                        "1749875293.8218691 User > Write a one sentence story\n",
                        "1749875299.4398446 Sara > The old lighthouse keeper discovered that the mysterious flashes he'd been answering for forty years were not from ships, but from another lighthouse keeper on a distant shore, equally alone, speaking in light.\n",
                        "1749875308.0710979 User > Please continue.\n",
                        "1749875337.486953 Sara > When the coast guard arrived that morning to tell him the lighthouse was being decommissioned, he frantically flashed his final message across the dark waters—\"I am here, I have always been here\"—and for the first time in\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")\n",
                "\n",
                "print(\"Step 2:\")\n",
                "print(\"Before:\")\n",
                "for message in fsm.state_history[2][\"input\"][\"monologue\"].list_messages():\n",
                "    print(\n",
                "        f\"{message.header.timestamp} {message.header.sender} > {message.body.content}\"\n",
                "    )\n",
                "\n",
                "print(\"After:\")\n",
                "for message in fsm.state_history[2][\"output\"][\"monologue\"].list_messages():\n",
                "    print(\n",
                "        f\"{message.header.timestamp} {message.header.sender} > {message.body.content}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### e) END (GENERATE->FINAL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State History:\n",
                        "State: INIT\n",
                        "- input: {'user_message': 'Write a one sentence story', 'monologue': <gai.asm.monologue.Monologue object at 0x71a16b3760b0>, 'step': 0, 'time': datetime.datetime(2025, 6, 14, 4, 26, 53, 691371), 'name': 'Sara', 'llm_config': {'client_type': 'anthropic', 'model': 'claude-opus-4-20250514'}}\n",
                        "- output: {'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a16b375780>, 'step': 1, 'time': datetime.datetime(2025, 6, 14, 4, 26, 53, 691447)}\n",
                        "--------------------\n",
                        "State: GENERATE\n",
                        "- input: {'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a16b3761a0>, 'step': 1, 'time': datetime.datetime(2025, 6, 14, 4, 26, 53, 691701), 'user_message': 'Write a one sentence story', 'llm_config': {'client_type': 'anthropic', 'model': 'claude-opus-4-20250514'}}\n",
                        "- output: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x71a16b311f40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x71a16b30ac20>, 'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17c7fdba0>, 'step': 2, 'time': datetime.datetime(2025, 6, 14, 4, 26, 56, 437800)}\n",
                        "--------------------\n",
                        "State: CONTINUE\n",
                        "- input: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x71a16b311f40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x71a16b30ac20>, 'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a16b375810>, 'step': 2, 'time': datetime.datetime(2025, 6, 14, 4, 27, 2, 826640), 'llm_config': {'client_type': 'anthropic', 'model': 'claude-opus-4-20250514'}}\n",
                        "- output: {'streamer': <async_generator object continue_action.<locals>.streamer at 0x71a16b3122c0>, 'get_assistant_message': <function continue_action.<locals>.<lambda> at 0x71a16b30b0a0>, 'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17cbe0400>, 'step': 3, 'time': datetime.datetime(2025, 6, 14, 4, 27, 5, 6575)}\n",
                        "--------------------\n",
                        "State: FINAL\n",
                        "- input: {'streamer': <async_generator object continue_action.<locals>.streamer at 0x71a16b3122c0>, 'get_assistant_message': <function continue_action.<locals>.<lambda> at 0x71a16b30b0a0>, 'name': 'Sara', 'monologue': <gai.asm.monologue.Monologue object at 0x71a17cbe0400>, 'step': 3, 'time': datetime.datetime(2025, 6, 14, 4, 27, 5, 6575)}\n",
                        "- output: {'monologue': <gai.asm.monologue.Monologue object at 0x71a16b375810>}\n",
                        "--------------------\n",
                        "Assistant Message:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\"As dawn broke over the harbor, Thomas set down his cup of cold coffee and watched the translucent hull of a schooner fade into the morning mist, just as it had every Tuesday since 1847. He'd figured out the\""
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "print(\"State History:\")\n",
                "for state in fsm.state_history:\n",
                "    print(f\"State: {state['state']}\")\n",
                "    print(f\"- input: {state['input']}\")\n",
                "    print(f\"- output: {state['output']}\")\n",
                "    print(\"-\" * 20)\n",
                "print(\"Assistant Message:\")\n",
                "fsm.state_bag[\"get_assistant_message\"]()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: ToolUse Assistant using Anthropic models\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> TOOL_CALL\n",
                "TOOL_CALL --> TOOL_USE\n",
                "TOOL_USE --> FINAL\n",
                "```\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.asm import AsyncStateMachine, FileMonologue\n",
                "from gai.mcp.client.mcp_client import McpAggregatedClient\n",
                "monologue = FileMonologue()\n",
                "monologue.reset()\n",
                "\n",
                "builder = AsyncStateMachine.StateMachineBuilder(\n",
                "    \"\"\"\n",
                "    INIT --> TOOL_CALL\n",
                "    TOOL_CALL--> TOOL_USE\n",
                "    TOOL_USE --> FINAL\n",
                "    \"\"\"\n",
                ")    \n",
                "fsm = builder.build(\n",
                "    {\n",
                "        \"INIT\": {\n",
                "            \"input_data\": {\n",
                "                \"user_message\": \"What time is it now?\",\n",
                "                \"llm_config\": {\"type\": \"getter\", \"dependency\": \"get_llm_config\"},\n",
                "                \"mcp_client\": {\"type\": \"getter\", \"dependency\": \"get_mcp_client\"},\n",
                "            }\n",
                "        },\n",
                "        \"TOOL_CALL\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"AnthropicToolCallState\",\n",
                "            \"title\": \"TOOL_CALL\",\n",
                "            \"input_data\": {\n",
                "                \"user_message\": {\"type\": \"state_bag\", \"dependency\": \"user_message\"},\n",
                "                \"llm_config\": {\"type\": \"state_bag\", \"dependency\": \"llm_config\"},\n",
                "                \"mcp_client\": {\"type\": \"state_bag\", \"dependency\": \"mcp_client\"},\n",
                "            },\n",
                "            \"output_data\": [\"streamer\", \"get_assistant_message\"],\n",
                "        },\n",
                "        \"TOOL_USE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"AnthropicToolUseState\",\n",
                "            \"title\": \"TOOL_USE\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\"type\": \"state_bag\", \"dependency\": \"llm_config\"},\n",
                "                \"mcp_client\": {\"type\": \"state_bag\", \"dependency\": \"mcp_client\"},\n",
                "            },\n",
                "            \"output_data\": [\"streamer\", \"get_assistant_message\"],\n",
                "        },\n",
                "        \"FINAL\": {\"output_data\": [\"monologue\"]},\n",
                "    },\n",
                "    get_llm_config=lambda state: {\n",
                "        \"client_type\": \"anthropic\",\n",
                "        # \"model\": \"claude-opus-4-20250514\",\n",
                "        \"model\": \"claude-sonnet-4-20250514\",\n",
                "        \"max_tokens\": 32000,\n",
                "        \"temperature\": 0.7,\n",
                "        \"top_p\": 0.95,\n",
                "        \"tools\": True,\n",
                "    },\n",
                "    get_mcp_client=lambda state: McpAggregatedClient([\"mcp-time\"]),  # Replace with actual MCP client if needed\n",
                "    monologue=monologue,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### a) Generate Tool Call (INIT -> TOOL_CALL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "\n",
                        "1750071176.122163 User > What time is it now?\n",
                        "1750071179.683299 Assistant > [{'id': 'toolu_01NSErGxSbMH1xp4oT5qUUpV', 'input': {'format': 'YYYY-MM-DD HH:mm:ss'}, 'name': 'current_time', 'type': 'tool_use'}]\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    if isinstance(chunk, str):\n",
                "        print(chunk, end=\"\", flush=True)\n",
                "print(\"\\n\\n\")\n",
                "for message in fsm.state_history[-1][\"output\"][\"monologue\"].list_messages():\n",
                "    print(\n",
                "        f\"{message.header.timestamp} {message.header.sender} > {message.body.content}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Use Tool (TOOL_CALL -> TOOL_USE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The current time is **2025-06-16 10:53:02 UTC**.\n",
                        "\n",
                        "\n",
                        "1750071176.122163 User > What time is it now?\n",
                        "1750071179.683299 Assistant > [{'id': 'toolu_01NSErGxSbMH1xp4oT5qUUpV', 'input': {'format': 'YYYY-MM-DD HH:mm:ss'}, 'name': 'current_time', 'type': 'tool_use'}]\n",
                        "1750071182.448625 User > [{'type': 'tool_result', 'tool_use_id': 'toolu_01NSErGxSbMH1xp4oT5qUUpV', 'content': 'Current UTC time is 2025-06-16 10:53:02, and the time in UTC is 2025-06-16 10:53:02.'}]\n",
                        "1750071184.4425583 Assistant > [{'citations': None, 'text': 'The current time is **2025-06-16 10:53:02 UTC**.', 'type': 'text'}]\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    if isinstance(chunk, str):\n",
                "        print(chunk, end=\"\", flush=True)\n",
                "print(\"\\n\\n\")\n",
                "for message in fsm.state_history[-1][\"output\"][\"monologue\"].list_messages():\n",
                "    print(\n",
                "        f\"{message.header.timestamp} {message.header.sender} > {message.body.content}\"\n",
                "    )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
