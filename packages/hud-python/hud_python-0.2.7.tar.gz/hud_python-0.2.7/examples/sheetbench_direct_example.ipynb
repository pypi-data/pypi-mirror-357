{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/hud-evals/hud-sdk/main/docs/logo/hud_logo.svg\" alt=\"HUD\" width=\"100\"/>\n",
    "</div>\n",
    "\n",
    "[ Running SheetBench on an excel agent ]\n",
    "\n",
    "```bash\n",
    "export HUD_API_KEY=your_api_key_here\n",
    "pip install hud-python\n",
    "pip install pandas # optional for debugging\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Verbose example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import pandas as pd  # optional for debugging\n",
    "from io import BytesIO  # optional for debugging\n",
    "\n",
    "from hud import gym\n",
    "from hud.taskset import load_taskset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the SheetBench-50 taskset and enables partial grading\n",
    "taskset = await load_taskset(\"SheetBench-50\", metadata={\"partial\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Sheet REV_QTR lists quarterly revenue from Q1-2022 through Q1-2025. Compute the compound annual growth rate between those two points. Enter the result in ANSWER!A1.\n",
      "['REV_QTR']\n",
      "   Quarter  Revenue\n",
      "0  Q1-2022  4200000\n",
      "1  Q2-2022  4404653\n",
      "2  Q3-2022  4746171\n",
      "3  Q4-2022  5062265\n",
      "4  Q1-2023  5365661\n"
     ]
    }
   ],
   "source": [
    "# Load in the first task for testing\n",
    "test_task = taskset[1]\n",
    "prompt = test_task.prompt\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "# The setup function for the SheetBench tasks is a direct xlsx download link\n",
    "download_link = test_task.setup.args[0]  # type: ignore\n",
    "\n",
    "# Download the xlsx file\n",
    "input_xlsx_req = requests.get(download_link)\n",
    "input_xlsx_req.raise_for_status()\n",
    "input_xlsx_file = input_xlsx_req.content\n",
    "\n",
    "print(pd.ExcelFile(BytesIO(input_xlsx_file)).sheet_names)\n",
    "print(\n",
    "    pd.ExcelFile(BytesIO(input_xlsx_file))\n",
    "    .parse(pd.ExcelFile(BytesIO(input_xlsx_file)).sheet_names[0])\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['REV_QTR', 'ANSWER']\n",
      "Empty DataFrame\n",
      "Columns: [0.241955862]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Your agent loop goes here, using *prompt* and *input_xlsx_file*, returns *output_xlsx_file*\n",
    "###\n",
    "\n",
    "# For testing, we'll use the gold solution\n",
    "gold_solution = requests.get(\n",
    "    \"https://gahludmjcsmszgyufydt.supabase.co/storage/v1/object/public/sheetbench/852a6a9e-7e9f-4563-8298-20e80ee0a66a/Copy%20of%20Derivations_CAGR_WORKBOOK.xlsx\"\n",
    ")\n",
    "output_xlsx_file = gold_solution.content\n",
    "print(pd.ExcelFile(BytesIO(output_xlsx_file)).sheet_names)\n",
    "print(\n",
    "    pd.ExcelFile(BytesIO(output_xlsx_file))\n",
    "    .parse(pd.ExcelFile(BytesIO(output_xlsx_file)).sheet_names[1])\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get the base64 encoded xlsx file\n",
    "base64_output_xlsx_file = base64.b64encode(output_xlsx_file).decode(\"utf-8\")\n",
    "\n",
    "# Adapt the task to set up an evaluation environment with the output xlsx file\n",
    "test_task.setup = (\"sheets_from_bytes\", base64_output_xlsx_file)\n",
    "test_task.id = None\n",
    "\n",
    "# Use a hud environment to evaluate the agent\n",
    "env = await gym.make(test_task)\n",
    "result = await env.evaluate()\n",
    "print(f\"Reward: {result['reward']}\")\n",
    "\n",
    "# obs, _ = await env.reset() # get obs[\"screenshot\"] to visualize\n",
    "# await env.stream() # to see the live state of the environment for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "await env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally turn off hud logging\n",
    "# import logging\n",
    "# logging.getLogger(\"hud\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_single_task(task):\n",
    "    prompt = task.prompt\n",
    "    input_xlsx_file = requests.get(task.setup.args[0]).content\n",
    "\n",
    "    # TODO: Implement the agent loop using *prompt* and *input_xlsx_file*\n",
    "    # TODO: Return the *output_xlsx_file* as a base64 encoded string\n",
    "\n",
    "    # The input alone will return a 0 reward\n",
    "    base64_output_xlsx_file = base64.b64encode(input_xlsx_file).decode(\"utf-8\")\n",
    "\n",
    "    # Run evaluation\n",
    "    task.setup = (\"sheets_from_bytes\", base64_output_xlsx_file)\n",
    "    task.id = None\n",
    "    env = await gym.make(task)\n",
    "    result = await env.evaluate()\n",
    "    await env.close()\n",
    "\n",
    "    return result[\"reward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Loading and evaluating 50 tasks should take around 2 minutes, without the agent loop\n",
    "import asyncio\n",
    "\n",
    "taskset = await load_taskset(\"SheetBench-50\", metadata={\"partial\": True})\n",
    "\n",
    "task_jobs = [run_single_task(task) for task in taskset]\n",
    "rewards = await asyncio.gather(*task_jobs)\n",
    "\n",
    "print(f\"Average reward: {sum(rewards) / len(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
