{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing PySpark Analyzer Locally\n",
    "\n",
    "This notebook demonstrates how to test the `pyspark-analyzer` package locally with Spark running in local mode.\n",
    "\n",
    "## Prerequisites\n",
    "- Java 17+ installed (required for PySpark)\n",
    "- Python 3.8+\n",
    "- The pyspark-analyzer package installed in development mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Java Environment and Imports\n",
    "\n",
    "First, we need to ensure Java is properly configured for PySpark. The following cell will automatically detect and configure Java 17 if it's not already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Java environment for PySpark\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Function to detect and set Java home\n",
    "def setup_java_for_notebook():\n",
    "    \"\"\"Setup Java environment for PySpark in Jupyter notebook.\"\"\"\n",
    "\n",
    "    # Skip if already configured\n",
    "    if os.environ.get(\"JAVA_HOME\"):\n",
    "        print(f\"✅ Java already configured at: {os.environ['JAVA_HOME']}\")\n",
    "        return\n",
    "\n",
    "    # Try to find Java 17 in common locations\n",
    "    java_paths = [\n",
    "        \"/opt/homebrew/opt/openjdk@17\",  # Apple Silicon Macs\n",
    "        \"/usr/local/opt/openjdk@17\",      # Intel Macs\n",
    "    ]\n",
    "\n",
    "    for path in java_paths:\n",
    "        if os.path.exists(path):\n",
    "            os.environ[\"JAVA_HOME\"] = path\n",
    "            os.environ[\"PATH\"] = f\"{path}/bin:{os.environ.get('PATH', '')}\"\n",
    "            print(f\"✅ Java configured at: {path}\")\n",
    "            break\n",
    "    else:\n",
    "        # Try to find Java using /usr/libexec/java_home on macOS\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"/usr/libexec/java_home\", \"-v\", \"17\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                java_home = result.stdout.strip()\n",
    "                os.environ[\"JAVA_HOME\"] = java_home\n",
    "                os.environ[\"PATH\"] = f\"{java_home}/bin:{os.environ.get('PATH', '')}\"\n",
    "                print(f\"✅ Java configured at: {java_home}\")\n",
    "            else:\n",
    "                print(\"❌ Could not find Java 17. Please install it using: brew install openjdk@17\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error finding Java: {e}\")\n",
    "\n",
    "    # Set other required environment variables\n",
    "    os.environ.setdefault(\"SPARK_LOCAL_IP\", \"127.0.0.1\")\n",
    "\n",
    "    # Set PySpark to use the current Python interpreter\n",
    "    if not os.environ.get(\"PYSPARK_PYTHON\"):\n",
    "        os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "        os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "    # Reduce Spark log verbosity\n",
    "    os.environ.setdefault(\"SPARK_SUBMIT_OPTS\", \"-Dlog4j.logLevel=ERROR\")\n",
    "\n",
    "# Run the setup\n",
    "setup_java_for_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# Run this cell if you haven't installed the packages yet\n",
    "!{sys.executable} -m pip install pyspark pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to path to import pyspark_analyzer\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType,\n",
    "    DoubleType, TimestampType, BooleanType\n",
    ")\n",
    "\n",
    "# Import pyspark_analyzer\n",
    "from pyspark_analyzer import analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session\n",
    "\n",
    "We'll create a local Spark session with some optimized settings for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Analyzer Local Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce output noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample Datasets\n",
    "\n",
    "Let's create various sample datasets to test different features of the analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample e-commerce dataset\n",
    "def create_ecommerce_data(num_rows=10000):\n",
    "    \"\"\"Create a sample e-commerce dataset for testing\"\"\"\n",
    "\n",
    "    # Generate sample data\n",
    "    data = []\n",
    "    categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', None]\n",
    "    payment_methods = ['Credit Card', 'PayPal', 'Debit Card', 'Apple Pay', None]\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        order_date = datetime.now() - timedelta(days=random.randint(0, 365))\n",
    "\n",
    "        data.append({\n",
    "            'order_id': i + 1,\n",
    "            'customer_id': random.randint(1, num_rows // 10),\n",
    "            'product_name': f'Product_{random.randint(1, 1000)}',\n",
    "            'category': random.choice(categories),\n",
    "            'price': round(random.uniform(10, 1000), 2) if random.random() > 0.05 else None,\n",
    "            'quantity': random.randint(1, 10),\n",
    "            'order_date': order_date,\n",
    "            'payment_method': random.choice(payment_methods),\n",
    "            'is_returned': random.random() < 0.1,\n",
    "            'rating': random.choice([1, 2, 3, 4, 5, None]),\n",
    "            'discount_percentage': random.choice([0, 10, 20, 30, None])\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"order_id\", IntegerType(), False),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"order_date\", TimestampType(), True),\n",
    "        StructField(\"payment_method\", StringType(), True),\n",
    "        StructField(\"is_returned\", BooleanType(), True),\n",
    "        StructField(\"rating\", IntegerType(), True),\n",
    "        StructField(\"discount_percentage\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create the dataset\n",
    "df_ecommerce = create_ecommerce_data(10000)\n",
    "print(f\"Created e-commerce dataset with {df_ecommerce.count():,} rows and {len(df_ecommerce.columns)} columns\")\n",
    "df_ecommerce.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Analysis\n",
    "\n",
    "Let's run the analyzer with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run basic analysis (returns pandas DataFrame by default)\n",
    "profile_df = analyze(df_ecommerce)\n",
    "\n",
    "# Display the results\n",
    "print(\"Column Statistics:\")\n",
    "profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access metadata\n",
    "print(\"Dataset Overview:\")\n",
    "for key, value in profile_df.attrs['overview'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Different Output Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary format for programmatic access\n",
    "profile_dict = analyze(df_ecommerce, output_format=\"dict\")\n",
    "\n",
    "print(\"Overview:\")\n",
    "for key, value in profile_dict['overview'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nSample column statistics (price):\")\n",
    "for key, value in profile_dict['columns']['price'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get human-readable summary\n",
    "summary = analyze(df_ecommerce, output_format=\"summary\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Sampling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset to test sampling\n",
    "df_large = create_ecommerce_data(15000000)\n",
    "print(f\"Created large dataset with {df_large.count():,} rows\")\n",
    "\n",
    "# Test different sampling options\n",
    "print(\"\\n1. Auto-sampling (default behavior):\")\n",
    "profile_auto = analyze(df_large, output_format=\"dict\")\n",
    "print(f\"   Sample size: {profile_auto['sampling']['sample_size']:,} rows\")\n",
    "print(f\"   Is sampled: {profile_auto['sampling']['is_sampled']}\")\n",
    "if profile_auto['sampling']['is_sampled']:\n",
    "    print(f\"   Estimated speedup: {profile_auto['sampling']['estimated_speedup']:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with specific target rows\n",
    "print(\"\\n2. Sample to specific number of rows:\")\n",
    "profile_target = analyze(df_large, target_rows=5000, seed=42, output_format=\"dict\")\n",
    "print(f\"   Sample size: {profile_target['sampling']['sample_size']:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with fraction sampling\n",
    "print(\"\\n3. Sample by fraction:\")\n",
    "profile_fraction = analyze(df_large, fraction=0.1, seed=42, output_format=\"dict\")\n",
    "print(f\"   Sample size: {profile_fraction['sampling']['sample_size']:,} rows\")\n",
    "print(\"   Fraction used: 0.1 (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without sampling\n",
    "print(\"\\n4. Disable sampling:\")\n",
    "profile_no_sample = analyze(df_large, sampling=False, output_format=\"dict\")\n",
    "print(f\"   Analyzed rows: {profile_no_sample['overview']['total_rows']:,}\")\n",
    "print(f\"   Is sampled: {profile_no_sample['sampling']['is_sampled']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable advanced statistics\n",
    "profile_advanced = analyze(df_ecommerce, include_advanced=True)\n",
    "\n",
    "profile_advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable data quality analysis\n",
    "profile_quality = analyze(df_ecommerce, include_quality=False, include_advanced=False)\n",
    "\n",
    "profile_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Profile Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile only specific columns\n",
    "columns_to_profile = ['price', 'category', 'order_date', 'is_returned']\n",
    "profile_subset = analyze(df_ecommerce, columns=columns_to_profile)\n",
    "\n",
    "print(f\"Profiled {len(profile_subset)} columns:\")\n",
    "profile_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test with Different Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with various data types\n",
    "from decimal import Decimal\n",
    "\n",
    "complex_data = [\n",
    "    {\n",
    "        'id': i,\n",
    "        'name': f'Item_{i}',\n",
    "        'description': 'Lorem ipsum ' * random.randint(1, 10) if random.random() > 0.1 else None,\n",
    "        'price_decimal': Decimal(str(round(random.uniform(10, 1000), 2))),\n",
    "        'created_date': datetime.now() - timedelta(days=random.randint(0, 365)),\n",
    "        'tags': ['tag1', 'tag2', 'tag3'][:random.randint(1, 3)],\n",
    "        'metadata': {'key': 'value', 'count': random.randint(1, 100)},\n",
    "        'is_active': random.choice([True, False, None]),\n",
    "        'score': random.uniform(0, 100) if random.random() > 0.1 else float('nan'),\n",
    "    }\n",
    "    for i in range(1000)\n",
    "]\n",
    "\n",
    "df_complex = spark.createDataFrame(complex_data)\n",
    "print(\"Complex dataset schema:\")\n",
    "df_complex.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze complex dataset\n",
    "profile_complex = analyze(df_complex)\n",
    "print(\"Profile of complex data types:\")\n",
    "profile_complex[['column_name', 'data_type', 'null_count', 'distinct_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create datasets of different sizes\n",
    "sizes = [1000, 10000, 50000]\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    df_test = create_ecommerce_data(size)\n",
    "\n",
    "    # With sampling\n",
    "    start = time.time()\n",
    "    _ = analyze(df_test, output_format=\"dict\")\n",
    "    time_with_sampling = time.time() - start\n",
    "\n",
    "    # Without sampling\n",
    "    start = time.time()\n",
    "    _ = analyze(df_test, sampling=False, output_format=\"dict\")\n",
    "    time_without_sampling = time.time() - start\n",
    "\n",
    "    results.append({\n",
    "        'rows': size,\n",
    "        'with_sampling': round(time_with_sampling, 2),\n",
    "        'without_sampling': round(time_without_sampling, 2),\n",
    "        'speedup': round(time_without_sampling / time_with_sampling, 1) if time_with_sampling > 0 else 1\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(results)\n",
    "print(\"Performance Comparison:\")\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get profile as pandas DataFrame\n",
    "import json\n",
    "\n",
    "final_profile = analyze(df_ecommerce, include_advanced=True, include_quality=True)\n",
    "\n",
    "# Save to different formats\n",
    "output_dir = \"profile_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV\n",
    "final_profile.to_csv(f\"{output_dir}/profile.csv\", index=False)\n",
    "print(f\"Saved profile to {output_dir}/profile.csv\")\n",
    "\n",
    "# Parquet\n",
    "final_profile.to_parquet(f\"{output_dir}/profile.parquet\", index=False)\n",
    "print(f\"Saved profile to {output_dir}/profile.parquet\")\n",
    "\n",
    "# HTML\n",
    "final_profile.to_html(f\"{output_dir}/profile.html\", index=False)\n",
    "print(f\"Saved profile to {output_dir}/profile.html\")\n",
    "\n",
    "# JSON (using dict format)\n",
    "profile_json = analyze(df_ecommerce, include_advanced=True, include_quality=True, output_format=\"dict\")\n",
    "with open(f\"{output_dir}/profile.json\", 'w') as f:\n",
    "    json.dump(profile_json, f, indent=2, default=str)\n",
    "print(f\"Saved profile to {output_dir}/profile.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Setting up a local Spark session\n",
    "2. Creating sample datasets for testing\n",
    "3. Running basic analysis with default settings\n",
    "4. Using different output formats (pandas, dict, summary)\n",
    "5. Testing sampling features with various configurations\n",
    "6. Enabling advanced statistics and data quality analysis\n",
    "7. Profiling specific columns\n",
    "8. Working with different data types\n",
    "9. Performance comparison with and without sampling\n",
    "10. Exporting results to various formats\n",
    "\n",
    "### Next Steps\n",
    "- Try with your own datasets\n",
    "- Experiment with larger datasets to see sampling benefits\n",
    "- Customize sampling thresholds and configurations\n",
    "- Integrate the profiler into your data pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
