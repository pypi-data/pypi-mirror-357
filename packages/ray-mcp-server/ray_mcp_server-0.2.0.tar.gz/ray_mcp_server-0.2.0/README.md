# âš¡ Ray MCP Server

ğŸš€ **Supercharge your AI workflows with distributed computing!** 

A powerful Model Context Protocol (MCP) server that brings Ray's distributed computing capabilities directly to Claude Desktop. Manage clusters, submit jobs, and orchestrate complex workflows through natural language commands! ğŸ¯

## âœ¨ Features

- ğŸ—ï¸ **Multi-Node Cluster Management**: Start and manage Ray clusters with head nodes and worker nodes
- ğŸš€ **Job Management**: Submit, monitor, and manage Ray jobs with ease
- ğŸ­ **Actor Management**: Create and manage Ray actors for stateful computations
- ğŸ“Š **Real-time Monitoring**: Get cluster status, resource usage, and performance metrics
- ğŸ” **Logging and Debugging**: Access logs and debug job issues seamlessly
- â° **Scheduling**: Schedule jobs with cron-like syntax for automated workflows

## ğŸš€ Quick Start

### ğŸ“¦ Installation

**ğŸš€ Super Easy with uvx (Recommended):**
```bash
# Install and run directly with uvx - no setup needed! âš¡
uvx ray-mcp-server
```

**ğŸ“¥ Or clone for development:**
```bash
# Clone the repository
git clone <repository-url>
cd ray-mcp

# Install dependencies with UV (lightning fast! âš¡)
uv sync

# You're ready to go! ğŸ‰
```

### ğŸ¯ Starting Ray Clusters

The server supports both single-node and multi-node cluster configurations:

#### ğŸ–¥ï¸ Simple Single-Node Cluster

```json
{
  "tool": "start_ray",
  "arguments": {
    "num_cpus": 4,
    "num_gpus": 1
  }
}
```

#### ğŸŒ Multi-Node Cluster (Default)

The server now defaults to starting multi-node clusters with 2 worker nodes (perfect for scaling! ğŸ“ˆ):

```json
{
  "tool": "start_ray",
  "arguments": {
    "num_cpus": 1
  }
}
```

This creates:
- ğŸ§  **Head node**: 1 CPU, 0 GPUs, 1GB object store memory
- âš™ï¸ **Worker node 1**: 2 CPUs, 0 GPUs, 500MB object store memory  
- âš™ï¸ **Worker node 2**: 2 CPUs, 0 GPUs, 500MB object store memory

#### ğŸ› ï¸ Custom Multi-Node Setup

For advanced configurations, you can specify custom worker nodes:

```json
{
  "tool": "start_ray",
  "arguments": {
    "num_cpus": 4,
    "num_gpus": 0,
    "object_store_memory": 1000000000,
    "worker_nodes": [
      {
        "num_cpus": 2,
        "num_gpus": 0,
        "object_store_memory": 500000000,
        "node_name": "cpu-worker-1"
      },
      {
        "num_cpus": 4,
        "num_gpus": 1,
        "object_store_memory": 1000000000,
        "node_name": "gpu-worker-1",
        "resources": {"custom_resource": 2}
      }
    ],
    "head_node_port": 10001,
    "dashboard_port": 8265,
    "head_node_host": "127.0.0.1"
  }
}
```

### ğŸ’« Basic Usage

Just ask Claude Desktop naturally! ğŸ—£ï¸

- *"What's my Ray cluster status?"* 
- *"Submit a job to process my data"*
- *"Show me cluster resources"*
- *"List all running jobs"*

Or use direct tool calls:
```json
{
  "tool": "cluster_status"
}
```

## ğŸ› ï¸ Available Tools

20+ powerful tools for comprehensive Ray management! ğŸ’ª

### ğŸ—ï¸ Cluster Operations
- ğŸš€ `start_ray` - Start a new Ray cluster with head node and optional worker nodes
- ğŸ”— `connect_ray` - Connect to an existing Ray cluster
- ğŸ›‘ `stop_ray` - Stop the current Ray cluster
- ğŸ“Š `cluster_status` - Get comprehensive cluster status
- ğŸ’¾ `cluster_resources` - Get resource usage information
- ğŸ–¥ï¸ `cluster_nodes` - List all cluster nodes
- âš™ï¸ `worker_status` - Get detailed status of worker nodes

### ğŸš€ Job Operations
- ğŸ“¤ `submit_job` - Submit a new job to the cluster
- ğŸ“‹ `list_jobs` - List all jobs (running, completed, failed)
- ğŸ” `job_status` - Get detailed status of a specific job
- âŒ `cancel_job` - Cancel a running or queued job
- ğŸ‘€ `monitor_job` - Monitor job progress in real-time
- ğŸ› `debug_job` - Debug a job with detailed information
- ğŸ“œ `get_logs` - Retrieve job logs and outputs

### ğŸ­ Actor Operations
- ğŸ‘¥ `list_actors` - List all actors in the cluster
- ğŸ’€ `kill_actor` - Terminate a specific actor

### ğŸ“ˆ Enhanced Monitoring
- âš¡ `performance_metrics` - Get detailed cluster performance metrics
- ğŸ¥ `health_check` - Perform comprehensive cluster health check
- ğŸ¯ `optimize_config` - Get cluster optimization recommendations

### â° Job Scheduling
- ğŸ“… `schedule_job` - Configure job scheduling parameters

## ğŸ“š Examples

Ready-to-run examples in the `examples/` directory:

- ğŸ¯ `simple_job.py` - Basic Ray job example (start here!)
- ğŸŒ `multi_node_cluster.py` - Multi-node cluster with worker management
- ğŸ­ `actor_example.py` - Actor-based stateful computation
- ğŸ”„ `data_pipeline.py` - Scalable data processing pipeline
- ğŸ¤– `distributed_training.py` - Distributed machine learning
- ğŸ¼ `workflow_orchestration.py` - Complex workflow orchestration

## ğŸ”Œ Claude Desktop Integration

### âš¡ Quick Setup

1. **ğŸ¯ Choose your installation method:**

   **Option A: uvx (Easiest! ğŸš€)**
   ```bash
   # No installation needed! uvx handles everything
   ```

   **Option B: Development setup**
   ```bash
   git clone <repository-url>
   cd ray-mcp
   uv sync
   ```

2. **âš™ï¸ Add to Claude Desktop config** (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):

   **For uvx installation:**
   ```json
   {
     "mcpServers": {
       "ray-mcp": {
         "command": "uvx",
         "args": ["ray-mcp-server"]
       }
     }
   }
   ```

   **For development setup:**
   ```json
   {
     "mcpServers": {
       "ray-mcp": {
         "command": "/opt/homebrew/bin/uv",
         "args": ["run", "--directory", "/absolute/path/to/ray-mcp", "ray-mcp-server"]
       }
     }
   }
   ```

3. **ğŸŒ For remote Ray clusters** (like Kubernetes), add port-forwarding and environment:
   ```bash
   kubectl port-forward -n ray-cluster ray-cluster-kuberay-head-<pod-id> 10001:10001
   ```
   
   **For uvx:**
   ```json
   {
     "mcpServers": {
       "ray-mcp": {
         "command": "uvx",
         "args": ["ray-mcp-server"],
         "env": {
           "RAY_ADDRESS": "ray://127.0.0.1:10001"
         }
       }
     }
   }
   ```

4. **ğŸ”„ Restart Claude Desktop** and test with: *"What Ray tools are available?"* ğŸ‰

### ğŸ“– Detailed Setup Guides

- ğŸš¢ **Kubernetes Ray Clusters**: See [docs/KUBERNETES_SETUP.md](docs/KUBERNETES_SETUP.md)
- âš™ï¸ **Configuration Examples**: See [docs/config/](docs/config/)
- ğŸ‘¨â€ğŸ’» **Development Guide**: See [CLAUDE.md](CLAUDE.md)

## ğŸ› ï¸ Development

### ğŸ§ª Running Tests

```bash
# Run all tests
uv run pytest

# Run specific test categories
uv run pytest tests/test_mcp_tools.py
uv run pytest tests/test_multi_node_cluster.py
uv run pytest tests/test_e2e_integration.py
```

### âœ¨ Code Quality

```bash
# Run linting and formatting checks
make lint

# Format code automatically
make format

# Run type checking
uv run pyright ray_mcp/

# Run code formatting
uv run black ray_mcp/
uv run isort ray_mcp/
```

## ğŸ“„ License

This project is licensed under the Apache License, Version 2.0 - see the [LICENSE](LICENSE) file for details.

This software includes code originally from [ray-mcp](https://github.com/pradeepiyer/ray-mcp) licensed under the MIT License. See [NOTICE](NOTICE) file for full attribution details.

---

**Ready to supercharge your Ray workflows? Get started now!** ğŸš€âœ¨ 