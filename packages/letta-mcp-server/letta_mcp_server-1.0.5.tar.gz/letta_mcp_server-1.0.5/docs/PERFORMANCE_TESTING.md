# Letta MCP Server Performance Testing Guide

This guide explains how to run and interpret performance tests for the Letta MCP Server.

## Overview

The performance testing framework validates the speed claims made in the README.md by comparing:
- **Direct Letta SDK** usage vs **MCP Server** performance
- Connection pooling benefits
- Resource efficiency under load
- Realistic dealer workflow scenarios

## Quick Start

### Prerequisites

1. **Letta API Key**: Set your API key in environment
   ```bash
   export LETTA_API_KEY="sk-let-your-key-here"
   ```

2. **Performance Testing Flag**: Enable performance tests
   ```bash
   export RUN_PERFORMANCE_TESTS=1
   ```

3. **Dependencies**: Install required packages
   ```bash
   pip install letta-client psutil
   ```

### Running Tests

#### Quick Performance Validation (Recommended)
```bash
python scripts/run_performance_validation.py --quick
```

#### Comprehensive Performance Suite
```bash
python scripts/run_performance_validation.py --full
```

#### Comparative Benchmarks Only
```bash
python scripts/run_performance_validation.py --comparative-only
```

#### Manual pytest Execution
```bash
RUN_PERFORMANCE_TESTS=1 pytest tests/performance/ -v
```

## Performance Claims Being Validated

Our README.md makes these specific performance claims:

| Operation | Claimed Improvement | Test Method |
|-----------|-------------------|-------------|
| Agent List | 4x faster | Direct SDK vs MCP comparison |
| Memory Update | 3.7x faster | Core memory operations timing |
| Agent Chat | 5x faster | Message sending benchmarks |
| Message Send | 15% faster | End-to-end messaging |
| Tool Attach | 5.3x faster | Tool management operations |

## Test Categories

### 1. Comparative Performance Tests
**File**: `tests/performance/test_comparative_benchmarks.py`

Tests that directly compare Direct Letta SDK vs MCP Server:
- Agent listing performance
- Agent details retrieval  
- Memory operations (get/update)
- Message sending performance
- Statistical analysis with multiple iterations

### 2. Connection Pooling Benefits
**File**: `tests/performance/test_comparative_benchmarks.py::TestConnectionPoolingBenefits`

Validates connection pooling performance benefits:
- Concurrent operation scaling (1, 5, 10, 20 requests)
- Resource efficiency under sustained load
- Throughput measurements
- Success rate validation

### 3. Realistic Workload Scenarios
**File**: `tests/performance/test_comparative_benchmarks.py::TestRealisticWorkloadScenarios`

Real-world usage pattern tests:
- Dealer analysis workflow simulation
- Multi-agent interaction scenarios
- End-to-end operation validation
- Workflow completion timing

### 4. Existing MCP Performance Tests
**File**: `tests/performance/test_benchmarks.py`

Comprehensive MCP server performance suite:
- Response time benchmarks
- Throughput testing
- Memory usage analysis
- Scalability characteristics

## Test Configuration

### Environment Variables

```bash
# Required
export LETTA_API_KEY="sk-let-your-api-key"
export RUN_PERFORMANCE_TESTS=1

# Optional
export LETTA_BASE_URL="https://api.letta.com"  # Default
export TEST_AGENT_ID="agent-01c2ef52-be32-401d-8d8f-edc561b39cbe"  # AXLE agent
```

### Test Parameters

Key configuration constants in test files:

```python
# Test iterations for statistical significance
AGENT_LIST_ITERATIONS = 10
AGENT_DETAILS_ITERATIONS = 8  
MEMORY_OPERATIONS_ITERATIONS = 6
MESSAGE_ITERATIONS = 4  # Fewer due to API cost

# Concurrent testing levels
CONCURRENT_LEVELS = [1, 5, 10, 20]

# Load testing parameters
SUSTAINED_LOAD_DURATION = 20  # seconds
REQUEST_INTERVAL = 0.3  # seconds between requests
```

## Interpreting Results

### Performance Metrics

Each test tracks comprehensive metrics:

```
Performance Summary:
  Average time: 0.324s
  Median time:  0.318s
  Min time:     0.287s
  Max time:     0.401s
  Std deviation: 0.043s
  Success rate: 100.0%
  Errors: 0
  Avg memory:   2.1 MB
```

### Improvement Calculations

Performance improvements are calculated as:
```
improvement = direct_sdk_time / mcp_server_time
```

Examples:
- 2.0x = MCP is 2x faster than Direct SDK
- 1.15x = MCP is 15% faster than Direct SDK
- 4.0x = MCP is 4x faster than Direct SDK

### Validation Status

Tests show validation status for README claims:
```
Performance Improvement: 4.23x
README Claim: 4x faster
Validation: âœ… PASSED
```

## Performance Benchmarking Methodology

### Statistical Rigor

- **Multiple Iterations**: Each test runs multiple times for statistical significance
- **Outlier Handling**: Standard deviation calculations identify performance variance
- **Error Tracking**: Failed requests are counted separately from timing
- **Memory Monitoring**: Real-time memory usage tracking with psutil

### Realistic Testing

- **Production API**: Tests use real Letta Cloud API endpoints
- **Real Agent**: Tests operate against actual AXLE agent with full capabilities
- **Concurrent Load**: Tests simulate realistic concurrent usage patterns
- **Workflow Simulation**: Tests follow actual dealer analysis workflows

### Performance Assertions

Tests include specific performance thresholds:

```python
# Response time assertions
assert avg_time < 2.0, f"Average response time too slow: {avg_time:.3f}s"
assert max_time < 5.0, f"Maximum response time too slow: {max_time:.3f}s"

# Success rate assertions  
assert success_rate >= 0.8, f"Success rate too low: {success_rate:.1%}"

# Improvement validation
assert improvement >= 3.0, f"Performance improvement insufficient: {improvement:.2f}x"
```

## Troubleshooting

### Common Issues

1. **API Key Missing**
   ```
   Error: LETTA_API_KEY environment variable is required
   ```
   **Solution**: Set your Letta API key in environment

2. **Agent Not Found**
   ```
   Error: Agent agent-01c2ef52-be32-401d-8d8f-edc561b39cbe not found
   ```
   **Solution**: Update TEST_AGENT_ID to your agent ID

3. **Rate Limiting**
   ```
   Error: 429 Too Many Requests
   ```
   **Solution**: Reduce test iterations or add delays between requests

4. **Network Timeouts**
   ```
   Error: Request timeout after 30s
   ```
   **Solution**: Check network connectivity and Letta API status

### Performance Test Debugging

Enable verbose output:
```bash
pytest tests/performance/ -v -s --tb=long
```

Run single test method:
```bash
pytest tests/performance/test_comparative_benchmarks.py::TestComparativePerformance::test_agent_listing_performance -v -s
```

## Expected Performance Characteristics

### Typical Results

Based on validation testing, expect these performance ranges:

| Operation | MCP Server Time | Direct SDK Time | Improvement |
|-----------|----------------|-----------------|-------------|
| List Agents | 0.3-0.5s | 1.2-1.8s | 3-4x faster |
| Get Agent | 0.4-0.6s | 1.0-1.5s | 2-3x faster |
| Get Memory | 0.5-0.8s | 1.5-2.5s | 3-4x faster |
| Send Message | 15-25s | 18-30s | 15-25% faster |

### Resource Usage

Expected resource characteristics:
- **Memory**: 50-150 MB steady state
- **CPU**: <20% during normal operations
- **Connections**: Efficient pooling with 10 concurrent connections
- **Throughput**: 5-10 operations/second sustained

## Contributing Performance Tests

### Adding New Performance Tests

1. **Create Test Class**: Follow existing patterns in `test_comparative_benchmarks.py`
2. **Use PerformanceMetrics**: Leverage the metrics tracking class
3. **Include Statistical Analysis**: Multiple iterations with timing statistics
4. **Add Realistic Scenarios**: Test actual usage patterns
5. **Document Claims**: Clearly state what performance claim is being validated

### Performance Test Checklist

- [ ] Tests run against real Letta API
- [ ] Multiple iterations for statistical significance
- [ ] Proper error handling and tracking
- [ ] Memory usage monitoring
- [ ] Clear performance assertions
- [ ] Documentation of expected improvements
- [ ] Integration with main test suite

---

*This performance testing framework ensures the Letta MCP Server delivers on its performance promises while maintaining production-quality reliability.*