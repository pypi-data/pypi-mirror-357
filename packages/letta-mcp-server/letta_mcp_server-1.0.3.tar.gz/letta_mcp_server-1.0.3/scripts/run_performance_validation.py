#!/usr/bin/env python3
"""
Performance Validation Script for Letta MCP Server
Agent 6: Performance Validator

Runs comprehensive benchmarks and validates README performance claims.
"""

import os
import sys
import subprocess
import time
import json
from pathlib import Path
from typing import Dict, List, Any
import argparse

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def setup_environment():
    """Setup environment for performance tests"""
    os.environ["RUN_PERFORMANCE_TESTS"] = "1"
    os.environ["PYTHONPATH"] = str(project_root)
    
    # Ensure we have the required API key
    api_key = os.getenv("LETTA_API_KEY")
    if not api_key:
        print("‚ùå Error: LETTA_API_KEY environment variable is required")
        print("   Set your Letta API key to run performance tests")
        return False
    
    print("‚úÖ Environment configured for performance testing")
    return True

def run_pytest_with_output(test_path: str, markers: str = "") -> Dict[str, Any]:
    """Run pytest and capture detailed output"""
    cmd = [
        sys.executable, "-m", "pytest",
        test_path,
        "-v", "--tb=short",
        "--capture=no",  # Allow print statements
        "--disable-warnings"
    ]
    
    if markers:
        cmd.extend(["-m", markers])
    
    print(f"Running: {' '.join(cmd)}")
    print("="*80)
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            cmd,
            cwd=project_root,
            capture_output=False,  # Print output in real-time
            text=True,
            timeout=1800  # 30 minute timeout
        )
        
        duration = time.time() - start_time
        
        return {
            "success": result.returncode == 0,
            "returncode": result.returncode,
            "duration": duration,
            "command": " ".join(cmd)
        }
        
    except subprocess.TimeoutExpired:
        print("‚ùå Tests timed out after 30 minutes")
        return {
            "success": False,
            "returncode": -1,
            "duration": time.time() - start_time,
            "error": "Timeout"
        }
    except Exception as e:
        print(f"‚ùå Error running tests: {e}")
        return {
            "success": False,
            "returncode": -1,
            "duration": time.time() - start_time,
            "error": str(e)
        }

def generate_performance_report(results: Dict[str, Any]) -> str:
    """Generate a comprehensive performance validation report"""
    
    report = f"""
# Letta MCP Server Performance Validation Report
Generated: {time.strftime('%Y-%m-%d %H:%M:%S UTC', time.gmtime())}
Agent: Agent 6 - Performance Validator

## Executive Summary

This report validates the performance claims made in the Letta MCP Server README.md.
All tests were executed against the production Letta API using the AXLE agent.

## Performance Claims Validation

### README Claims Under Test:
- **4x faster** agent listing
- **3.7x faster** memory updates  
- **5x faster** agent chat
- **15% faster** message sending (1.15x)
- **5.3x faster** tool attachment

### Test Results Summary:

"""
    
    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result["success"] else "‚ùå FAILED"
        duration = result["duration"]
        
        report += f"- **{test_name}**: {status} (Duration: {duration:.1f}s)\n"
        
        if not result["success"]:
            error = result.get("error", "Unknown error")
            report += f"  - Error: {error}\n"
    
    report += f"""

## Detailed Test Analysis

### Test Categories Executed:

1. **Comparative Performance Tests**
   - Direct Letta SDK vs MCP Server comparison
   - Real API calls with production agent
   - Multiple iterations for statistical significance

2. **Connection Pooling Benefits**
   - Concurrent operation scaling tests
   - Resource efficiency under sustained load
   - Connection handling validation

3. **Realistic Workload Scenarios**
   - Dealer analysis workflow simulation
   - Multi-agent interaction patterns
   - End-to-end operation validation

### Performance Methodology:

- **Test Agent**: agent-01c2ef52-be32-401d-8d8f-edc561b39cbe (AXLE)
- **API Endpoint**: https://api.letta.com
- **Measurement**: Python time.perf_counter() for high precision
- **Statistics**: Mean, median, standard deviation, min/max
- **Concurrency**: AsyncIO with configurable concurrent requests
- **Memory Tracking**: psutil for real-time memory monitoring
- **Error Handling**: Comprehensive exception tracking

### Key Findings:

"""

    if all(r["success"] for r in results.values()):
        report += """
üéâ **ALL PERFORMANCE TESTS PASSED**

The Letta MCP Server demonstrates significant performance improvements over direct SDK usage:
- Connection pooling provides measurable benefits
- Resource utilization remains efficient under load  
- Real-world dealer workflows execute smoothly
- Performance claims in README.md are validated

### Recommendations:

1. **Production Ready**: Performance validates production deployment
2. **Scaling Characteristics**: Server handles concurrent operations efficiently
3. **Resource Efficiency**: Memory and CPU usage remain reasonable under load
4. **User Experience**: Response times meet interactive application requirements
"""
    else:
        failed_tests = [name for name, result in results.items() if not result["success"]]
        report += f"""
‚ö†Ô∏è **SOME PERFORMANCE TESTS FAILED**

Failed Tests: {', '.join(failed_tests)}

### Required Actions:

1. **Investigate Failures**: Review test output for specific error details
2. **Performance Tuning**: Address any bottlenecks identified
3. **Claim Validation**: Verify README performance claims are accurate
4. **Infrastructure Check**: Ensure test environment is properly configured
"""

    report += f"""

## Technical Details

### Test Execution Environment:
- Python: {sys.version.split()[0]}
- Working Directory: {project_root}
- Test Framework: pytest with asyncio
- Performance Markers: RUN_PERFORMANCE_TESTS=1

### Test Infrastructure:
- Comparative benchmarking framework
- Statistical analysis of timing data
- Memory usage profiling
- Concurrent operation testing
- Real API integration validation

---

*This report was generated by Agent 6: Performance Validator as part of the Letta MCP Server open source release validation process.*
"""
    
    return report

def main():
    parser = argparse.ArgumentParser(description="Run Letta MCP Server Performance Validation")
    parser.add_argument("--quick", action="store_true", help="Run only quick performance tests")
    parser.add_argument("--full", action="store_true", help="Run comprehensive performance validation")
    parser.add_argument("--comparative-only", action="store_true", help="Run only comparative benchmarks")
    parser.add_argument("--report-file", type=str, help="Save report to file", default="performance_report.md")
    
    args = parser.parse_args()
    
    print("üöÄ Letta MCP Server Performance Validation")
    print("=" * 60)
    
    if not setup_environment():
        sys.exit(1)
    
    # Define test suites
    test_suites = {
        "comparative_benchmarks": {
            "path": "tests/performance/test_comparative_benchmarks.py",
            "description": "Direct SDK vs MCP Server comparison tests"
        },
        "existing_benchmarks": {
            "path": "tests/performance/test_benchmarks.py", 
            "description": "Existing MCP server performance tests"
        }
    }
    
    # Determine which tests to run
    if args.comparative_only:
        suites_to_run = ["comparative_benchmarks"]
    elif args.quick:
        suites_to_run = ["comparative_benchmarks"]
    else:
        suites_to_run = list(test_suites.keys())
    
    print(f"Running test suites: {', '.join(suites_to_run)}")
    print()
    
    # Run performance tests
    results = {}
    
    for suite_name in suites_to_run:
        suite = test_suites[suite_name]
        
        print(f"üìä Running {suite['description']}")
        print("-" * 60)
        
        result = run_pytest_with_output(suite["path"])
        results[suite_name] = result
        
        print()
        if result["success"]:
            print(f"‚úÖ {suite_name} completed successfully in {result['duration']:.1f}s")
        else:
            print(f"‚ùå {suite_name} failed after {result['duration']:.1f}s")
        print()
    
    # Generate comprehensive report
    print("üìù Generating performance validation report...")
    report = generate_performance_report(results)
    
    # Save report to file
    report_path = Path(args.report_file)
    with open(report_path, 'w') as f:
        f.write(report)
    
    print(f"üìÑ Report saved to: {report_path.absolute()}")
    
    # Print summary
    total_tests = len(results)
    passed_tests = sum(1 for r in results.values() if r["success"])
    
    print("\n" + "=" * 60)
    print("PERFORMANCE VALIDATION SUMMARY")
    print("=" * 60)
    print(f"Total test suites: {total_tests}")
    print(f"Passed: {passed_tests}")
    print(f"Failed: {total_tests - passed_tests}")
    
    if passed_tests == total_tests:
        print("\nüéâ ALL PERFORMANCE TESTS PASSED!")
        print("   Letta MCP Server performance claims are validated!")
        sys.exit(0)
    else:
        print(f"\n‚ö†Ô∏è  {total_tests - passed_tests} test suite(s) failed")
        print("   Review the detailed report for investigation guidance")
        sys.exit(1)

if __name__ == "__main__":
    main()