import argparse
import glob
import logging
import multiprocessing as mp
import os
import pickle
import shutil
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from chython import smiles
from doptools.chem.chem_features import ChythonCircus, ChythonLinear
from functools import partial
from multiprocessing import Manager
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.datasets import load_svmlight_file
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import (balanced_accuracy_score, cohen_kappa_score,
                             matthews_corrcoef, r2_score as r2,
                             root_mean_squared_error as rmse)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler
from sklearn.svm import SVC, SVR
from xgboost import XGBRegressor, XGBClassifier

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='model_rebuilding.log', filemode='w')

def populate_trials_dictionary(trials_folders):
    """
    Populate a dictionary with trial information from specified folders.

    Args:
        trials_folders (list): List of folders where 'trials.best' files are stored.

    Returns:
        dict: A dictionary where each key is a 'method' from 'trials.best' files,
              and the value is a dictionary containing the path to 'trials.best' and the folder path.
    """
    trials_dict = {}
    processed_folders = set()

    for folder in trials_folders:
        if folder in processed_folders:
            logging.critical(f"Duplicate folder detected: {folder}")
            sys.exit(1)  # Exit due to duplicate folder input

        processed_folders.add(folder)
        trials_file = os.path.join(folder, 'trials.best')

        if os.path.isfile(trials_file):
            df = pd.read_csv(trials_file, sep='\s+')
            if 'method' in df.columns:
                method_value = df['method'].iloc[0]
                if method_value in trials_dict:
                    logging.critical(f"Duplicate method detected: {method_value} in {trials_file}")
                    sys.exit(1)  # Exit due to duplicate method entry
                trials_dict[method_value] = {
                    'trials_file': trials_file,
                    'trials_folder': folder
                }
            else:
                logging.critical(f"'method' column not found in {trials_file}")
                sys.exit(1)
        else:
            logging.critical(f"'trials.best' not found in {folder}")
            sys.exit(1)  # Exit due to missing file

    return trials_dict


def create_output_dir(outdir):
    """
    Create an output directory if it does not already exist.

    Args:
        outdir (str): The path of the output directory to be created.

    Returns:
        None
    """
    if os.path.exists(outdir):
        logging.info('The output directory {} already exists. The data may be overwritten'.format(outdir))
    else:
        os.makedirs(outdir)
        logging.info('The output directory {} created'.format(outdir))


def select_best_CV_models(trials_info_dict, model_type, nb_classes):
    """
    Selects up to 15 best models based on the model's score in CV. Only one model is selected per descriptor space per ML method.

    Args:
        trials_info_dict (dict): A dictionary containing trial information with keys as methods and values as paths.
        model_type (str): The type of model ('class' or 'regression').
        nb_classes (int): The number of classes for classification models or 0 for regression models.

    Returns:
        pandas.DataFrame: A DataFrame containing the selected best models sorted by score in descending order.
    """
    models_by_desc = {}
    highest_score = float('-inf')

    for method, info in trials_info_dict.items():
        trials_file = info['trials_file']
        # This part is needed to fix the trials.best file generated by optuna, as instead of outputting "None", which is a valid argument value for RF method it would just ommit it completely.
        if method in ['RFC', 'RFR']:
            with open(trials_file, 'r') as file:
                lines = file.readlines()
                header_columns = len(lines[0].strip().split())
                corrected_lines = [lines[0]]

                for line in lines[1:]:
                    columns = line.strip().split()
                    if len(columns) == header_columns - 1:
                        columns.insert(6, 'None')
                    corrected_lines.append(' '.join(columns) + '\n')

            with open(trials_file, 'w') as file:
                file.writelines(corrected_lines)

        model_stats = pd.read_csv(trials_file, sep='\s+')
        highest_score = max(highest_score, model_stats['score'].max())
        # Per each descriptor space only one (the best) descriptor space is selected.
        for desc, group in model_stats.groupby('desc'):
            if desc not in models_by_desc:
                models_by_desc[desc] = []
            models_by_desc[desc].extend(group.to_dict('records'))

    threshold = 1 / nb_classes if model_type == 'class' else 0.5
    if highest_score < threshold:
        if not args.desperate:
            logging.info(f"The highest score across all the models is less than {threshold}. The script execution was stopped. However if you still want to rebuild those models rerun the script with the argument --desperate.")
            sys.exit(1)

    score_threshold = max(highest_score - 0.1, threshold)
    best_models_list = [max(models, key=lambda x: x['score']) for desc, models in models_by_desc.items() if any(m['score'] >= score_threshold for m in models)]
    best_models = pd.DataFrame(best_models_list).sort_values(by='score', ascending=False).reset_index(drop=True)
    if model_type == 'reg':
        return best_models.head(15)
    else:
        return best_models.head(10)


def create_model_folder(desc_folder, outdir, models_from_CV, input_df, test_set_df):
    """
    Create a folder containing relevant files (pickled pipelines and associated descriptor files) based on the best models and copy the training set file.

    Args:
        desc_folder (str): The path of the folder containing .pkl files.
        outdir (str): The path of the output directory where the files will be copied.
        models_from_CV (pandas.DataFrame): A DataFrame containing the "best" CV models.
        input_df (pandas.DataFrame): A DataFrame containing only the relevant rows to the model in place.
        test_set_df (pandas.DataFrame) A DataFrame containing the external test set.

    Returns:
        None
    """
    unique_descs = set(models_from_CV['desc'])
    input_df.to_csv(os.path.join(outdir, 'training_set.csv'), index=False)
    if test_set_df is not None and not test_set_df.empty:
        test_set_df.to_csv(os.path.join(outdir, 'external_test_set.csv'), index=False)
    # Iterate through the .svm files in desc_folder and copy relevant files to outdir
    for file_path in glob.glob(os.path.join(desc_folder, '*.svm')):
        file_name = os.path.basename(file_path)
        desc_name = file_name.split('.')[1]
        if desc_name in unique_descs:
            shutil.copyfile(file_path, os.path.join(outdir, file_name))


def load_pkl(pkl_file):
    """
    Load a pickled file from the given path.

    Args:
    pkl_file (str): The file path of the pickled file.

    Returns:
    The unpickled data from the file.
    """
    with open(pkl_file, 'rb') as file:
        loaded_pkl = pickle.load(file)
    # workaround. Currently when running preparer with full_config it sets up the fmt='smiles' which is non desirable in my code.
    if hasattr(loaded_pkl, 'fmt') and loaded_pkl.fmt == 'smiles':
        loaded_pkl.fmt = 'mol'
    return loaded_pkl


def rebuild_and_evaluate_reg_model(model_row_tuple, shared_data, outdir, desc_folder, property_col, model_type, predict_df):
    """
    Rebuilds a regression model from specified parameters and evaluates it using the provided prediction dataset.
    This function serves as a workaround to overcome the problem when a regression model gets decent score during CV.
    However in reality (even on fit) the model is bad.

    Args:
        model_row_tuple (tuple): A tuple containing the index and the pandas Series with model configuration.
        shared_data (list): Shared list containing the training dataset as dictionaries.
        outdir (str): Output directory path where results or models might be saved (not used in the current implementation).
        desc_folder (str): Directory path where descriptor .pkl files are stored.
        property_col (str): Name of the column in the dataframe that contains the target property values.
        model_type (str): Type of the model ('reg' for regression, 'class' for classification).
        predict_df (pandas.DataFrame): DataFrame containing the prediction set with molecules and properties.

    Returns:
        float: The R2 score of the model evaluated on the prediction dataset.
    """
    _, model_row = model_row_tuple
    input_df = pd.DataFrame(list(shared_data))

    # Extract X (molecule objects) and y (property) from the input data
    X_train = input_df['Molecule']
    y_train = input_df[property_col]

    # Load the .pkl file corresponding to the descriptor name
    pkl_file_path = os.path.join(desc_folder, f"{property_col}.{model_row['desc']}.pkl")
    descriptor_object = load_pkl(pkl_file_path)

    # Determine the ML method and create the model
    if model_row['method'] == 'RFR':
        model = RandomForestRegressor(
            max_depth=int(model_row['max_depth']),
            max_features=model_row['max_features'] if not pd.isna(model_row['max_features']) else None,
            max_samples=model_row['max_samples'],
            n_estimators=int(model_row['n_estimators'])
        )
    elif model_row['method'] == 'SVR':
        model = SVR(
            C=model_row['C'],
            kernel=model_row['kernel'],
            coef0=model_row['coef0']
        )
    elif model_row['method'] == 'XGBR':
        model = XGBRegressor(
            max_depth=int(model_row['max_depth']),
            eta=model_row['eta'],
            gamma=model_row['gamma'],
            reg_alpha=int(model_row['reg_alpha']),
            reg_lambda=model_row['reg_lambda'],
            colsample_bytree=model_row['colsample_bytree'],
            min_child_weight=int(model_row['min_child_weight']),
            n_estimators=int(model_row['n_estimators']),
            subsample=model_row['subsample'],
            sampling_method=str(model_row['sampling_method']),
            booster=str(model_row['booster']),
            tree_method=str(model_row['tree_method'])
        )

    # Build the pipeline
    steps = [('Fragmentor', descriptor_object)]
    if model_row.get('scaling', '') == 'scaled':
        steps.append(('Scaler', MinMaxScaler()))
    steps.append(('Variance', VarianceThreshold()))
    steps.append(('Model', model))
    pipeline = Pipeline(steps)
    pipeline.fit(X_train, y_train)

    # Apply the model on either test or training set
    X_test = predict_df['Molecule']
    y_test = predict_df[property_col]
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    score = r2(y_test, y_pred)
    return score


def rebuild_model(model_row_tuple, shared_data, outdir, desc_folder, property_col, model_type):
    """
    Rebuild a machine learning model based on the provided model information and input data.

    Args:
        model_row_tuple (tuple): A tuple containing the index and model information from the best models DataFrame.
        shared_data (list): Shared list containing the training dataset as dictionaries.
        outdir (str): Output directory path where results or models might be saved (not used in the current implementation).
        desc_folder (str): Directory path where descriptor .pkl files are stored.
        property_col (str): Name of the column in the dataframe that contains the target property values.
        model_type (str): Type of the model ('reg' for regression, 'class' for classification).

    Returns:
        None
    """
    _, model_row = model_row_tuple # iterrows (from the main block) generates indexes, I don't need them
    input_df = pd.DataFrame(list(shared_data))

    # Extract X (molecule objects) and y (property) from the input data
    X = input_df['Molecule']
    y = input_df[property_col]

    # Load the .pkl file corresponding to the descriptor name
    pkl_file_path = os.path.join(desc_folder, f"{property_col}.{model_row['desc']}.pkl")
    descriptor_object = load_pkl(pkl_file_path)

    # Determine the ML method and create the model
    if model_type == 'reg':
        if model_row['method'] == 'RFR':
            model = RandomForestRegressor(
                max_depth=int(model_row['max_depth']),
                max_features = model_row['max_features'] if not pd.isna(model_row['max_features']) else None,
                max_samples=model_row['max_samples'],
                n_estimators=int(model_row['n_estimators'])
            )
        elif model_row['method'] == 'SVR':
            model = SVR(
                C=model_row['C'],
                kernel=model_row['kernel'],
                coef0=model_row['coef0']
            )
        elif model_row['method'] == 'XGBR':
            model = XGBRegressor(
                max_depth=int(model_row['max_depth']),
                eta=model_row['eta'],
                gamma=model_row['gamma'],
                reg_alpha=int(model_row['reg_alpha']),
                reg_lambda=model_row['reg_lambda'],
                colsample_bytree=model_row['colsample_bytree'],
                min_child_weight=int(model_row['min_child_weight']),
                n_estimators=int(model_row['n_estimators']),
                subsample=model_row['subsample'],
                sampling_method=str(model_row['sampling_method']),
                booster=str(model_row['booster']),
                tree_method=str(model_row['tree_method'])
            )
    else:  # Classification model
        if model_row['method'] == 'RFC':
            model = RandomForestClassifier(
                max_depth=int(model_row['max_depth']),
                max_features = model_row['max_features'] if not pd.isna(model_row['max_features']) else None,
                max_samples=model_row['max_samples'],
                n_estimators=int(model_row['n_estimators'])
            )
        elif model_row['method'] == 'SVC':
            model = SVC(
                C=model_row['C'],
                kernel=model_row['kernel'],
                coef0=model_row['coef0']
            )
        elif model_row['method'] == 'XGBC':
            model = XGBClassifier(
                max_depth=int(model_row['max_depth']),
                eta=model_row['eta'],
                gamma=model_row['gamma'],
                reg_alpha=int(model_row['reg_alpha']),
                reg_lambda=model_row['reg_lambda'],
                colsample_bytree=model_row['colsample_bytree'],
                min_child_weight=int(model_row['min_child_weight']),
                n_estimators=int(model_row['n_estimators']),
                subsample=model_row['subsample'],
                sampling_method=str(model_row['sampling_method']),
                booster=str(model_row['booster']),
                tree_method=str(model_row['tree_method'])
            )
    steps = [('Fragmentor', descriptor_object)]
    if model_row['scaling'] == 'scaled':
        steps.append(('Scaler', MinMaxScaler()))
    steps.append(('Variance', VarianceThreshold()))
    # Train the model on the input data
    steps.append(('Model', model))
    # model.fit(X_scaled, y)
    my_pipeline = Pipeline(steps)
    my_pipeline.fit(X, y)

    # Save the model
    model_filename = f"Model_{model_row.name}_{model_row['method']}_{model_row['desc']}_{model_row['scaling']}.pkl"
    model_filepath = os.path.join(outdir, model_filename)
    with open(model_filepath, 'wb') as model_file:
        pickle.dump(my_pipeline, model_file)
    logging.info(f"{model_filename} saved.")


def aggregate_CV_predictions(trials_info_dict, best_models, model_type):
    """
    Aggregate predictions from various models and create a summary DataFrame.

    Args:
        trials_info_dict (dict): Dictionary containing trial information with keys as methods and values as paths.
        best_models (pandas.DataFrame): A DataFrame containing the selected best models.
        model_type (str): The type of model ('reg' for regression or 'class' for classification).

    Returns:
        pandas.DataFrame: A summary DataFrame containing aggregated predictions.
    """
    aggregated_predictions_list = []
    actual_values = None

    # Iterate over each model in the best_models DataFrame.
    for _, model_row in best_models.iterrows():
        method = model_row['method']
        if method in trials_info_dict:
            trial_str = str(model_row['trial'])
            folder_path = trials_info_dict[method]['trials_folder']
            trial_folder = f"trial.{trial_str}"
            file_path = os.path.join(folder_path, trial_folder, "predictions")

            if os.path.isfile(file_path):
                # Read the predictions for the current model.
                trial_predictions = pd.read_csv(file_path, sep='\s+')
                # Extract the actual values and predicted values based on column headers
                if actual_values is None:
                    actual_values = trial_predictions.filter(like='.observed').iloc[:, 0]
                predicted_columns = [col for col in trial_predictions.columns if '.predicted.' in col]

                # Aggregate across rows (compounds)
                if model_type == 'reg':
                    # Calculate the mean for each row across predicted columns
                    aggregated_trial_prediction = trial_predictions[predicted_columns].mean(axis=1).rename(f"{trial_str}")
                else:
                    # Calculate the mode for each row across predicted columns and convert to integer
                    mode_result = trial_predictions[predicted_columns].mode(axis=1)[0].astype(int)
                    aggregated_trial_prediction = mode_result.rename(f"{trial_str}")

                aggregated_predictions_list.append(aggregated_trial_prediction)

    # Concatenate all aggregated trial predictions into a single DataFrame
    all_aggregated_predictions = pd.concat(aggregated_predictions_list, axis=1)

    if model_type == 'reg':
        # Calculate the mean and standard deviation across all trials
        final_mean = all_aggregated_predictions.mean(axis=1)
        final_std = all_aggregated_predictions.std(axis=1)
        final_df = pd.DataFrame({property_col: actual_values, 'AvgPred': final_mean, 'StdDev': final_std})
    else:
        # Calculate the mode and mode count across all trials
        mode_values = all_aggregated_predictions.mode(axis=1).iloc[:, 0].astype(int)
        class_probs = all_aggregated_predictions.apply(lambda row: (row.value_counts() / len(row)).round(2).to_dict(), axis=1)
        final_df = pd.DataFrame({property_col: actual_values, 'Mode': mode_values, 'ClassProb': class_probs})

    return final_df


def evaluate_AD_apply_model(desc_file, shared_molecules):
    """
    Evaluate the applicability domain of the compounds and apply the model in the given descriptor space

    Args:
    train_fragmentor (Fragmentor Object): The pkl file path to the given descriptor space.
    shared_pred_molecules (list): A list of dictionaries, where each dictionary contains the 'SMILES' string
                             and the corresponding 'Molecule' object (chython.containers.molecule.MoleculeContainer).
    desc_file (str) : File path to the training set descriptors in svm format.
    pred_descs (np.array): Numpy array containing the descriptors for all the predicting molecules
    model_path (str): File path to model pipeline pkl.

    Returns:
    pd.DataFrame: A DataFrame containing the AD assessments (column name: [f'Conf-{desc_space}'])
                    Prediction if within AD, else None (column name: [model_name])
                    as well as SMILES.

    """
    def frag_ctrl(p_DF, train_fragments, desc_space):
        """
        Update the confidence level column based on the fragment control check.

        Args:
        p_DF (pd.DataFrame): The DataFrame containing molecules and confidence levels.
        train_fragments (set): The set of fragment features present in the training data.
        desc_space (str): The name of the descriptor space.

        Returns:
        pd.DataFrame: The updated DataFrame with confidence levels.
        """
        def conf_update(row):
            """
            Update the confidence level for a single row based on the fragment control check.

            Args:
            row (pd.Series): A row from the input DataFrame.

            Returns:
            pd.Series: The updated row with the confidence level updated.
            """
            if isinstance(model_pipeline[0], ChythonLinear):
                # Create a ChythonLinear test fragmentor
                test_fragmentor = ChythonLinear(lower=model_pipeline[0].lower, upper=model_pipeline[0].upper)
            elif isinstance(model_pipeline[0], ChythonCircus):
                # Create a ChythonCircus test fragmentor
                test_fragmentor = ChythonCircus(lower=model_pipeline[0].lower, upper=model_pipeline[0].upper)
            else:
                logging.error("Unsupported fragmentor type while using fragment control.")
                raise ValueError("Unsupported fragmentor type while using fragment control")
            test_features = set(test_fragmentor.fit([row['Molecule']]).get_feature_names())
            if test_features <= train_fragments:
                row[f'Conf-{desc_space}'] = 1
            return row
        return p_DF.apply(conf_update, axis=1)

    def bbox(p_DF, max_train_descs, p_descs, desc_space):
            """
            Update the confidence level column based on the bounding box check.

            Args:
            p_DF (pd.DataFrame): The DataFrame containing molecules and confidence levels.
            max_train_descs (np.ndarray): The maximum values of descriptors in the training data.
            p_descs (np.ndarray): The descriptors for the prediction molecules.
            model_name (str): The name of the model.

            Returns:
            pd.DataFrame: The updated DataFrame with confidence levels.
            """
            # Check if the sizes match. Rarely the svm file would miss last couple of descriptors, simply because they're zeros!
            pad_size = p_descs.shape[1] - max_train_descs.shape[0]
            if pad_size > 0:
                # Pad max_val_descs with zeros at the end
                max_train_descs = np.pad(max_train_descs, (0, pad_size), 'constant', constant_values=(0, 0))
            # Check if each descriptor is <= max_train_descs
            descriptor_masks = p_descs <= max_train_descs
            # Combine the masks along the descriptor axis
            passes_check = np.all(descriptor_masks, axis=1)
            # Update the columns based on the check results
            p_DF.loc[passes_check, f'Conf-{desc_space}'] = 1
            return p_DF

    # Convert the list of MoleculeContainer objects to a DataFrame
    shared_predict_df = pd.DataFrame(list(shared_molecules))
    desc_space = os.path.basename(desc_file).split('.')[1]  # Extract the descriptor space name

    model_files = glob.glob(f"{model_folder}/Model_*_*_{desc_space}_*.pkl") # Get all the files (it should be only 1) matching the criterium in a list
    if len(model_files) == 1:
        model_path = model_files[0]
    elif len(model_files) == 0:
        logging.error(f"No model found for the descriptor space '{desc_space}'. Please check the pkl_folder and descriptor space.")
        raise ValueError(f"No model found for the descriptor space '{desc_space}'. Please check the pkl_folder and descriptor space.")
    else:  # len(model_files) > 1
        logging.error(f"Multiple models found for the descriptor space '{desc_space}'. Please ensure there is only one model per descriptor space.")
        raise ValueError(f"Multiple models found for the descriptor space '{desc_space}'. Please ensure there is only one model per descriptor space.")
    model_name = os.path.splitext(os.path.basename(model_path))[0] # Extract the file name without extension
    print(model_name)
    model_pipeline = load_pkl(model_path)

    # Initialize column 'Conf' + desc_space populated with zeros
    shared_predict_df[f'Conf-{desc_space}'] = 0
    # Initialize the column with None for all compounds
    shared_predict_df[model_name] = None
    if not (isinstance(model_pipeline[0], ChythonLinear) or isinstance(model_pipeline[0], ChythonCircus)):
        # Compute the descriptors for the predicting compounds
        p_descs_pipeline = model_pipeline[0].transform(shared_predict_df['Molecule'])
        pred_descs = np.array(p_descs_pipeline) # convert the descriptors into np.array for bounding box assessment
        # Load the SVM file into a sparse matrix
        desc_svm, _ = load_svmlight_file(desc_file)
        # Convert the sparse matrix to a dense NumPy array
        train_descs = desc_svm.toarray()
        max_val_descs = np.max(train_descs, axis = 0)
        shared_predict_df = bbox(shared_predict_df, max_val_descs, pred_descs, desc_space)
        del train_descs, desc_svm, pred_descs, max_val_descs
        # Apply the model
        shared_predict_df[model_name] = Pipeline(model_pipeline.steps[1:]).predict(p_descs_pipeline)
    else:
        # Check the AD
        training_features = set(model_pipeline[0].get_feature_names())
        shared_predict_df = frag_ctrl(shared_predict_df, training_features, desc_space)
        # Apply model
        shared_predict_df[model_name] = model_pipeline.predict(shared_predict_df['Molecule'])
    shared_predict_df = shared_predict_df.drop('Molecule', axis = 1)
    return shared_predict_df


def aggregate_test_predictions (all_predictions, ext_test_set_DF, model_type):
    """
    Aggregates prediction data for a given external test set DataFrame and calculates
    confidence levels and statistical summaries based on the model type.

    This function processes an external test set DataFrame by appending predictions
    from a dictionary, calculating aggregated confidence values, and then dividing the
    DataFrame into two groups based on confidence. Additional statistical summaries
    are calculated for each group depending on the specified model type.

    Parameters:
    - all_predictions (dict): A dictionary where keys are SMILES strings and values
      are predictions associated with those SMILES.
    - ext_test_set_DF (pd.DataFrame): A DataFrame containing the external test set
      data. This DataFrame must include a 'SMILES' column and a 'Molecule' column
      that will be dropped within the function.
    - model_type (str): Specifies the type of model used for predictions. Accepts
      'reg' for regression models, which will compute mean and standard deviation,
      or any other string (typically 'class') for classification models, which will
      compute mode and class probabilities.

    Returns:
    - dict: A dictionary containing two DataFrames under the keys 'In_AD' and 'Out_AD':
        - 'In_AD': DataFrame containing data points where the confidence is not zero.
        - 'Out_AD': DataFrame containing data points where the confidence is zero.
          Each DataFrame includes additional columns for statistical summaries specific
          to the provided model type.
    """

    def in_AD_aggregation (df_row):
        """
        Aggregates predictions for molecules within the applicability domain.

        Args:
            df_row (pd.Series): A row from a DataFrame representing a molecule's data including model predictions and confidences.

        Returns:
            pd.Series: A Series containing aggregated prediction results. For regression models, it includes the average prediction
                       and standard deviation. For classification models, it returns the mode of predictions and class probabilities.
        """
        valid_models = [] # models for which a given molecule is within the applicability domain
        for conf_col in conf_columns: # conf_columns is declared in the parent function
            descriptor_space = conf_col.split("Conf-")[1]
            for model_col in model_cols: # model_cols is declared in the parent function
                if descriptor_space in model_col:
                    if df_row[conf_col] == 1:
                        valid_models.append(model_col)
        if model_type == 'reg':
            avg_pred = df_row[valid_models].mean()
            std_dev = df_row[valid_models].std()
            return pd.Series({'AvgPred': avg_pred, 'StdDev': std_dev})
        else:
            mode = df_row[valid_models].mode().iloc[0]
            class_probs = (df_row[valid_models].value_counts() / len(df_row[valid_models])).round(2).to_dict()
            return pd.Series({'Mode': int(mode), 'ClassProbs': class_probs})

    ext_test_set_DF.drop('Molecule', axis = 1, inplace=True)
    # Build a DataFrame directly from all_predictions for existing SMILES in ext_test_set_DF
    new_df = pd.DataFrame({smiles: all_predictions[smiles] for smiles in ext_test_set_DF['SMILES']}).T
    # Identify columns in new_df that are not in ext_test_set_DF
    new_columns = new_df.columns.difference(ext_test_set_DF.columns)
    # Filter new_df to include only the new columns
    new_df_filtered = new_df[new_columns]
    # Join the DataFrames on the index, which should be set to SMILES for both DataFrames
    # Ensure indices are set if not already
    if not ext_test_set_DF.index.equals(new_df_filtered.index):
        ext_test_set_DF = ext_test_set_DF.set_index('SMILES')
        new_df_filtered = new_df_filtered.reindex(ext_test_set_DF.index)
    # Merge the DataFrames
    ext_test_set_DF = ext_test_set_DF.join(new_df_filtered)

    # Resetting the index and making 'SMILES' a column again.
    ext_test_set_DF.reset_index(inplace=True)
    ext_test_set_DF.rename(columns={'index': 'SMILES'}, inplace=True)

    # Count the number of 'Conf-' columns and prepare for Confidence calculation
    conf_model_count = sum("Conf-" in col for col in ext_test_set_DF.columns)
    model_cols = [col for col in ext_test_set_DF.columns if col.startswith("Model_")]
    conf_columns = [col for col in ext_test_set_DF.columns if "Conf-" in col]
    all_columns_to_drop = model_cols + conf_columns

    # Calculate the aggregated confidence values
    in_AD_count = ext_test_set_DF.filter(regex="Conf-").sum(axis=1)
    ext_test_set_DF['Confidence'] = in_AD_count.astype(str) + f"/{conf_model_count}"

    # Splitting the DataFrame based on 'Confidence'
    zero_confidence_value = f"0/{conf_model_count}"
    DF_in_AD = ext_test_set_DF[ext_test_set_DF['Confidence'] != zero_confidence_value].copy()
    DF_out_AD = ext_test_set_DF[ext_test_set_DF['Confidence'] == zero_confidence_value].copy()

    if model_type == 'reg':
        if not DF_in_AD.empty: # calculate the mean and std. dev. for the compounds that are inside the AD
            in_AD_results = DF_in_AD.apply(in_AD_aggregation, axis=1)
            DF_in_AD['AvgPred'] = in_AD_results['AvgPred']
            DF_in_AD['StdDev'] = in_AD_results['StdDev']
            DF_in_AD.drop(columns=all_columns_to_drop, inplace=True)
        if not DF_out_AD.empty: # outside the AD
            DF_out_AD['AvgPred'] = DF_out_AD[model_cols].mean(axis=1)
            DF_out_AD['StdDev'] = DF_out_AD[model_cols].std(axis=1)
            DF_out_AD.drop(columns=all_columns_to_drop, inplace=True)
    else:
        if not DF_in_AD.empty: # inside the AD calculations
            in_AD_results = DF_in_AD.apply(in_AD_aggregation, axis=1)
            DF_in_AD['Mode'] = in_AD_results['Mode']
            DF_in_AD['ClassProbs'] = in_AD_results['ClassProbs']
            DF_in_AD.drop(columns=all_columns_to_drop, inplace=True)
        if not DF_out_AD.empty: # outside the AD calculations
            DF_out_AD['Mode'] = DF_out_AD[model_cols].mode(axis=1).iloc[:, 0].astype(int)
            DF_out_AD['ClassProbs'] = DF_out_AD[model_cols].apply(lambda row: (row.value_counts() / len(row)).round(2).to_dict(), axis=1)
            DF_out_AD.drop(columns=all_columns_to_drop, inplace=True)

    return {'In_AD': DF_in_AD, 'Out_AD': DF_out_AD}


def calculate_scores(final_df, property_col, model_type):
    """
    Calculate evaluation scores based on the true and predicted values.

    Args:
        final_df (pandas.DataFrame): DataFrame containing true and predicted values.
        property_col (str): The name of the column containing true values.
        model_type (str): The type of model ('reg' for regression or 'class' for classification).

    Returns:
        dict: A dictionary containing the calculated evaluation scores.
    """
    scores = {}
    y_true = final_df[property_col]
    if model_type == 'class':
        y_pred = final_df['Mode']
        scores['Balanced_Accuracy'] = np.round(balanced_accuracy_score(y_true, y_pred), 2)
        scores['Cohen_Kappa'] = np.round(cohen_kappa_score(y_true, y_pred), 2)
        scores['Matthews_CorrCoef'] = np.round(matthews_corrcoef(y_true, y_pred), 2)
    elif model_type == 'reg':
        y_pred = final_df['AvgPred']
        scores['R2_score'] = np.round(r2(y_true, y_pred), 2)
        scores['RMSE'] = np.round(rmse(y_true, y_pred), 2)
    return scores


def plot_regression(dataframe, property_col, scores, outdir, test_set_df):
    """
    Create a regression plot based on the true and predicted values and save it to the specified output directory.

    Args:
        dataframe (pandas.DataFrame): DataFrame containing true and predicted values.
        property_col (str): The name of the column containing true values.
        scores (dict): A dictionary containing evaluation scores, including 'RMSE' and 'R2_score'.
        outdir (str): The output directory to save the regression plot.
        test_set_df (pandas.DataFrame) : DataFrame containing external test set.

    Returns:
        None
    """

    y_true = dataframe[property_col]
    y_pred = dataframe['AvgPred']
    summary_stats = y_true.describe().round(2)
    model_scores_rounded = {key: round(value, 2) for key, value in scores.items()}

    if test_set_df is not None and not test_set_df.empty:
        first_line = "External test set statistics:"
    else:
        first_line = "Training set statistics:"
    with open(os.path.join(outdir, "stats.txt"), 'w') as file:
        file.write(f"{first_line}\n")
        file.write(summary_stats.to_string())
        file.write("\n\nModel Scores:\n")
        file.write('\n'.join(f'{key}: {value}' for key, value in model_scores_rounded.items()))

    # Calculate x-axis and y-axis ranges
    xmin = min(y_true)
    xmax = max(y_true)
    ymin = min(y_pred)
    ymax = max(y_pred)

    # Adjust the ranges based on the comparison
    if xmin > ymin:
        ymin -= 0.25
        xmin = ymin
    else:
        xmin -= 0.25
        ymin = xmin

    if xmax < ymax:
        ymax += 0.25
        xmax = ymax
    else:
        xmax += 0.25
        ymax = xmax

    # Actual plotting
    plt.scatter(y_true, y_pred, color='darkgreen')
    plt.plot([xmin, xmax], [ymin, ymax], color='black', linestyle='-', linewidth=1)
    rmse = scores['RMSE']
    r2_score = scores['R2_score']
    plt.plot([xmin, xmax], [xmin + 3*0.6, xmax + 3*0.6], color='red', linestyle='-', linewidth=1)
    plt.plot([xmin, xmax], [xmin - 3*0.6, xmax - 3*0.6], color='red', linestyle='-', linewidth=1)
    plt.xlabel('Actual', fontsize=16)
    plt.ylabel('Predicted', fontsize=16)
    if test_set_df is not None and not test_set_df.empty:
        plt.title(f"R² = {r2_score:.2f}; RMSE = {rmse:.2f}")
    else:
        plt.title(f"Q² = {r2_score:.2f}; RMSE = {rmse:.2f}")
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    # Set x-axis and y-axis ranges
    plt.xlim(xmin, xmax)
    plt.ylim(ymin, ymax)

    plt.savefig(os.path.join(outdir, 'regression_plot.png'))


def generate_confusion_matrix(dataframe, scores, outdir, nb_classes, class_info, test_set_df):
    """
    Generate a confusion matrix and write it along with scores to a file in the specified output directory.

    Args:
        dataframe (pandas.DataFrame): DataFrame containing true and predicted values.
        scores (dict): A dictionary containing evaluation scores, including 'Balanced_Accuracy', 'Cohen_Kappa', and 'Matthews_CorrCoef'.
        outdir (str): The output directory to save the confusion matrix and scores file.
        nb_classes (int): The number of classes.
        class_info (str): File path to the text file containing the information about classes.
        test_set_df (pandas.DataFrame) : DataFrame containing external test set.

    Returns:
        None
    """

    y_true = dataframe[property_col]
    y_pred = dataframe['Mode']
    # Extract unique class labels from y_true
    class_labels = np.unique(y_true)
    if len(class_labels) > nb_classes:
        class_labels = class_labels[:nb_classes]  # Use only the first nb_classes unique labels
    elif len(class_labels) < nb_classes:
        logging.error("The number of unique class labels is less than nb_classes.")
        raise ValueError("The number of unique class labels is less than nb_classes.")

    # Initialize the confusion matrix with zeros
    confusion_matrix = np.zeros((nb_classes, nb_classes), dtype=int)

    # Populate the confusion matrix
    for true_class, pred_class in zip(y_true, y_pred):
        if true_class in class_labels and pred_class in class_labels:
            true_idx = np.where(class_labels == true_class)[0][0]
            pred_idx = np.where(class_labels == pred_class)[0][0]
            confusion_matrix[true_idx, pred_idx] += 1

    # Create the DataFrame with explicit labels
    row_labels = [f'Actual Class {label}' for label in class_labels]
    col_labels = [f'Predicted Class {label}' for label in class_labels]
    df_cm = pd.DataFrame(confusion_matrix, index=row_labels, columns=col_labels)
    # copy class_info into model folder
    shutil.copyfile(class_info, os.path.join(outdir, "class_info.txt"))

    if test_set_df is not None and not test_set_df.empty:
        first_line = "Confusion matrix and statistics for the external test set:"
    else:
        first_line = "Confusion matrix and statistics for the training set:"

    # Write the confusion matrix and scores to a file
    with open(os.path.join(outdir, 'stats.txt'), 'w') as f:
        f.write(f"{first_line}\n")
        f.write(df_cm.to_string())
        f.write("\n")
        f.write(f"Balanced Accuracy: {scores['Balanced_Accuracy']:.2f}\n")
        f.write(f"Cohen's Kappa: {scores['Cohen_Kappa']:.2f}\n")
        f.write(f"Matthews Correlation Coefficient: {scores['Matthews_CorrCoef']:.2f}\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog='Model builder',
        description='Reads the trials.best generated by Optuna Optimiser and prepares an ensemble model if possible.'
    )

    parser.add_argument('-i', '--input', required=True, type=str,
                        help='Input file containing training data, requires csv format.')

    parser.add_argument('-t', '--test_set', type=str,
                        help='Input file containing external test set data, requires csv format.')

    parser.add_argument('--structure_col', default='SMILES', type=str,
                        help='Column name containing the SMILES in the input file. Default = SMILES.')

    parser.add_argument('--property_col', required=True, type=str,
                        help='Column name containing the property in the input file.')
    
    parser.add_argument('--custom_descriptor_col', type=str, nargs='+',
                    help='Column name(s) containing the descriptor(s) that need to be added.')

    parser.add_argument('--model_type', required=True, type=str, default='reg', choices=['reg', 'class'],
                        help='Specify if the model is classification or regression.')

    parser.add_argument('--class_info', type=str,
                        help='Path to the file containing class definitions generated by data_curation.py script.')

    parser.add_argument('--trials_folder', nargs='+', type=str, required=True,
                        help='Folder(s) where trials.best for ML methods are stored. Accepts multiple folders.')

    parser.add_argument('--desc_folder', type=str, required=True,
                        help='Folder where the .pkl descriptors are stored.')

    parser.add_argument('-p', '--parallel', type=int, default=15,
                        help='Number of parallel processes to run. Default = 15.')

    parser.add_argument('-o', '--output', required=True,
                        help='Folder name where the model will be saved.')

    parser.add_argument('--desperate', action='store_true',
                        help='Argument to be specified if none of the individual models were good enough, but you still want to get something.')

    args = parser.parse_args()
    # Process file paths and declare variables
    input_file = args.input
    ext_test_file = args.test_set
    custom_descriptors = args.custom_descriptor_col if args.custom_descriptor_col is not None else None
    trials_folders = [folder.rstrip('/') for folder in args.trials_folder]
    model_folder = args.output.rstrip('/')
    model_type = args.model_type
    class_info = args.class_info
    desc_folder = args.desc_folder
    structure_col = args.structure_col
    property_col = args.property_col

    # Maybe not the most elegant solution, but it does what it needs to do. Maybe will refactor one day
    final_DF = None
    final_DF_out_AD = None

    # Validate model type and number of classes
    if model_type == 'class':
        if args.class_info is None:
            logging.error("--class_info must be specified when --model_type is 'class'.")
            parser.error("--class_info must be specified when --model_type is 'class'.")

        # Read the file to count the number of lines, subtracting 1 to exclude the header
        with open(class_info, 'r') as file:
            nb_classes = sum(1 for _ in file) - 1
    else:
        nb_classes = None

    # Populate a dictionary with trials information from specified folders.
    trials_info_dict = populate_trials_dictionary(trials_folders)
    # Load the input file
    input_df = pd.read_csv(input_file).dropna(subset=[property_col]).reset_index(drop=True)
    test_set_df = pd.read_csv(ext_test_file).dropna(subset=[property_col]).reset_index(drop=True) if ext_test_file else None
    if custom_descriptors is not None:
        # Check for the presence of custom_descriptors in input_df
        missing_columns_input = [col for col in custom_descriptors if col not in input_df.columns]
        if missing_columns_input:
            logging.error(f"Missing columns in the input dataset: {missing_columns_input}")
            sys.exit(1)
        # Check for the presence of custom_descriptors in test_set_df if it exists and is not empty
        if test_set_df is not None and not test_set_df.empty:
            missing_columns_test = [col for col in custom_descriptors if col not in test_set_df.columns]
            if missing_columns_test:
                logging.error(f"Missing columns in the external test set: {missing_columns_test}")
                sys.exit(1)

    # Check for the presence of 'CSN' and 'ID' columns and raise an error if both are present
    uppercase_columns = input_df.columns.str.upper()
    if 'CSN' in uppercase_columns and 'ID' in uppercase_columns:
        logging.error("CSV file contains both 'CSN' and 'ID' columns. Please use only one as the molecule ID.")
        raise ValueError("CSV file contains both 'CSN' and 'ID' columns. Please use only one as the molecule ID.")
    else:
        # Find the actual column name with the correct case
        if 'CSN' in uppercase_columns:
            mol_id = input_df.columns[uppercase_columns == 'CSN'][0]  # Get the actual column name that matches 'CSN'
        elif 'ID' in uppercase_columns:
            mol_id = input_df.columns[uppercase_columns == 'ID'][0]  # Get the actual column name that matches 'ID'
    columns_to_keep = [structure_col, mol_id, property_col]
    if custom_descriptors is not None:
        columns_to_keep += custom_descriptors

    input_df = input_df.drop(columns=[col for col in input_df.columns if col not in columns_to_keep])

    create_output_dir(model_folder)
    models_from_CV = select_best_CV_models(trials_info_dict, model_type, nb_classes)
    create_model_folder(desc_folder, model_folder, models_from_CV, input_df, test_set_df)
    input_df['Molecule'] = input_df[structure_col].apply(smiles) # smiles comes from chython, transforms SMILES into MolObject
    if test_set_df is not None and not test_set_df.empty:
        test_set_df = test_set_df.drop(columns=[col for col in test_set_df.columns if col not in columns_to_keep])
        test_set_df['Molecule'] = test_set_df[structure_col].apply(smiles)
    # Manager list to share data across processes
    with Manager() as manager:
        shared_data = manager.list(input_df.to_dict('records'))
        # Use multiprocessing to rebuild and evaluate models
        with mp.Pool(processes=args.parallel if args.parallel > 0 else 1) as pool:
            first_func_args = [(_, row) for _, row in models_from_CV.iterrows()]
            kwargs = {
                'shared_data': shared_data,
                'outdir': model_folder,
                'desc_folder': desc_folder,
                'property_col': property_col,
                'model_type' : model_type,
                'predict_df': test_set_df if test_set_df is not None and not test_set_df.empty else input_df
            }
            if model_type == 'reg':
                # partial() method is used togerther with map(). map() accepts only functions with 1 argument, so partial() is very handy,
                # because in essence it fixes all the arguments (kwargs) except one (first_func_args)
                partial_rebuild_and_evaluatefunc = partial(rebuild_and_evaluate_reg_model, **kwargs)
                results = pool.map(partial_rebuild_and_evaluatefunc, first_func_args)
                # Assign the results back to the model_from_CV. It is safe to do that, because when using pool.map() the order of the results is preserved relative to the order of the inputs.
                models_from_CV['evaluation_score'] = results
                indices_to_drop = models_from_CV[models_from_CV['evaluation_score'] < 0.5].index # Do you really want to live in a world were models with such score are getting accepted?
                if not args.desperate:
                    if len(indices_to_drop) == len(models_from_CV):
                        logging.error('During models\' evaluation with the external test set all the available individual models have shown a very poor score. If you still want to continue, re-run the script with the argument --desperate')
                        sys.exit(1)
                    models_from_CV.drop(index=indices_to_drop, inplace=True)
                if test_set_df is not None and not test_set_df.empty: # If I have test set, then I remove the CV score.
                    models_from_CV.drop(columns=['score'], inplace=True)
                    models_from_CV.rename(columns={'evaluation_score': 'score'}, inplace=True)
                    models_from_CV.sort_values(by='score', ascending=False, inplace=True)
                best_models = models_from_CV.iloc[:10].reset_index(drop=True)
                rebuild_kwargs = {k: v for k, v in kwargs.items() if k != 'predict_df'} # don't need to provide predict_df for the next function.
                second_func_args = [(_, row) for _, row in best_models.iterrows()]
                partial_rebuild_model = partial(rebuild_model, **rebuild_kwargs)
                pool.map(partial_rebuild_model, second_func_args)
                # At this point the model folder contains up to 15 svm files, however we're retaining only up to 10. Unnecessary svm files are deleted with the code bellow
                needed_descriptors = set(best_models['desc'])
                files_in_directory = os.listdir(model_folder)
                svm_files = [file for file in files_in_directory if file.endswith('.svm')]
                for svm_file in svm_files:
                    descriptor = svm_file.split('.')[1]
                    if descriptor not in needed_descriptors:
                        file_path = os.path.join(model_folder, svm_file)
                        os.remove(file_path)
            else: # classification task
                best_models = models_from_CV
                rebuild_kwargs = {k: v for k, v in kwargs.items() if k != 'predict_df'} # don't need to provide predict_df for the next function.
                partial_rebuild_func = partial(rebuild_model, **rebuild_kwargs)
                pool.map(partial_rebuild_func, first_func_args)
    del(models_from_CV)
    # Apply the models on the external test set, and aggregate the predictions for all the individual models making part of the ensemble model.
    if test_set_df is not None and not test_set_df.empty:
        with Manager() as manager:
            test_set_file_data = test_set_df.apply(lambda row: row.to_dict(), axis=1).tolist()
            shared_molecules = manager.list()
            shared_molecules.extend(test_set_file_data)
            all_results = dict(zip(test_set_df['SMILES'], [{} for _ in range(len(test_set_df['SMILES']))]))
            with mp.Pool(processes=args.parallel if args.parallel > 0 else 1) as pool:
                for desc_file in glob.glob(model_folder + '/*.svm'):
                    result = pool.apply_async(evaluate_AD_apply_model, args=(desc_file, shared_molecules))
                    updated_shared_predict_df = result.get()
                    for _, row in updated_shared_predict_df.iterrows():
                        all_results[row['SMILES']].update(row.drop('SMILES').to_dict())
        results = aggregate_test_predictions(all_results, test_set_df, model_type)
        final_DF = results['In_AD']
        final_DF_out_AD = results['Out_AD']
    else:
        # Aggregate CV predictions and calculate model scores
        final_DF = aggregate_CV_predictions(trials_info_dict, best_models, model_type)
        final_DF = pd.concat([final_DF, input_df[mol_id], input_df[structure_col]], axis=1)

    # Needed for "sensible" score calculations.
    if model_type == 'reg':
        minimal_row_requirement = 2
    else:
        minimal_row_requirement = nb_classes

    # Handling compounds in AD
    if len(final_DF) >= minimal_row_requirement:
        scores = calculate_scores(final_DF, property_col, model_type)
        logging.info("The obtained predictions are saved to 'predictions.csv'")
        in_AD_output_filename = os.path.join(model_folder, "predictions.csv")
        final_DF.to_csv(in_AD_output_filename, index=False)
        if model_type == 'reg':
            plot_regression(final_DF, property_col, scores, model_folder, test_set_df)
        else:
            generate_confusion_matrix(final_DF, scores, model_folder, nb_classes, class_info, test_set_df)
    else:
        logging.info("Not enough compounds are within the model's AD to compute sensible scores.")
        in_AD_output_filename = os.path.join(model_folder, "in_AD_predictions.csv")
        final_DF.to_csv(in_AD_output_filename, index=False)

    # Handling compounds outside of AD
    if final_DF_out_AD is not None and not final_DF_out_AD.empty:
        logging.info("The predictions for compounds that are OUTSIDE of AD for the whole consensus model are saved to 'outside_of_AD_predictions.csv'")
        out_AD_output_filename = os.path.join(model_folder, "outside_of_AD_predictions.csv")
        final_DF_out_AD.to_csv(out_AD_output_filename, index=False)
