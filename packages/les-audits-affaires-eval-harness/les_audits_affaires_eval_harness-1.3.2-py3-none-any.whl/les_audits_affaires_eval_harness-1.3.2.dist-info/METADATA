Metadata-Version: 2.4
Name: les-audits-affaires-eval-harness
Version: 1.3.2
Summary: Evaluation harness for Les Audits-Affaires LLM benchmark
Home-page: https://github.com/legmlai/les-audits-affaires-eval-harness
Author: LegML Team
Author-email: LegML Team <contact@legml.ai>
License: MIT
Project-URL: Homepage, https://github.com/legmlai/les-audits-affaires-eval-harness
Project-URL: Documentation, https://les-audits-affaires-eval-harness.readthedocs.io
Project-URL: Repository, https://github.com/legmlai/les-audits-affaires-eval-harness.git
Project-URL: Issues, https://github.com/legmlai/les-audits-affaires-eval-harness/issues
Project-URL: Changelog, https://github.com/legmlai/les-audits-affaires-eval-harness/blob/main/CHANGELOG.md
Keywords: llm,evaluation,legal,french,benchmark,nlp
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiohttp>=3.9
Requires-Dist: requests>=2.31
Requires-Dist: openai>=1.18
Requires-Dist: tenacity>=8.2
Requires-Dist: python-dotenv>=1.0
Requires-Dist: datasets>=2.18
Requires-Dist: tqdm>=4.66
Requires-Dist: jsonlines>=4.0
Requires-Dist: pandas>=2.2
Requires-Dist: matplotlib>=3.8
Requires-Dist: seaborn>=0.13
Requires-Dist: openpyxl>=3.1
Requires-Dist: numpy>=1.24
Requires-Dist: scipy>=1.11
Provides-Extra: dev
Requires-Dist: pytest>=8.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23; extra == "dev"
Requires-Dist: pytest-mock>=3.12; extra == "dev"
Requires-Dist: black>=24.0; extra == "dev"
Requires-Dist: isort>=5.13; extra == "dev"
Requires-Dist: flake8>=7.0; extra == "dev"
Requires-Dist: mypy>=1.8; extra == "dev"
Requires-Dist: pre-commit>=3.6; extra == "dev"
Requires-Dist: bandit>=1.7; extra == "dev"
Requires-Dist: safety>=3.0; extra == "dev"
Requires-Dist: mkdocs>=1.5; extra == "dev"
Requires-Dist: mkdocs-material>=9.5; extra == "dev"
Requires-Dist: mkdocstrings[python]>=0.24; extra == "dev"
Requires-Dist: build>=1.0; extra == "dev"
Requires-Dist: twine>=4.0; extra == "dev"
Requires-Dist: bump2version>=1.0; extra == "dev"
Provides-Extra: visualization
Requires-Dist: plotly>=5.17; extra == "visualization"
Requires-Dist: dash>=2.16; extra == "visualization"
Requires-Dist: streamlit>=1.29; extra == "visualization"
Provides-Extra: all
Requires-Dist: les-audits-affaires-eval-harness[dev,visualization]; extra == "all"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# Les Audits-Affaires - Harness d'√âvaluation LLM

<p align="left">üá¨üáß <a href="README_EN.md">English version</a></p>
<p align="center">
  <img src="legml-ai-white.svg" alt="LegML.ai logo" width="180"/>
</p>

Un framework d'√©valuation complet pour tester les mod√®les de langage sur le benchmark juridique fran√ßais **Les Audits-Affaires**. √âvalue les mod√®les sur 5 cat√©gories de comp√©tences juridiques en utilisant le dataset `legmlai/les-audits-affaires`.

## D√©marrage Rapide

### Installation
```bash
pip install -e .
```

### Utilisation de Base
```bash
# √âvaluer OpenAI GPT-4o avec √©valuateur Azure OpenAI
export EXTERNAL_PROVIDER=openai
export EXTERNAL_MODEL=gpt-4o
export OPENAI_API_KEY=votre_cle_openai
export AZURE_OPENAI_API_KEY=votre_cle_azure
export AZURE_OPENAI_ENDPOINT=votre_endpoint_azure

lae-eval run --max-samples 10
```

## Configuration

### Mod√®le √† √âvaluer

**Fournisseurs Externes :**
```bash
# OpenAI
export EXTERNAL_PROVIDER=openai
export EXTERNAL_MODEL=gpt-4o
export OPENAI_API_KEY=votre_cle

# Mistral
export EXTERNAL_PROVIDER=mistral  
export EXTERNAL_MODEL=mistral-large-latest
export MISTRAL_API_KEY=votre_cle

# Claude
export EXTERNAL_PROVIDER=claude
export EXTERNAL_MODEL=claude-3-5-sonnet-20241022
export ANTHROPIC_API_KEY=votre_cle

# Gemini
export EXTERNAL_PROVIDER=gemini
export EXTERNAL_MODEL=gemini-1.5-pro
export GOOGLE_API_KEY=votre_cle
```

**Mod√®les Locaux :**
```bash
export MODEL_ENDPOINT=http://localhost:8000/generate
export MODEL_NAME=nom_de_votre_modele
```

### Configuration de l'√âvaluateur

**Azure OpenAI (Par d√©faut) :**
```bash
export AZURE_OPENAI_API_KEY=votre_cle
export AZURE_OPENAI_ENDPOINT=votre_endpoint
```

**√âvaluateurs Alternatifs :**
```bash
# OpenAI comme √©valuateur
export EVALUATOR_PROVIDER=openai
export EVALUATOR_MODEL=gpt-4o
export EVALUATOR_OPENAI_API_KEY=votre_cle

# Mistral comme √©valuateur
export EVALUATOR_PROVIDER=mistral
export EVALUATOR_MODEL=mistral-large-latest
export EVALUATOR_MISTRAL_API_KEY=votre_cle

# Claude comme √©valuateur
export EVALUATOR_PROVIDER=claude
export EVALUATOR_MODEL=claude-3-5-sonnet-20241022
export EVALUATOR_ANTHROPIC_API_KEY=votre_cle

# Gemini comme √©valuateur
export EVALUATOR_PROVIDER=gemini
export EVALUATOR_MODEL=gemini-1.5-pro
export EVALUATOR_GOOGLE_API_KEY=votre_cle

# Mod√®le local comme √©valuateur
export EVALUATOR_PROVIDER=local
export EVALUATOR_ENDPOINT=http://localhost:8001/generate
```

## Commandes

### Lancer l'√âvaluation
```bash
# √âvaluation compl√®te (2 658 √©chantillons)
lae-eval run

# √âchantillons limit√©s
lae-eval run --max-samples 100

# Mode synchrone (plus stable)
lae-eval run --sync

# Endpoint chat pour mod√®les locaux
lae-eval run --chat

# R√©pertoire de sortie personnalis√©
lae-eval run --output-dir resultats_personnalises
```

### Tester les Composants
```bash
# Tester la connexion au mod√®le
lae-eval test-model

# Tester la connexion √† l'√©valuateur
lae-eval test-evaluator

# Afficher la configuration actuelle
lae-eval info
```

## Architecture

### Flux de Travail

```mermaid
graph TD
    A[Dataset: 2 658 Questions Juridiques] --> B[Mod√®le √† √âvaluer]
    B --> C[R√©ponse Structur√©e]
    C --> D[√âvaluateur LLM]
    D --> E[Scores: 5 Cat√©gories]
    E --> F[R√©sultats: JSON/Excel/Rapports]
    
    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#fce4ec
    style F fill:#e1f5fe
```

### Format de R√©ponse Requis

Les mod√®les doivent r√©pondre avec cette structure :
```
[Analyse et raisonnement...]

‚Ä¢ Action Requise: [action sp√©cifique] parce que [r√©f√©rence l√©gale]
‚Ä¢ D√©lai Legal: [d√©lai] parce que [r√©f√©rence l√©gale]
‚Ä¢ Documents Obligatoires: [documents requis] parce que [r√©f√©rence l√©gale]
‚Ä¢ Impact Financier: [co√ªts/frais] parce que [r√©f√©rence l√©gale]
‚Ä¢ Cons√©quences Non-Conformit√©: [risques] parce que [r√©f√©rence l√©gale]
```

### Cat√©gories d'√âvaluation

1. **Action Requise** - Actions l√©gales n√©cessaires
2. **D√©lai Legal** - √âch√©ances et d√©lais l√©gaux
3. **Documents Obligatoires** - Documentation obligatoire
4. **Impact Financier** - Implications financi√®res
5. **Cons√©quences Non-Conformit√©** - Cons√©quences du non-respect

Chaque cat√©gorie not√©e de 0 √† 100 avec justifications d√©taill√©es.

## R√©sultats

### Fichiers de Sortie
- `evaluation_results.json` - R√©sultats d√©taill√©s avec scores et justifications
- `evaluation_summary.csv` - Statistiques agr√©g√©es
- `evaluation_report.xlsx` - Rapport Excel multi-feuilles avec visualisations
- `score_distribution.png` - Graphiques de distribution des scores
- `evaluation.log` - Logs d'ex√©cution d√©taill√©s

### M√©triques Cl√©s
- **Score Global** - Moyenne de toutes les cat√©gories
- **Scores par Cat√©gorie** - Performance individuelle par domaine juridique
- **Qualit√© des R√©ponses** - Conformit√© du format et compl√©tude
- **Statistiques de Traitement** - Temps et taux d'erreur

## D√©pannage

### Probl√®mes Courants

**Erreurs de Connexion API :**
```bash
# V√©rifier les identifiants
lae-eval info
env | grep -E "(API_KEY|ENDPOINT)"

# Tester avec un √©chantillon minimal
lae-eval run --max-samples 1
```

**Probl√®mes de Mod√®le Local :**
```bash
# Tester l'endpoint manuellement
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Test", "max_new_tokens": 100}'

# Essayer l'endpoint chat
lae-eval run --chat
```

**Probl√®mes de Performance :**
```bash
# R√©duire la concurrence
export BATCH_SIZE=5
export CONCURRENT_REQUESTS=10

# Utiliser le mode synchrone
lae-eval run --sync
```

### Mode Debug
```bash
export LOG_LEVEL=DEBUG
lae-eval run --max-samples 1
# V√©rifier evaluation.log pour les informations d√©taill√©es
```

## Dataset

**Source :** `legmlai/les-audits-affaires` sur HuggingFace
- 2 658 sc√©narios de droit des affaires fran√ßais
- Questions juridiques r√©elles de divers contextes d'entreprise
- Ground truth valid√©e par des experts sur 5 cat√©gories juridiques
- Couverture compl√®te du droit commercial fran√ßais

## üìö Documentation

- **[Guide de Versioning Automatique](docs/VERSIONING.md)** - Syst√®me de release automatique avec Conventional Commits
- **[Fournisseurs Externes](EXTERNAL_PROVIDERS.md)** - Configuration des diff√©rents mod√®les et √©valuateurs
- **[Historique des Modifications](CHANGELOG.md)** - Changelog d√©taill√© des versions

## D√©veloppement

### Configuration
```bash
git clone <repository-url>
cd les-audits-affaires-eval-harness
python -m venv venv
source venv/bin/activate
pip install -e ".[dev]"
```

### Qualit√© du Code
```bash
pytest tests/
black src/ tests/
isort src/ tests/
mypy src/
```

## Licence

Licence MIT - voir le fichier LICENSE pour les d√©tails.

## Support

1. Consultez ce README pour les solutions courantes
2. Lancez `lae-eval info` pour v√©rifier la configuration
3. Testez avec `--max-samples 1` d'abord
4. V√©rifiez `evaluation.log` pour les messages d'erreur d√©taill√©s
5. Ouvrez une issue si les probl√®mes persistent 
