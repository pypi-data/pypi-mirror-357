# Les Audits-Affaires - Harness d'√âvaluation LLM

<p align="left">üá¨üáß <a href="README_EN.md">English version</a></p>
<p align="center">
  <img src="legml-ai-white.svg" alt="LegML.ai logo" width="180"/>
</p>

Un framework d'√©valuation complet pour tester les mod√®les de langage sur le benchmark juridique fran√ßais **Les Audits-Affaires**. √âvalue les mod√®les sur 5 cat√©gories de comp√©tences juridiques en utilisant le dataset `legmlai/les-audits-affaires`.

## D√©marrage Rapide

### Installation
```bash
pip install -e .
```

### Utilisation de Base
```bash
# √âvaluer OpenAI GPT-4o avec √©valuateur Azure OpenAI
export EXTERNAL_PROVIDER=openai
export EXTERNAL_MODEL=gpt-4o
export OPENAI_API_KEY=votre_cle_openai
export AZURE_OPENAI_API_KEY=votre_cle_azure
export AZURE_OPENAI_ENDPOINT=votre_endpoint_azure

lae-eval run --max-samples 10
```

## Configuration

### Mod√®le √† √âvaluer

**Fournisseurs Externes :**
```bash
# OpenAI
export EXTERNAL_PROVIDER=openai
export EXTERNAL_MODEL=gpt-4o
export OPENAI_API_KEY=votre_cle

# Mistral
export EXTERNAL_PROVIDER=mistral  
export EXTERNAL_MODEL=mistral-large-latest
export MISTRAL_API_KEY=votre_cle

# Claude
export EXTERNAL_PROVIDER=claude
export EXTERNAL_MODEL=claude-3-5-sonnet-20241022
export ANTHROPIC_API_KEY=votre_cle

# Gemini
export EXTERNAL_PROVIDER=gemini
export EXTERNAL_MODEL=gemini-1.5-pro
export GOOGLE_API_KEY=votre_cle
```

**Mod√®les Locaux :**
```bash
export MODEL_ENDPOINT=http://localhost:8000/generate
export MODEL_NAME=nom_de_votre_modele
```

### Configuration de l'√âvaluateur

**Azure OpenAI (Par d√©faut) :**
```bash
export AZURE_OPENAI_API_KEY=votre_cle
export AZURE_OPENAI_ENDPOINT=votre_endpoint
```

**√âvaluateurs Alternatifs :**
```bash
# OpenAI comme √©valuateur
export EVALUATOR_PROVIDER=openai
export EVALUATOR_MODEL=gpt-4o
export EVALUATOR_OPENAI_API_KEY=votre_cle

# Mistral comme √©valuateur
export EVALUATOR_PROVIDER=mistral
export EVALUATOR_MODEL=mistral-large-latest
export EVALUATOR_MISTRAL_API_KEY=votre_cle

# Claude comme √©valuateur
export EVALUATOR_PROVIDER=claude
export EVALUATOR_MODEL=claude-3-5-sonnet-20241022
export EVALUATOR_ANTHROPIC_API_KEY=votre_cle

# Gemini comme √©valuateur
export EVALUATOR_PROVIDER=gemini
export EVALUATOR_MODEL=gemini-1.5-pro
export EVALUATOR_GOOGLE_API_KEY=votre_cle

# Mod√®le local comme √©valuateur
export EVALUATOR_PROVIDER=local
export EVALUATOR_ENDPOINT=http://localhost:8001/generate
```

## Commandes

### Lancer l'√âvaluation
```bash
# √âvaluation compl√®te (2 658 √©chantillons)
lae-eval run

# √âchantillons limit√©s
lae-eval run --max-samples 100

# Mode synchrone (plus stable)
lae-eval run --sync

# Endpoint chat pour mod√®les locaux
lae-eval run --chat

# R√©pertoire de sortie personnalis√©
lae-eval run --output-dir resultats_personnalises
```

### Tester les Composants
```bash
# Tester la connexion au mod√®le
lae-eval test-model

# Tester la connexion √† l'√©valuateur
lae-eval test-evaluator

# Afficher la configuration actuelle
lae-eval info
```

## Architecture

### Flux de Travail

```mermaid
graph TD
    A[Dataset: 2 658 Questions Juridiques] --> B[Mod√®le √† √âvaluer]
    B --> C[R√©ponse Structur√©e]
    C --> D[√âvaluateur LLM]
    D --> E[Scores: 5 Cat√©gories]
    E --> F[R√©sultats: JSON/Excel/Rapports]
    
    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#fce4ec
    style F fill:#e1f5fe
```

### Format de R√©ponse Requis

Les mod√®les doivent r√©pondre avec cette structure :
```
[Analyse et raisonnement...]

‚Ä¢ Action Requise: [action sp√©cifique] parce que [r√©f√©rence l√©gale]
‚Ä¢ D√©lai Legal: [d√©lai] parce que [r√©f√©rence l√©gale]
‚Ä¢ Documents Obligatoires: [documents requis] parce que [r√©f√©rence l√©gale]
‚Ä¢ Impact Financier: [co√ªts/frais] parce que [r√©f√©rence l√©gale]
‚Ä¢ Cons√©quences Non-Conformit√©: [risques] parce que [r√©f√©rence l√©gale]
```

### Cat√©gories d'√âvaluation

1. **Action Requise** - Actions l√©gales n√©cessaires
2. **D√©lai Legal** - √âch√©ances et d√©lais l√©gaux
3. **Documents Obligatoires** - Documentation obligatoire
4. **Impact Financier** - Implications financi√®res
5. **Cons√©quences Non-Conformit√©** - Cons√©quences du non-respect

Chaque cat√©gorie not√©e de 0 √† 100 avec justifications d√©taill√©es.

## R√©sultats

### Fichiers de Sortie
- `evaluation_results.json` - R√©sultats d√©taill√©s avec scores et justifications
- `evaluation_summary.csv` - Statistiques agr√©g√©es
- `evaluation_report.xlsx` - Rapport Excel multi-feuilles avec visualisations
- `score_distribution.png` - Graphiques de distribution des scores
- `evaluation.log` - Logs d'ex√©cution d√©taill√©s

### M√©triques Cl√©s
- **Score Global** - Moyenne de toutes les cat√©gories
- **Scores par Cat√©gorie** - Performance individuelle par domaine juridique
- **Qualit√© des R√©ponses** - Conformit√© du format et compl√©tude
- **Statistiques de Traitement** - Temps et taux d'erreur

## D√©pannage

### Probl√®mes Courants

**Erreurs de Connexion API :**
```bash
# V√©rifier les identifiants
lae-eval info
env | grep -E "(API_KEY|ENDPOINT)"

# Tester avec un √©chantillon minimal
lae-eval run --max-samples 1
```

**Probl√®mes de Mod√®le Local :**
```bash
# Tester l'endpoint manuellement
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Test", "max_new_tokens": 100}'

# Essayer l'endpoint chat
lae-eval run --chat
```

**Probl√®mes de Performance :**
```bash
# R√©duire la concurrence
export BATCH_SIZE=5
export CONCURRENT_REQUESTS=10

# Utiliser le mode synchrone
lae-eval run --sync
```

### Mode Debug
```bash
export LOG_LEVEL=DEBUG
lae-eval run --max-samples 1
# V√©rifier evaluation.log pour les informations d√©taill√©es
```

## Dataset

**Source :** `legmlai/les-audits-affaires` sur HuggingFace
- 2 658 sc√©narios de droit des affaires fran√ßais
- Questions juridiques r√©elles de divers contextes d'entreprise
- Ground truth valid√©e par des experts sur 5 cat√©gories juridiques
- Couverture compl√®te du droit commercial fran√ßais

## D√©veloppement

### Configuration
```bash
git clone <repository-url>
cd les-audits-affaires-eval-harness
python -m venv venv
source venv/bin/activate
pip install -e ".[dev]"
```

### Qualit√© du Code
```bash
pytest tests/
black src/ tests/
isort src/ tests/
mypy src/
```

## Licence

Licence MIT - voir le fichier LICENSE pour les d√©tails.

## Support

1. Consultez ce README pour les solutions courantes
2. Lancez `lae-eval info` pour v√©rifier la configuration
3. Testez avec `--max-samples 1` d'abord
4. V√©rifiez `evaluation.log` pour les messages d'erreur d√©taill√©s
5. Ouvrez une issue si les probl√®mes persistent 