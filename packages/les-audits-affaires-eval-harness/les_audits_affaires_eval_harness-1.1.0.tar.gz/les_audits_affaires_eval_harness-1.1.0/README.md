# Les Audits-Affaires - Harness d'Ã‰valuation LLM

<p align="left">ğŸ‡¬ğŸ‡§ <a href="README_EN.md">English version</a></p>
<p align="center">
  <img src="legml-ai-white.svg" alt="LegML.ai logo" width="180"/>
</p>

Un framework d'Ã©valuation complet pour les modÃ¨les de langage sur le benchmark juridique franÃ§ais **Les Audits-Affaires**.

## ğŸ¯ AperÃ§u

Ce harness d'Ã©valuation fournit une mÃ©thode systÃ©matique pour Ã©valuer les LLM sur des tÃ¢ches juridiques franÃ§aises en utilisant le dataset `legmlai/les-audits-affaires`. Le framework utilise Azure OpenAI GPT-4o comme Ã©valuateur expert pour noter les rÃ©ponses des modÃ¨les selon cinq catÃ©gories juridiques clÃ©s :

- **Action Requise** - Actions lÃ©gales nÃ©cessaires
- **DÃ©lai LÃ©gal** - Ã‰chÃ©ances et dÃ©lais lÃ©gaux
- **Documents Obligatoires** - Documentation requise
- **Impact Financier** - Implications financiÃ¨res
- **ConsÃ©quences Non-ConformitÃ©** - ConsÃ©quences du non-respect

## ğŸ—ºï¸ Workflow en un coup d'Å“il

```mermaid
graph TD;
    Q["Questions HF"] --> G["GÃ©nÃ©rer RÃ©ponses (LLM)"];
    G --> E["Ã‰valuation GPT-4o"];
    E --> S["Scores & Analyse"];
```

## ğŸš€ FonctionnalitÃ©s

- **Ã‰valuation Asynchrone/Synchrone** : Traitement par batch efficace avec concurrence contrÃ´lÃ©e
- **Notation ComplÃ¨te** : Ã‰valuation sur 5 catÃ©gories avec justifications dÃ©taillÃ©es
- **Gestion d'Erreurs Robuste** : Gestion gracieuse des Ã©checs d'API et tentatives de reprise
- **Formats de Sortie Multiples** : JSON, CSV, Excel et rapports Markdown
- **Suivi des ProgrÃ¨s** : Barres de progression en temps rÃ©el et sauvegarde intermÃ©diaire
- **Outils d'Analyse** : Visualisation et analyse statistique intÃ©grÃ©es
- **Configuration Flexible** : Personnalisation facile des paramÃ¨tres d'Ã©valuation
- **Fournisseurs Externes** : Support pour OpenAI, Mistral, Claude, Gemini

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- AccÃ¨s Ã  l'API Azure OpenAI
- AccÃ¨s Ã  votre endpoint de modÃ¨le ou clÃ©s API des fournisseurs externes

## ğŸ› ï¸ Installation

### Installation via pip (recommandÃ©e)
```bash
pip install les-audits-affaires-eval-harness
```

### Installation depuis les sources
```bash
git clone <repository-url>
cd les-audits-affaires-eval-harness
pip install -e .
```

### Installation pour le dÃ©veloppement
```bash
pip install -e ".[dev]"
```

## âš™ï¸ Configuration

### Variables d'Environnement

CrÃ©ez un fichier `.env` basÃ© sur `.env.example` :

```bash
# Configuration Azure OpenAI (obligatoire)
AZURE_OPENAI_ENDPOINT=https://votre-endpoint.cognitiveservices.azure.com/
AZURE_OPENAI_API_KEY=votre_clÃ©_api
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# Configuration du modÃ¨le Ã  Ã©valuer
MODEL_ENDPOINT=https://votre-modele.ngrok-free.app/generate
MODEL_NAME=nom-de-votre-modele

# Fournisseurs externes (optionnel)
OPENAI_API_KEY=sk-...
MISTRAL_API_KEY=...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...

# Configuration d'Ã©valuation
MAX_SAMPLES=1000
BATCH_SIZE=20
TEMPERATURE=0.1
MAX_TOKENS=32768
CONCURRENT_REQUESTS=150
```

## ğŸš€ Utilisation

### Interface en Ligne de Commande

```bash
# Ã‰valuation de base (asynchrone par dÃ©faut)
lae-eval run --max-samples 50

# Ã‰valuation synchrone (traitement sÃ©quentiel)
lae-eval run --sync --max-samples 50

# Mode strict avec formatage amÃ©liorÃ© (asynchrone)
lae-eval run --strict --max-samples 100

# Mode strict synchrone
lae-eval run --sync --strict --max-samples 100

# Reprendre depuis un Ã©chantillon spÃ©cifique
lae-eval run --start-from 200

# Tester les fournisseurs externes
lae-eval test-providers

# GÃ©nÃ©rer des analyses
lae-eval analyze --plots --report --excel
```

### Utilisation Programmatique

```python
from les_audits_affaires_eval import LesAuditsAffairesEvaluator
from les_audits_affaires_eval.clients import create_client
import asyncio

# Ã‰valuation avec fournisseur externe (asynchrone)
async with create_client("openai", model="gpt-4o") as client:
    response = await client.generate_response("Question juridique...")

# Ã‰valuation asynchrone (par dÃ©faut, haut dÃ©bit)
evaluator = LesAuditsAffairesEvaluator(use_chat_endpoint=True)
    results = await evaluator.run_evaluation(max_samples=100)

# Ã‰valuation synchrone (sÃ©quentielle, plus simple)
evaluator = LesAuditsAffairesEvaluator(use_strict_mode=True)
results = evaluator.run_evaluation_sync(max_samples=50)

# Utiliser asyncio.run() pour les mÃ©thodes async
results = asyncio.run(evaluator.run_evaluation(max_samples=100))
```

## ğŸ¤– ModÃ¨les SupportÃ©s

### ModÃ¨les Locaux/PersonnalisÃ©s
- Tout endpoint HTTP avec endpoints `/generate` ou `/chat`
- Configurable via `MODEL_ENDPOINT` et `MODEL_NAME`

### Fournisseurs Externes
- **OpenAI** : GPT-4o, GPT-4-turbo, GPT-3.5-turbo
- **Mistral** : mistral-large-latest, mistral-medium-latest
- **Claude** : claude-3-5-sonnet, claude-3-haiku
- **Gemini** : gemini-1.5-pro, gemini-1.0-pro

## ğŸ”„ Fonctionnement

```mermaid
graph LR
    A[Charger Questions] --> B[GÃ©nÃ©rer RÃ©ponses]
    B --> C[Ã‰valuateur Azure OpenAI]
    C --> D[Scores & Analyse]
    
    B1[Votre ModÃ¨le] --> B
    B2[OpenAI/Mistral/Claude/Gemini] --> B
```

1. **Chargement** des questions juridiques depuis le dataset HuggingFace
2. **GÃ©nÃ©ration** des rÃ©ponses via votre modÃ¨le ou fournisseurs externes
3. **Ã‰valuation** des rÃ©ponses avec Azure OpenAI et prompts d'expertise juridique
4. **Notation** sur 5 catÃ©gories juridiques (0-100 chacune)
5. **Analyse** des rÃ©sultats avec graphiques, rapports et exports Excel

## ğŸ›¤ï¸ Pipeline Technique ComplÃ¨te

Le cadre d'Ã©valuation suit **un pipeline Ã  six Ã©tapes** :

```mermaid
flowchart TD;
    subgraph "GÃ©nÃ©ration du Benchmark";
        A1["Personas synthÃ©tiques (400+)"] --> A2["Cas & questions juridiques (2 670)"];
        A2 --> A3["RÃ©fÃ©rences lÃ©gales ground-truth<br/>(5 catÃ©gories)"];
    end;

    subgraph "Ã‰valuation du ModÃ¨le";
        B1["Prompt STRICT 5 catÃ©gories"] --> B2["ModÃ¨le Ã  tester"];
        B2 -->|"RÃ©ponse brute"| B3["Extraction / Normalisation"];
        B3 --> C1["Prompt d'Ã©valuation<br/>(GPT-4o)"];
        C1 --> C2["Scores JSON 0-100 Ã— 5 + justification"];
    end;

    A3 -->|"Dataset HF"| B1;
    C2 --> D1["AggrÃ©gation & Rapports"];
```

### 1. GÃ©nÃ©ration du Dataset
- 400 + personas couvrant rÃ©gions, secteurs, tailles d'entreprise
- 9 codes juridiques franÃ§ais (commerce, travail, finance, etc.)
- Chaque cas contient : `question` + **ground-truth** structurÃ© sur 5 rubriques
- Pipeline open-source (voir [`datasets/legmlai/les-audits-affaires`](https://huggingface.co/datasets/legmlai/les-audits-affaires))

### 2. Prompt STRICT (injection dans le modÃ¨le Ã  tester)
```text
Tu es un expert juridique franÃ§ais spÃ©cialisÃ© en droit des affairesâ€¦

INSTRUCTIONS CRITIQUES â€“ RESPECTE CE FORMAT EXACTEMENT :
RÃ©ponds UNIQUEMENT avec ces 5 Ã©lÃ©ments dans cet ordre prÃ©cis :
â€¢ Action Requise: â€¦ parce que [rÃ©fÃ©rence lÃ©gale]
â€¢ DÃ©lai Legal: â€¦ parce que â€¦
â€¢ Documents Obligatoires: â€¦ parce que â€¦
â€¢ Impact Financier: â€¦ parce que â€¦
â€¢ ConsÃ©quences Non-ConformitÃ©: â€¦ parce que â€¦
```
Objectif : **forcer** le modÃ¨le Ã  structurer sa rÃ©ponse et citer la loi.

### 3. Extraction / Normalisation
- Nettoyage Ã©ventuel (tags, markdown)
- VÃ©rification de la prÃ©sence des 5 rubriques

### 4. Prompt d'Ã‰valuation (LLM Evaluator)
```text
Tu es un juriste-expert franÃ§ais â€¦
BarÃ¨me : 5 rubriques Ã— 100 pts.

"question": "{user_question}",
"model_response": "{model_response}",
"ground_truth": { â€¦ }

# Format JSON strict demandÃ©
{
  "score_global": 0,
  "scores": { â€¦ },
  "justifications": { â€¦ }
}
```
Le **GPT-4o** (ou tout autre LLM expert) renvoie un JSON structurÃ© avec :
- `scores` individuels (0-100)
- `score_global` (moyenne simple)
- `justifications` textuelles

### 5. AgrÃ©gation & Rapports
- Calcul de statistiques (moyennes, mÃ©dianes, Ã©carts-types)
- Export : CSV, Excel, JSONL
- Visualisations automatiques (distribution des scores, heatmaps, etc.)

### 6. Suivi & ReproductibilitÃ©
- Chaque exÃ©cution produit un dossier `results/<model>/`
- Les logs dÃ©taillent prompts, rÃ©ponses, scores, temps de latence
- Pipeline entiÃ¨rement scriptÃ© via `lae-eval run` â†’ `lae-eval analyze`

> ğŸ“Œ **But final** : fournir **un indicateur fiable de compÃ©tence juridique** des LLM en droit des affaires franÃ§ais, afin de guider le dÃ©veloppement de modÃ¨les experts plus petits et sobres en carbone.

## ğŸ“Š Format de RÃ©ponse Attendu

Les modÃ¨les doivent rÃ©pondre avec cette structure :

```
[Analyse et raisonnement...]

â€¢ Action Requise: [action spÃ©cifique] parce que [rÃ©fÃ©rence lÃ©gale]
â€¢ DÃ©lai Legal: [Ã©chÃ©ance] parce que [rÃ©fÃ©rence lÃ©gale]
â€¢ Documents Obligatoires: [documents requis] parce que [rÃ©fÃ©rence lÃ©gale]
â€¢ Impact Financier: [coÃ»ts/frais] parce que [rÃ©fÃ©rence lÃ©gale]
â€¢ ConsÃ©quences Non-ConformitÃ©: [risques] parce que [rÃ©fÃ©rence lÃ©gale]
```

## ğŸ“ Structure des RÃ©sultats

L'Ã©valuation gÃ©nÃ¨re plusieurs fichiers dans le rÃ©pertoire `results/{nom_modele}/` :

```
results/nom_modele/
â”œâ”€â”€ evaluation_results.json      # RÃ©sultats complets
â”œâ”€â”€ evaluation_summary.json      # Statistiques rÃ©sumÃ©es
â”œâ”€â”€ evaluation_summary.csv       # Format CSV pour analyse
â”œâ”€â”€ detailed_results.jsonl       # RÃ©sultats dÃ©taillÃ©s ligne par ligne
â”œâ”€â”€ analysis_report.md           # Rapport d'analyse complet
â”œâ”€â”€ evaluation_results.xlsx      # Excel avec plusieurs feuilles
â”œâ”€â”€ score_distributions.png      # Graphiques de distribution des scores
â”œâ”€â”€ correlation_heatmap.png      # Carte de corrÃ©lation des catÃ©gories
â””â”€â”€ evaluation.log              # Logs d'exÃ©cution dÃ©taillÃ©s
```

## ğŸ“ˆ MÃ©triques d'Ã‰valuation

### SystÃ¨me de Notation

Chaque Ã©chantillon reÃ§oit des scores (0-100) sur 5 catÃ©gories :
- **Action Requise** : Actions lÃ©gales nÃ©cessaires
- **DÃ©lai LÃ©gal** : Ã‰chÃ©ances et dÃ©lais lÃ©gaux
- **Documents Obligatoires** : Documentation requise
- **Impact Financier** : Implications financiÃ¨res
- **ConsÃ©quences Non-ConformitÃ©** : ConsÃ©quences du non-respect

### Score Global
Le score global est la moyenne arithmÃ©tique des 5 scores de catÃ©gorie.

### CritÃ¨res d'Ã‰valuation
Pour chaque catÃ©gorie, l'Ã©valuateur Azure OpenAI Ã©value :
- **Exactitude juridique** : PrÃ©cision lÃ©gale
- **Concordance** : Accord avec la vÃ©ritÃ© terrain
- **ClartÃ©** : ClartÃ© de la rÃ©ponse
- **Justification** : QualitÃ© du raisonnement juridique

## ğŸ”§ DÃ©veloppement

```bash
# Installation pour dÃ©veloppement
pip install -e ".[dev]"

# ExÃ©cuter les tests
make test

# Formater le code
make format

# VÃ©rifier la qualitÃ©
make quality

# Voir toutes les commandes
make help
```

## ğŸ—ï¸ Architecture

```
src/les_audits_affaires_eval/
â”œâ”€â”€ clients/              # Clients de modÃ¨les
â”‚   â”œâ”€â”€ external_providers.py
â”‚   â””â”€â”€ model_client.py
â”œâ”€â”€ evaluation/           # Logique d'Ã©valuation principale
â”œâ”€â”€ utils.py             # Analyse et visualisation
â”œâ”€â”€ config.py            # Configuration
â””â”€â”€ cli.py               # Interface en ligne de commande
```

## ğŸ” DÃ©pannage

### ProblÃ¨mes Courants

1. **Erreurs de Connexion** :
   - VÃ©rifiez que votre endpoint de modÃ¨le est accessible
   - VÃ©rifiez que le tunnel ngrok est actif
   - Assurez-vous de la connectivitÃ© rÃ©seau

2. **Erreurs Azure OpenAI** :
   - VÃ©rifiez la clÃ© API et l'endpoint
   - VÃ©rifiez les quotas et limites de taux
   - Assurez-vous que le nom de dÃ©ploiement est correct

3. **ProblÃ¨mes de Chargement du Dataset** :
   - VÃ©rifiez la connexion internet
   - VÃ©rifiez l'accÃ¨s au dataset HuggingFace
   - Essayez de charger manuellement avec la bibliothÃ¨que `datasets`

### Mode Debug

Activez les logs dÃ©taillÃ©s :

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“š DÃ©tails du Benchmark

- **Dataset** : `legmlai/les-audits-affaires` sur HuggingFace
- **Questions** : 1000+ scÃ©narios de droit des affaires franÃ§ais
- **Ã‰valuation** : GPT-4o avec prompts d'expert juridique
- **Notation** : 0-100 par catÃ©gorie, moyennÃ© pour le score global
- **Langues** : Domaine juridique franÃ§ais

## ğŸ¤ Contribution

1. Forkez le dÃ©pÃ´t
2. CrÃ©ez une branche de fonctionnalitÃ©
3. Effectuez vos modifications avec des tests
4. ExÃ©cutez `make quality`
5. Soumettez une pull request

## ğŸ“„ Licence

Licence MIT - voir le fichier LICENSE pour les dÃ©tails.

## ğŸ™ Remerciements

- **Dataset Les Audits-Affaires** : `legmlai/les-audits-affaires`
- **Azure OpenAI** : Pour les services d'Ã©valuation
- **HuggingFace** : Pour l'hÃ©bergement du dataset et les outils

---

**Bonne Ã‰valuation ! ğŸš€** 