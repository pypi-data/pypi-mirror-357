Metadata-Version: 2.4
Name: les-audits-affaires-eval-harness
Version: 1.1.0
Summary: Evaluation harness for Les Audits-Affaires LLM benchmark
Home-page: https://github.com/legmlai/les-audits-affaires-eval-harness
Author: LegML Team
Author-email: LegML Team <contact@legml.ai>
License: MIT
Project-URL: Homepage, https://github.com/legmlai/les-audits-affaires-eval-harness
Project-URL: Documentation, https://les-audits-affaires-eval-harness.readthedocs.io
Project-URL: Repository, https://github.com/legmlai/les-audits-affaires-eval-harness.git
Project-URL: Issues, https://github.com/legmlai/les-audits-affaires-eval-harness/issues
Project-URL: Changelog, https://github.com/legmlai/les-audits-affaires-eval-harness/blob/main/CHANGELOG.md
Keywords: llm,evaluation,legal,french,benchmark,nlp
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiohttp>=3.9
Requires-Dist: requests>=2.31
Requires-Dist: openai>=1.18
Requires-Dist: tenacity>=8.2
Requires-Dist: python-dotenv>=1.0
Requires-Dist: datasets>=2.18
Requires-Dist: tqdm>=4.66
Requires-Dist: jsonlines>=4.0
Requires-Dist: pandas>=2.2
Requires-Dist: matplotlib>=3.8
Requires-Dist: seaborn>=0.13
Requires-Dist: openpyxl>=3.1
Requires-Dist: numpy>=1.24
Requires-Dist: scipy>=1.11
Provides-Extra: dev
Requires-Dist: pytest>=8.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23; extra == "dev"
Requires-Dist: pytest-mock>=3.12; extra == "dev"
Requires-Dist: black>=24.0; extra == "dev"
Requires-Dist: isort>=5.13; extra == "dev"
Requires-Dist: flake8>=7.0; extra == "dev"
Requires-Dist: mypy>=1.8; extra == "dev"
Requires-Dist: pre-commit>=3.6; extra == "dev"
Requires-Dist: bandit>=1.7; extra == "dev"
Requires-Dist: safety>=3.0; extra == "dev"
Requires-Dist: mkdocs>=1.5; extra == "dev"
Requires-Dist: mkdocs-material>=9.5; extra == "dev"
Requires-Dist: mkdocstrings[python]>=0.24; extra == "dev"
Requires-Dist: build>=1.0; extra == "dev"
Requires-Dist: twine>=4.0; extra == "dev"
Requires-Dist: bump2version>=1.0; extra == "dev"
Provides-Extra: visualization
Requires-Dist: plotly>=5.17; extra == "visualization"
Requires-Dist: dash>=2.16; extra == "visualization"
Requires-Dist: streamlit>=1.29; extra == "visualization"
Provides-Extra: all
Requires-Dist: les-audits-affaires-eval-harness[dev,visualization]; extra == "all"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# Les Audits-Affaires - Harness d'√âvaluation LLM

<p align="left">üá¨üáß <a href="README_EN.md">English version</a></p>
<p align="center">
  <img src="legml-ai-white.svg" alt="LegML.ai logo" width="180"/>
</p>

Un framework d'√©valuation complet pour les mod√®les de langage sur le benchmark juridique fran√ßais **Les Audits-Affaires**.

## üéØ Aper√ßu

Ce harness d'√©valuation fournit une m√©thode syst√©matique pour √©valuer les LLM sur des t√¢ches juridiques fran√ßaises en utilisant le dataset `legmlai/les-audits-affaires`. Le framework utilise Azure OpenAI GPT-4o comme √©valuateur expert pour noter les r√©ponses des mod√®les selon cinq cat√©gories juridiques cl√©s :

- **Action Requise** - Actions l√©gales n√©cessaires
- **D√©lai L√©gal** - √âch√©ances et d√©lais l√©gaux
- **Documents Obligatoires** - Documentation requise
- **Impact Financier** - Implications financi√®res
- **Cons√©quences Non-Conformit√©** - Cons√©quences du non-respect

## üó∫Ô∏è Workflow en un coup d'≈ìil

```mermaid
graph TD;
    Q["Questions HF"] --> G["G√©n√©rer R√©ponses (LLM)"];
    G --> E["√âvaluation GPT-4o"];
    E --> S["Scores & Analyse"];
```

## üöÄ Fonctionnalit√©s

- **√âvaluation Asynchrone/Synchrone** : Traitement par batch efficace avec concurrence contr√¥l√©e
- **Notation Compl√®te** : √âvaluation sur 5 cat√©gories avec justifications d√©taill√©es
- **Gestion d'Erreurs Robuste** : Gestion gracieuse des √©checs d'API et tentatives de reprise
- **Formats de Sortie Multiples** : JSON, CSV, Excel et rapports Markdown
- **Suivi des Progr√®s** : Barres de progression en temps r√©el et sauvegarde interm√©diaire
- **Outils d'Analyse** : Visualisation et analyse statistique int√©gr√©es
- **Configuration Flexible** : Personnalisation facile des param√®tres d'√©valuation
- **Fournisseurs Externes** : Support pour OpenAI, Mistral, Claude, Gemini

## üìã Pr√©requis

- Python 3.8+
- Acc√®s √† l'API Azure OpenAI
- Acc√®s √† votre endpoint de mod√®le ou cl√©s API des fournisseurs externes

## üõ†Ô∏è Installation

### Installation via pip (recommand√©e)
```bash
pip install les-audits-affaires-eval-harness
```

### Installation depuis les sources
```bash
git clone <repository-url>
cd les-audits-affaires-eval-harness
pip install -e .
```

### Installation pour le d√©veloppement
```bash
pip install -e ".[dev]"
```

## ‚öôÔ∏è Configuration

### Variables d'Environnement

Cr√©ez un fichier `.env` bas√© sur `.env.example` :

```bash
# Configuration Azure OpenAI (obligatoire)
AZURE_OPENAI_ENDPOINT=https://votre-endpoint.cognitiveservices.azure.com/
AZURE_OPENAI_API_KEY=votre_cl√©_api
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# Configuration du mod√®le √† √©valuer
MODEL_ENDPOINT=https://votre-modele.ngrok-free.app/generate
MODEL_NAME=nom-de-votre-modele

# Fournisseurs externes (optionnel)
OPENAI_API_KEY=sk-...
MISTRAL_API_KEY=...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...

# Configuration d'√©valuation
MAX_SAMPLES=1000
BATCH_SIZE=20
TEMPERATURE=0.1
MAX_TOKENS=32768
CONCURRENT_REQUESTS=150
```

## üöÄ Utilisation

### Interface en Ligne de Commande

```bash
# √âvaluation de base (asynchrone par d√©faut)
lae-eval run --max-samples 50

# √âvaluation synchrone (traitement s√©quentiel)
lae-eval run --sync --max-samples 50

# Mode strict avec formatage am√©lior√© (asynchrone)
lae-eval run --strict --max-samples 100

# Mode strict synchrone
lae-eval run --sync --strict --max-samples 100

# Reprendre depuis un √©chantillon sp√©cifique
lae-eval run --start-from 200

# Tester les fournisseurs externes
lae-eval test-providers

# G√©n√©rer des analyses
lae-eval analyze --plots --report --excel
```

### Utilisation Programmatique

```python
from les_audits_affaires_eval import LesAuditsAffairesEvaluator
from les_audits_affaires_eval.clients import create_client
import asyncio

# √âvaluation avec fournisseur externe (asynchrone)
async with create_client("openai", model="gpt-4o") as client:
    response = await client.generate_response("Question juridique...")

# √âvaluation asynchrone (par d√©faut, haut d√©bit)
evaluator = LesAuditsAffairesEvaluator(use_chat_endpoint=True)
    results = await evaluator.run_evaluation(max_samples=100)

# √âvaluation synchrone (s√©quentielle, plus simple)
evaluator = LesAuditsAffairesEvaluator(use_strict_mode=True)
results = evaluator.run_evaluation_sync(max_samples=50)

# Utiliser asyncio.run() pour les m√©thodes async
results = asyncio.run(evaluator.run_evaluation(max_samples=100))
```

## ü§ñ Mod√®les Support√©s

### Mod√®les Locaux/Personnalis√©s
- Tout endpoint HTTP avec endpoints `/generate` ou `/chat`
- Configurable via `MODEL_ENDPOINT` et `MODEL_NAME`

### Fournisseurs Externes
- **OpenAI** : GPT-4o, GPT-4-turbo, GPT-3.5-turbo
- **Mistral** : mistral-large-latest, mistral-medium-latest
- **Claude** : claude-3-5-sonnet, claude-3-haiku
- **Gemini** : gemini-1.5-pro, gemini-1.0-pro

## üîÑ Fonctionnement

```mermaid
graph LR
    A[Charger Questions] --> B[G√©n√©rer R√©ponses]
    B --> C[√âvaluateur Azure OpenAI]
    C --> D[Scores & Analyse]
    
    B1[Votre Mod√®le] --> B
    B2[OpenAI/Mistral/Claude/Gemini] --> B
```

1. **Chargement** des questions juridiques depuis le dataset HuggingFace
2. **G√©n√©ration** des r√©ponses via votre mod√®le ou fournisseurs externes
3. **√âvaluation** des r√©ponses avec Azure OpenAI et prompts d'expertise juridique
4. **Notation** sur 5 cat√©gories juridiques (0-100 chacune)
5. **Analyse** des r√©sultats avec graphiques, rapports et exports Excel

## üõ§Ô∏è Pipeline Technique Compl√®te

Le cadre d'√©valuation suit **un pipeline √† six √©tapes** :

```mermaid
flowchart TD;
    subgraph "G√©n√©ration du Benchmark";
        A1["Personas synth√©tiques (400+)"] --> A2["Cas & questions juridiques (2 670)"];
        A2 --> A3["R√©f√©rences l√©gales ground-truth<br/>(5 cat√©gories)"];
    end;

    subgraph "√âvaluation du Mod√®le";
        B1["Prompt STRICT 5 cat√©gories"] --> B2["Mod√®le √† tester"];
        B2 -->|"R√©ponse brute"| B3["Extraction / Normalisation"];
        B3 --> C1["Prompt d'√©valuation<br/>(GPT-4o)"];
        C1 --> C2["Scores JSON 0-100 √ó 5 + justification"];
    end;

    A3 -->|"Dataset HF"| B1;
    C2 --> D1["Aggr√©gation & Rapports"];
```

### 1. G√©n√©ration du Dataset
- 400 + personas couvrant r√©gions, secteurs, tailles d'entreprise
- 9 codes juridiques fran√ßais (commerce, travail, finance, etc.)
- Chaque cas contient : `question` + **ground-truth** structur√© sur 5 rubriques
- Pipeline open-source (voir [`datasets/legmlai/les-audits-affaires`](https://huggingface.co/datasets/legmlai/les-audits-affaires))

### 2. Prompt STRICT (injection dans le mod√®le √† tester)
```text
Tu es un expert juridique fran√ßais sp√©cialis√© en droit des affaires‚Ä¶

INSTRUCTIONS CRITIQUES ‚Äì RESPECTE CE FORMAT EXACTEMENT :
R√©ponds UNIQUEMENT avec ces 5 √©l√©ments dans cet ordre pr√©cis :
‚Ä¢ Action Requise: ‚Ä¶ parce que [r√©f√©rence l√©gale]
‚Ä¢ D√©lai Legal: ‚Ä¶ parce que ‚Ä¶
‚Ä¢ Documents Obligatoires: ‚Ä¶ parce que ‚Ä¶
‚Ä¢ Impact Financier: ‚Ä¶ parce que ‚Ä¶
‚Ä¢ Cons√©quences Non-Conformit√©: ‚Ä¶ parce que ‚Ä¶
```
Objectif : **forcer** le mod√®le √† structurer sa r√©ponse et citer la loi.

### 3. Extraction / Normalisation
- Nettoyage √©ventuel (tags, markdown)
- V√©rification de la pr√©sence des 5 rubriques

### 4. Prompt d'√âvaluation (LLM Evaluator)
```text
Tu es un juriste-expert fran√ßais ‚Ä¶
Bar√®me : 5 rubriques √ó 100 pts.

"question": "{user_question}",
"model_response": "{model_response}",
"ground_truth": { ‚Ä¶ }

# Format JSON strict demand√©
{
  "score_global": 0,
  "scores": { ‚Ä¶ },
  "justifications": { ‚Ä¶ }
}
```
Le **GPT-4o** (ou tout autre LLM expert) renvoie un JSON structur√© avec :
- `scores` individuels (0-100)
- `score_global` (moyenne simple)
- `justifications` textuelles

### 5. Agr√©gation & Rapports
- Calcul de statistiques (moyennes, m√©dianes, √©carts-types)
- Export : CSV, Excel, JSONL
- Visualisations automatiques (distribution des scores, heatmaps, etc.)

### 6. Suivi & Reproductibilit√©
- Chaque ex√©cution produit un dossier `results/<model>/`
- Les logs d√©taillent prompts, r√©ponses, scores, temps de latence
- Pipeline enti√®rement script√© via `lae-eval run` ‚Üí `lae-eval analyze`

> üìå **But final** : fournir **un indicateur fiable de comp√©tence juridique** des LLM en droit des affaires fran√ßais, afin de guider le d√©veloppement de mod√®les experts plus petits et sobres en carbone.

## üìä Format de R√©ponse Attendu

Les mod√®les doivent r√©pondre avec cette structure :

```
[Analyse et raisonnement...]

‚Ä¢ Action Requise: [action sp√©cifique] parce que [r√©f√©rence l√©gale]
‚Ä¢ D√©lai Legal: [√©ch√©ance] parce que [r√©f√©rence l√©gale]
‚Ä¢ Documents Obligatoires: [documents requis] parce que [r√©f√©rence l√©gale]
‚Ä¢ Impact Financier: [co√ªts/frais] parce que [r√©f√©rence l√©gale]
‚Ä¢ Cons√©quences Non-Conformit√©: [risques] parce que [r√©f√©rence l√©gale]
```

## üìÅ Structure des R√©sultats

L'√©valuation g√©n√®re plusieurs fichiers dans le r√©pertoire `results/{nom_modele}/` :

```
results/nom_modele/
‚îú‚îÄ‚îÄ evaluation_results.json      # R√©sultats complets
‚îú‚îÄ‚îÄ evaluation_summary.json      # Statistiques r√©sum√©es
‚îú‚îÄ‚îÄ evaluation_summary.csv       # Format CSV pour analyse
‚îú‚îÄ‚îÄ detailed_results.jsonl       # R√©sultats d√©taill√©s ligne par ligne
‚îú‚îÄ‚îÄ analysis_report.md           # Rapport d'analyse complet
‚îú‚îÄ‚îÄ evaluation_results.xlsx      # Excel avec plusieurs feuilles
‚îú‚îÄ‚îÄ score_distributions.png      # Graphiques de distribution des scores
‚îú‚îÄ‚îÄ correlation_heatmap.png      # Carte de corr√©lation des cat√©gories
‚îî‚îÄ‚îÄ evaluation.log              # Logs d'ex√©cution d√©taill√©s
```

## üìà M√©triques d'√âvaluation

### Syst√®me de Notation

Chaque √©chantillon re√ßoit des scores (0-100) sur 5 cat√©gories :
- **Action Requise** : Actions l√©gales n√©cessaires
- **D√©lai L√©gal** : √âch√©ances et d√©lais l√©gaux
- **Documents Obligatoires** : Documentation requise
- **Impact Financier** : Implications financi√®res
- **Cons√©quences Non-Conformit√©** : Cons√©quences du non-respect

### Score Global
Le score global est la moyenne arithm√©tique des 5 scores de cat√©gorie.

### Crit√®res d'√âvaluation
Pour chaque cat√©gorie, l'√©valuateur Azure OpenAI √©value :
- **Exactitude juridique** : Pr√©cision l√©gale
- **Concordance** : Accord avec la v√©rit√© terrain
- **Clart√©** : Clart√© de la r√©ponse
- **Justification** : Qualit√© du raisonnement juridique

## üîß D√©veloppement

```bash
# Installation pour d√©veloppement
pip install -e ".[dev]"

# Ex√©cuter les tests
make test

# Formater le code
make format

# V√©rifier la qualit√©
make quality

# Voir toutes les commandes
make help
```

## üèóÔ∏è Architecture

```
src/les_audits_affaires_eval/
‚îú‚îÄ‚îÄ clients/              # Clients de mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ external_providers.py
‚îÇ   ‚îî‚îÄ‚îÄ model_client.py
‚îú‚îÄ‚îÄ evaluation/           # Logique d'√©valuation principale
‚îú‚îÄ‚îÄ utils.py             # Analyse et visualisation
‚îú‚îÄ‚îÄ config.py            # Configuration
‚îî‚îÄ‚îÄ cli.py               # Interface en ligne de commande
```

## üîç D√©pannage

### Probl√®mes Courants

1. **Erreurs de Connexion** :
   - V√©rifiez que votre endpoint de mod√®le est accessible
   - V√©rifiez que le tunnel ngrok est actif
   - Assurez-vous de la connectivit√© r√©seau

2. **Erreurs Azure OpenAI** :
   - V√©rifiez la cl√© API et l'endpoint
   - V√©rifiez les quotas et limites de taux
   - Assurez-vous que le nom de d√©ploiement est correct

3. **Probl√®mes de Chargement du Dataset** :
   - V√©rifiez la connexion internet
   - V√©rifiez l'acc√®s au dataset HuggingFace
   - Essayez de charger manuellement avec la biblioth√®que `datasets`

### Mode Debug

Activez les logs d√©taill√©s :

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üìö D√©tails du Benchmark

- **Dataset** : `legmlai/les-audits-affaires` sur HuggingFace
- **Questions** : 1000+ sc√©narios de droit des affaires fran√ßais
- **√âvaluation** : GPT-4o avec prompts d'expert juridique
- **Notation** : 0-100 par cat√©gorie, moyenn√© pour le score global
- **Langues** : Domaine juridique fran√ßais

## ü§ù Contribution

1. Forkez le d√©p√¥t
2. Cr√©ez une branche de fonctionnalit√©
3. Effectuez vos modifications avec des tests
4. Ex√©cutez `make quality`
5. Soumettez une pull request

## üìÑ Licence

Licence MIT - voir le fichier LICENSE pour les d√©tails.

## üôè Remerciements

- **Dataset Les Audits-Affaires** : `legmlai/les-audits-affaires`
- **Azure OpenAI** : Pour les services d'√©valuation
- **HuggingFace** : Pour l'h√©bergement du dataset et les outils

---

**Bonne √âvaluation ! üöÄ** 
