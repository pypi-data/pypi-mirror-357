import logging
import re
from datetime import date
from functools import cache

import litellm

from bulkllm.schema import LLMConfig

logger = logging.getLogger(__name__)

default_max_tokens = None

default_temperature = 1

default_system_prompt = "You are a helpful AI assistant."
default_system_prompt = ""
default_models = []


openai_configs = [
    LLMConfig(
        slug="openai-o4-mini-2025-04-16-low",
        display_name="o4-mini (Low) 20250416",
        company_name="openai",
        litellm_model_name="openai/o4-mini-2025-04-16",
        llm_family="openai/o4-mini",
        temperature=1,
        max_completion_tokens=8000 - 1,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="low",
        release_date=date(2025, 4, 16),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-o4-mini-2025-04-16-medium",
        display_name="o4-mini (Medium) 20250416",
        company_name="openai",
        litellm_model_name="openai/o4-mini-2025-04-16",
        llm_family="openai/o4-mini",
        temperature=1,
        max_completion_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="medium",
        release_date=date(2025, 4, 16),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-o4-mini-2025-04-16-high",
        display_name="o4-mini (High) 20250416",
        company_name="openai",
        litellm_model_name="openai/o4-mini-2025-04-16",
        llm_family="openai/o4-mini",
        temperature=1,
        max_completion_tokens=8000 - 2,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="high",
        release_date=date(2025, 4, 16),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-o3-mini-2025-01-31-low",
        display_name="o3-mini (Low) 20250131",
        company_name="openai",
        litellm_model_name="openai/o3-mini-2025-01-31",
        llm_family="openai/o3-mini",
        temperature=1,
        max_tokens=8000 - 1,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="low",
        release_date=date(2025, 1, 31),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-o3-mini-2025-01-31-medium",
        display_name="o3-mini (Medium) 20250131",
        company_name="openai",
        litellm_model_name="openai/o3-mini-2025-01-31",
        llm_family="openai/o3-mini",
        temperature=1,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="medium",
        release_date=date(2025, 1, 31),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-o3-mini-2025-01-31-high",
        display_name="o3-mini (High) 20250131",
        company_name="openai",
        litellm_model_name="openai/o3-mini-2025-01-31",
        llm_family="openai/o3-mini",
        temperature=1,
        max_tokens=8000 - 2,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="high",
        release_date=date(2025, 1, 31),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-gpt-4.1-2025-04-14",
        display_name="GPT-4.1 20250414",
        company_name="openai",
        litellm_model_name="openai/gpt-4.1-2025-04-14",
        llm_family="openai/gpt-4.1",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 14),
    ),
    LLMConfig(
        slug="openai-gpt-4.1-mini-2025-04-14",
        display_name="GPT-4.1 Mini 20250414",
        company_name="openai",
        litellm_model_name="openai/gpt-4.1-mini-2025-04-14",
        llm_family="openai/gpt-4.1-mini",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 14),
    ),
    LLMConfig(
        slug="openai-gpt-4.1-nano-2025-04-14",
        display_name="GPT-4.1 Nano 20250414",
        company_name="openai",
        litellm_model_name="openai/gpt-4.1-nano-2025-04-14",
        llm_family="openai/gpt-4.1-nano",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 14),
    ),
    LLMConfig(
        slug="openai-gpt-4o-2024-05-13",
        display_name="GPT-4o 20240513",
        company_name="openai",
        litellm_model_name="openai/gpt-4o-2024-05-13",
        llm_family="openai/gpt-4o",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 5, 13),
    ),
    LLMConfig(
        slug="openai-gpt-4o-2024-08-06",
        display_name="GPT-4o 20240806",
        company_name="openai",
        litellm_model_name="openai/gpt-4o-2024-08-06",
        llm_family="openai/gpt-4o",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 8, 6),
    ),
    LLMConfig(
        slug="openai-gpt-4o-2024-11-20",
        display_name="GPT-4o 20241120",
        company_name="openai",
        litellm_model_name="openai/gpt-4o-2024-11-20",
        llm_family="openai/gpt-4o",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 11, 20),
    ),
    LLMConfig(
        slug="openai-gpt-4o-mini-2024-07-18",
        display_name="GPT-4o mini 20240718",
        company_name="openai",
        litellm_model_name="openai/gpt-4o-mini-2024-07-18",
        llm_family="openai/gpt-4o-mini",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 7, 18),
    ),
    LLMConfig(
        slug="openai-gpt-3.5-turbo-0125",
        display_name="GPT-3.5 Turbo 20230125",
        company_name="openai",
        litellm_model_name="openai/gpt-3.5-turbo-0125",
        llm_family="openai/gpt-3.5-turbo",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2023, 1, 25),
    ),
    LLMConfig(
        slug="openai-gpt-3.5-turbo-1106",
        display_name="GPT-3.5 Turbo 20231106",
        company_name="openai",
        litellm_model_name="openai/gpt-3.5-turbo-1106",
        llm_family="openai/gpt-3.5-turbo",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2023, 11, 6),
    ),
    # LLMConfig(
    #     slug="openai-babbage-002",
    #     display_name="Babbage 002",
    #     company_name="Openai",
    #     litellm_model_name="openai/babbage-002",
    #     llm_family="openai/babbage",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    LLMConfig(
        slug="openai-chatgpt-4o-latest",
        display_name="ChatGPT-4o",
        company_name="Openai",
        litellm_model_name="openai/chatgpt-4o-latest",
        llm_family="openai/chatgpt-4o-latest",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 5, 13),
    ),
    LLMConfig(
        slug="openai-codex-mini-latest",
        display_name="codex-mini-latest",
        company_name="Openai",
        litellm_model_name="openai/codex-mini-latest",
        llm_family="openai/codex-mini-latest",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=None,
    ),
    # LLMConfig(
    #     slug="openai-davinci-002",
    #     display_name="Davinci 002",
    #     company_name="Openai",
    #     litellm_model_name="openai/davinci-002",
    #     llm_family="openai/davinci",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2022, 3, 15),
    # ),
    # LLMConfig(
    #     slug="openai-gpt-3.5-0301",
    #     display_name="Gpt 3.5 0301",
    #     company_name="Openai",
    #     litellm_model_name="openai/gpt-3.5-0301",
    #     llm_family="openai/gpt-3.5",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2023, 3, 1),
    # ),
    # LLMConfig(
    #     slug="openai-gpt-3.5-turbo-0613",
    #     display_name="Gpt 3.5 Turbo 0613",
    #     company_name="Openai",
    #     litellm_model_name="openai/gpt-3.5-turbo-0613",
    #     llm_family="openai/gpt-3.5-turbo",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2023, 6, 13),
    # ),
    LLMConfig(
        slug="openai-gpt-3.5-turbo-16k-0613",
        display_name="Gpt 3.5 Turbo 16K 0613",
        company_name="Openai",
        litellm_model_name="openai/gpt-3.5-turbo-16k-0613",
        llm_family="openai/gpt-3.5-turbo-16k",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2023, 6, 13),
        is_deprecated=date(2024, 9, 13),
    ),
    # LLMConfig(
    #     slug="openai-gpt-3.5-turbo-instruct",
    #     display_name="Gpt 3.5 Turbo Instruct",
    #     company_name="Openai",
    #     litellm_model_name="openai/gpt-3.5-turbo-instruct",
    #     llm_family="openai/gpt-3.5-turbo-instruct",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    LLMConfig(
        slug="openai-gpt-4-0125-preview",
        display_name="GPT-4 Turbo Preview",
        company_name="Openai",
        litellm_model_name="openai/gpt-4-0125-preview",
        llm_family="openai/gpt-4-0125-preview",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 1, 25),
    ),
    LLMConfig(
        slug="openai-gpt-4-0314",
        display_name="Gpt 4 0314",
        company_name="Openai",
        litellm_model_name="openai/gpt-4-0314",
        llm_family="openai/gpt",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2023, 3, 14),
        is_deprecated=date(2024, 6, 13),
    ),
    LLMConfig(
        slug="openai-gpt-4-0613",
        display_name="GPT-4",
        company_name="Openai",
        litellm_model_name="openai/gpt-4-0613",
        llm_family="openai/gpt",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2023, 6, 13),
    ),
    LLMConfig(
        slug="openai-gpt-4-1106-preview",
        display_name="Gpt 4 1106 Preview",
        company_name="Openai",
        litellm_model_name="openai/gpt-4-1106-preview",
        llm_family="openai/gpt-4-1106-preview",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2023, 11, 6),
    ),
    # LLMConfig(
    #     slug="openai-gpt-4-1106-vision-preview",
    #     display_name="Gpt 4 1106 Vision Preview",
    #     company_name="Openai",
    #     litellm_model_name="openai/gpt-4-1106-vision-preview",
    #     llm_family="openai/gpt-4-1106-vision-preview",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2023, 11, 6),
    # ),
    LLMConfig(
        slug="openai-gpt-4-turbo-2024-04-09",
        display_name="GPT-4 Turbo",
        company_name="Openai",
        litellm_model_name="openai/gpt-4-turbo-2024-04-09",
        llm_family="openai/gpt-4-turbo",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 4, 9),
    ),
    LLMConfig(
        slug="openai-gpt-4.5-preview-2025-02-27",
        display_name="GPT-4.5 Preview",
        company_name="Openai",
        litellm_model_name="openai/gpt-4.5-preview-2025-02-27",
        llm_family="openai/gpt-4.5-preview",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 27),
    ),
    # LLMConfig(
    #     slug="openai-gpt-4o-mini-search-preview-2025-03-11",
    #     display_name="GPT-4o mini Search Preview",
    #     company_name="Openai",
    #     litellm_model_name="openai/gpt-4o-mini-search-preview-2025-03-11",
    #     llm_family="openai/gpt-4o-mini-search-preview",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 3, 11),
    # ),
    # LLMConfig(
    #     slug="openai-gpt-4o-search-preview-2025-03-11",
    #     display_name="GPT-4o Search Preview",
    #     company_name="Openai",
    #     litellm_model_name="openai/gpt-4o-search-preview-2025-03-11",
    #     llm_family="openai/gpt-4o-search-preview",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 3, 11),
    # ),
    LLMConfig(
        slug="openai-o1-2024-12-17",
        display_name="O1 2024 12 17",
        company_name="Openai",
        litellm_model_name="openai/o1-2024-12-17",
        llm_family="openai/o1",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 12, 17),
    ),
    LLMConfig(
        slug="openai-o1-mini-2024-09-12",
        display_name="O1 Mini 2024 09 12",
        company_name="Openai",
        litellm_model_name="openai/o1-mini-2024-09-12",
        llm_family="openai/o1-mini",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 9, 12),
    ),
    LLMConfig(
        slug="openai-o1-preview-20240912",
        display_name="o1 Preview",
        company_name="Openai",
        litellm_model_name="openai/o1-preview-2024-09-12",
        llm_family="openai/o1-preview",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 9, 12),
    ),
    LLMConfig(
        slug="openai-o1-pro-20250319",
        display_name="O1 Pro 20250319",
        company_name="Openai",
        litellm_model_name="openai/o1-pro-2025-03-19",
        llm_family="openai/o1-pro",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 3, 19),
    ),
    LLMConfig(
        slug="openai-o3-20250416",
        display_name="o3 20250416",
        company_name="openai",
        litellm_model_name="openai/o3-2025-04-16",
        llm_family="openai/o3",
        temperature=1,
        max_completion_tokens=8000 - 2,
        thinking_config={},
        system_prompt=default_system_prompt,
        reasoning_effort="high",
        release_date=date(2025, 4, 16),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="openai-o3-pro-20250610",
        display_name="O3 Pro 20250610",
        company_name="Openai",
        litellm_model_name="openai/o3-pro-2025-06-10",
        llm_family="openai/o3-pro",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        reasoning_effort="high",
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 10),
    ),
]
default_models.extend(openai_configs)

openrouter_configs = [
    LLMConfig(
        slug="openrouter-deepseek-deepseek-r1",
        display_name="DeepSeek R1 20250120",
        company_name="DeepSeek",
        litellm_model_name="openrouter/deepseek/deepseek-r1",
        llm_family="deepseek/deepseek-r1",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        is_reasoning=True,
        release_date=date(2025, 1, 20),
    ),
    LLMConfig(
        slug="openrouter-deepseek-deepseek-chat-v3-0324",
        display_name="DeepSeek V3 20250324",
        company_name="DeepSeek",
        litellm_model_name="openrouter/deepseek/deepseek-chat-v3-0324",
        llm_family="deepseek/deepseek-chat",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 3, 24),
    ),
    LLMConfig(
        slug="openrouter-deepseek-deepseek-chat",
        display_name="DeepSeek V3 20241226",
        company_name="DeepSeek",
        litellm_model_name="openrouter/deepseek/deepseek-chat",
        llm_family="deepseek/deepseek-chat",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 12, 26),
    ),
    LLMConfig(
        slug="openrouter-amazon-nova-pro-v1",
        display_name="Nova Pro V1",
        company_name="Amazon",
        litellm_model_name="openrouter/amazon/nova-pro-v1",
        llm_family="amazon/nova-pro",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 12, 3),
    ),
    LLMConfig(
        slug="openrouter-qwen-qwq-32b",
        display_name="Qwen QwQ-32B",
        company_name="Alibaba",
        litellm_model_name="openrouter/qwen/qwq-32b",
        llm_family="qwen/qwq-32b",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        is_reasoning=True,
        release_date=date(2025, 3, 6),
    ),
    LLMConfig(
        slug="openrouter-meta-llama-llama-3.3-70b-instruct",
        display_name="Llama 3.3 70b Instruct",
        company_name="Meta",
        litellm_model_name="openrouter/meta-llama/llama-3.3-70b-instruct",
        llm_family="meta-llama/llama-3.3-70b-instruct",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 12, 6),
    ),
    LLMConfig(
        slug="openrouter-mistralai-mistral-large-2411",
        display_name="Mistral Large 2411",
        company_name="MistralAI",
        litellm_model_name="openrouter/mistralai/mistral-large-2411",
        llm_family="mistralai/mistral-large",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 11, 18),
    ),
    LLMConfig(
        slug="openrouter-mistralai-mistral-small-3.1-24b-instruct",
        display_name="Mistral Small 3.1 24b Instruct 20250317",
        company_name="MistralAI",
        litellm_model_name="openrouter/mistralai/mistral-small-3.1-24b-instruct",
        llm_family="mistralai/mistral-small-3.1-24b-instruct",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 3, 17),
    ),
    LLMConfig(
        slug="openrouter-google-gemma-3-27b-it",
        display_name="Gemma 3 27b IT",
        company_name="Google",
        litellm_model_name="openrouter/google/gemma-3-27b-it",
        llm_family="google/gemma-3-27b-it",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 3, 12),
    ),
    LLMConfig(
        slug="openrouter-meta-llama-llama-4-maverick",
        display_name="Llama 4 Maverick",
        company_name="Meta",
        litellm_model_name="openrouter/meta-llama/llama-4-maverick",
        llm_family="meta-llama/llama-4",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 5),
    ),
    LLMConfig(
        slug="openrouter-meta-llama-llama-4-scout",
        display_name="Llama 4 Scout",
        company_name="Meta",
        litellm_model_name="openrouter/meta-llama/llama-4-scout",
        llm_family="meta-llama/llama-4",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 5),
    ),
]
default_models.extend(openrouter_configs)

anthropic_configs = [
    LLMConfig(
        slug="anthropic-claude-3.7-sonnet-20250219",
        display_name="Claude 3.7 Sonnet 20250219",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-7-sonnet-20250219",
        llm_family="anthropic/claude-3.7-sonnet",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 19),
    ),
    LLMConfig(
        slug="anthropic-claude-3.7-sonnet-20250219-thinking",
        display_name="Claude 3.7 Sonnet (thinking) 20250219",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-7-sonnet-20250219",
        llm_family="anthropic/claude-3.7-sonnet",
        temperature=default_temperature,
        max_tokens=8000 + 1,
        thinking_config={"type": "enabled", "budget_tokens": 2048},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 19),
        is_reasoning=True,
    ),
    LLMConfig(
        slug="anthropic-claude-3.5-sonnet-20241022",
        display_name="Claude 3.5 Sonnet 20241022",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-5-sonnet-20241022",
        llm_family="anthropic/claude-3.5-sonnet",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 10, 22),
    ),
    LLMConfig(
        slug="anthropic-claude-3.5-sonnet-20240620",
        display_name="Claude 3.5 Sonnet 20240620",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-5-sonnet-20240620",
        llm_family="anthropic/claude-3.5-sonnet",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 6, 20),
    ),
    LLMConfig(
        slug="anthropic-claude-3-5-haiku-20241022",
        display_name="Claude 3.5 Haiku 20241022",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-5-haiku-20241022",
        llm_family="anthropic/claude-3-5-haiku",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 10, 22),
    ),
    # LLMConfig(
    #     slug="anthropic-claude-2.0",
    #     display_name="Claude 2.0",
    #     company_name="Anthropic",
    #     litellm_model_name="anthropic/claude-2.0",
    #     llm_family="anthropic/claude-2.0",
    #     temperature=default_temperature,
    #     max_tokens=4096,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2023, 7, 11),
    # ),
    # LLMConfig(
    #     slug="anthropic-claude-2.1",
    #     display_name="Claude 2.1",
    #     company_name="Anthropic",
    #     litellm_model_name="anthropic/claude-2.1",
    #     llm_family="anthropic/claude-2.1",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2023, 11, 21),
    # ),
    LLMConfig(
        slug="anthropic-claude-3-haiku-20240307",
        display_name="Claude Haiku 3",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-haiku-20240307",
        llm_family="anthropic/claude-3-haiku",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 3, 7),
    ),
    LLMConfig(
        slug="anthropic-claude-3-opus-20240229",
        display_name="Claude Opus 3",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-opus-20240229",
        llm_family="anthropic/claude-3-opus",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 2, 29),
    ),
    LLMConfig(
        slug="anthropic-claude-3-sonnet-20240229",
        display_name="Claude Sonnet 3",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-3-sonnet-20240229",
        llm_family="anthropic/claude-3-sonnet",
        temperature=default_temperature,
        max_tokens=4096,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 2, 29),
    ),
    LLMConfig(
        slug="anthropic-claude-opus-4-20250514",
        display_name="Claude Opus 4",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-opus-4-20250514",
        llm_family="anthropic/claude-opus",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 5, 14),
    ),
    LLMConfig(
        slug="anthropic-claude-sonnet-4-20250514",
        display_name="Claude Sonnet 4",
        company_name="Anthropic",
        litellm_model_name="anthropic/claude-sonnet-4-20250514",
        llm_family="anthropic/claude-sonnet",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 5, 14),
    ),
]
default_models.extend(anthropic_configs)

gemini_configs = [
    LLMConfig(
        slug="gemini-gemini-1.5-flash-002",
        display_name="Gemini 1.5 Flash 002",
        company_name="Google",
        litellm_model_name="gemini/gemini-1.5-flash-002",
        llm_family="gemini/gemini-1.5-flash",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 9, 24),
    ),
    LLMConfig(
        slug="gemini-gemini-1.5-pro-002",
        display_name="Gemini 1.5 Pro 002",
        company_name="Google",
        litellm_model_name="gemini/gemini-1.5-pro-002",
        llm_family="gemini/gemini-1.5-pro",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 9, 24),
    ),
    LLMConfig(
        slug="gemini-gemini-2.0-flash-lite",
        display_name="Gemini 2.0 Flash Lite",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.0-flash-lite",
        llm_family="gemini/gemini-2.0-flash-lite",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 5),
    ),
    LLMConfig(
        slug="gemini-gemini-2.0-flash",
        display_name="Gemini 2.0 Flash",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.0-flash",
        llm_family="gemini/gemini-2.0-flash",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 5),
    ),
    LLMConfig(
        slug="gemini-2.5-flash-preview-04-17",
        display_name="Gemini 2.5 Flash Preview 20250417",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash-preview-04-17",
        llm_family="gemini/gemini-2.5-flash-preview",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 17),
    ),
    # LLMConfig(
    #     slug="gemini-gemini-1.0-pro-vision-latest",
    #     display_name="Gemini 1.0 Pro Vision",
    #     company_name="Gemini",
    #     litellm_model_name="gemini/gemini-1.0-pro-vision-latest",
    #     llm_family="gemini/gemini-1.0-pro-vision-latest",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 2, 15),
    #     is_deprecated=True,
    # ),
    LLMConfig(
        slug="gemini-gemini-2.0-flash-lite-preview",
        display_name="Gemini 2.0 Flash-Lite Preview",
        company_name="Gemini",
        litellm_model_name="gemini/gemini-2.0-flash-lite-preview",
        llm_family="gemini/gemini-2.0-flash-lite-preview",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 25),
    ),
    LLMConfig(
        slug="gemini-2.5-pro-preview-03-25",
        display_name="Gemini 2.5 Pro Preview 20250325",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-pro-preview-03-25",
        llm_family="gemini/gemini-2.5-pro-preview",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 3, 25),
    ),
    LLMConfig(
        slug="gemini-2.5-pro-preview-05-06",
        display_name="Gemini 2.5 Pro Preview 05-06",
        company_name="Gemini",
        litellm_model_name="gemini/gemini-2.5-pro-preview-05-06",
        llm_family="gemini/gemini-2.5-pro-preview",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 5, 6),
    ),
    LLMConfig(
        slug="gemini-2.5-pro-preview-06-05",
        display_name="Gemini 2.5 Pro Preview",
        company_name="Gemini",
        litellm_model_name="gemini/gemini-2.5-pro-preview-06-05",
        llm_family="gemini/gemini-2.5-pro-preview",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 5),
    ),
    LLMConfig(
        slug="gemini-gemini-2.5-flash-20250617",
        display_name="Gemini 2.5 Flash 20250617",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash",
        llm_family="gemini/gemini-2.5-flash",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 17),
    ),
    LLMConfig(
        slug="gemini-gemini-2.5-flash-thinking-20250617",
        display_name="Gemini 2.5 Flash Thinking 20250617",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash",
        llm_family="gemini/gemini-2.5-flash",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={"type": "enabled", "budget_tokens": 8192},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 17),
    ),
    LLMConfig(
        slug="gemini-gemini-2.5-flash-lite-20250617",
        display_name="Gemini 2.5 Flash Lite 20250617",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash-lite-preview-06-17",
        llm_family="gemini/gemini-2.5-flash-lite",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 17),
    ),
    LLMConfig(
        slug="gemini-gemini-2.5-flash-lite-20250617-low",
        display_name="Gemini 2.5 Flash Lite 20250617 (low)",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash-lite-preview-06-17",
        llm_family="gemini/gemini-2.5-flash-lite",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={"type": "enabled", "budget_tokens": 1024},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 17),
    ),
    LLMConfig(
        slug="gemini-gemini-2.5-flash-lite-20250617-medium",
        display_name="Gemini 2.5 Flash Lite 20250617 (medium)",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash-lite-preview-06-17",
        llm_family="gemini/gemini-2.5-flash-lite",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={"type": "enabled", "budget_tokens": 2048},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 17),
    ),
    LLMConfig(
        slug="gemini-gemini-2.5-flash-lite-20250617-high",
        display_name="Gemini 2.5 Flash Lite 20250617 (high)",
        company_name="Google",
        litellm_model_name="gemini/gemini-2.5-flash-lite-preview-06-17",
        llm_family="gemini/gemini-2.5-flash-lite",
        temperature=default_temperature,
        max_tokens=default_max_tokens,
        thinking_config={"type": "enabled", "budget_tokens": 4096},
        system_prompt=default_system_prompt,
        release_date=date(2025, 6, 17),
    ),
]
default_models.extend(gemini_configs)

xai_configs = [
    LLMConfig(
        slug="xai-grok-2-1212",
        display_name="Grok 2 20241212",
        company_name="xai",
        litellm_model_name="xai/grok-2-1212",
        llm_family="xai/grok-2",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2024, 12, 12),
    ),
    LLMConfig(
        slug="xai-grok-3-20250409",
        display_name="Grok 3 Beta",
        company_name="xai",
        litellm_model_name="xai/grok-3-beta",
        llm_family="xai/grok-3",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 9),
    ),
    LLMConfig(
        slug="xai-grok-3-mini-low-20250409",
        display_name="Grok 3 Mini Beta (low)",
        company_name="xai",
        litellm_model_name="xai/grok-3-mini-beta",
        llm_family="xai/grok-3-mini",
        temperature=default_temperature,
        max_tokens=32000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 9),
        reasoning_effort="low",
        is_reasoning=True,
    ),
    LLMConfig(
        slug="xai-grok-3-mini-high-20250409",
        display_name="Grok 3 Mini Beta (high)",
        company_name="xai",
        litellm_model_name="xai/grok-3-mini-beta",
        llm_family="xai/grok-3-mini",
        temperature=default_temperature,
        max_tokens=32000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 4, 9),
        reasoning_effort="high",
        is_reasoning=True,
    ),
    LLMConfig(
        slug="xai-grok-3",
        display_name="Grok 3",
        company_name="Xai",
        litellm_model_name="xai/grok-3",
        llm_family="xai/grok",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 17),
    ),
    LLMConfig(
        slug="xai-grok-3-mini",
        display_name="Grok 3 Mini",
        company_name="Xai",
        litellm_model_name="xai/grok-3-mini",
        llm_family="xai/grok-3-mini",
        temperature=default_temperature,
        max_tokens=8000,
        thinking_config={},
        system_prompt=default_system_prompt,
        release_date=date(2025, 2, 17),
    ),
]
default_models.extend(xai_configs)

mistral_configs = [
    # LLMConfig(
    #     slug="mistral-codestral-2405",
    #     display_name="Codestral 2405",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/codestral-2405",
    #     llm_family="mistral/codestral",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 5, 29),
    # ),
    # LLMConfig(
    #     slug="mistral-codestral-2501",
    #     display_name="Codestral 2501",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/codestral-2501",
    #     llm_family="mistral/codestral",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 1, 13),
    # ),
    # LLMConfig(
    #     slug="mistral-devstral-small-2505",
    #     display_name="Devstral Small 2505",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/devstral-small-2505",
    #     llm_family="mistral/devstral-small",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 5, 21),
    # ),
    # LLMConfig(
    #     slug="mistral-magistral-medium-2506",
    #     display_name="Magistral Medium 2506",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/magistral-medium-2506",
    #     llm_family="mistral/magistral-medium",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 6, 10),
    # ),
    # LLMConfig(
    #     slug="mistral-magistral-small-2506",
    #     display_name="Magistral Small 2506",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/magistral-small-2506",
    #     llm_family="mistral/magistral-small",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 6, 10),
    # ),
    # LLMConfig(
    #     slug="mistral-ministral-3b-2410",
    #     display_name="Ministral 3B 2410",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/ministral-3b-2410",
    #     llm_family="mistral/ministral-3b",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 10, 9),
    # ),
    # LLMConfig(
    #     slug="mistral-ministral-8b-2410",
    #     display_name="Ministral 8B 2410",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/ministral-8b-2410",
    #     llm_family="mistral/ministral-8b",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 10, 9),
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-large-2402",
    #     display_name="Mistral Large 2402",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-large-2402",
    #     llm_family="mistral/mistral-large",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-large-2407",
    #     display_name="Mistral Large 2407",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-large-2407",
    #     llm_family="mistral/mistral-large",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 7, 24),
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-large-2411",
    #     display_name="Mistral Large 2411",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-large-2411",
    #     llm_family="mistral/mistral-large",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 11, 18),
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-medium-2312",
    #     display_name="Mistral Medium 2312",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-medium-2312",
    #     llm_family="mistral/mistral-medium",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-medium-2505",
    #     display_name="Mistral Medium 2505",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-medium-2505",
    #     llm_family="mistral/mistral-medium",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 5, 7),
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-saba-2502",
    #     display_name="Mistral Saba 2502",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-saba-2502",
    #     llm_family="mistral/mistral-saba",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 2, 17),
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-small-2402",
    #     display_name="Mistral Small 2402",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-small-2402",
    #     llm_family="mistral/mistral-small",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-small-2409",
    #     display_name="Mistral Small 2409",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-small-2409",
    #     llm_family="mistral/mistral-small",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 1, 13),
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-small-2501",
    #     display_name="Mistral Small 2501",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-small-2501",
    #     llm_family="mistral/mistral-small",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-mistral-small-2503",
    #     display_name="Mistral Small 2503",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/mistral-small-2503",
    #     llm_family="mistral/mistral-small",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-open-mistral-7b",
    #     display_name="Open Mistral 7B",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/open-mistral-7b",
    #     llm_family="mistral/open-mistral-7b",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-open-mistral-nemo",
    #     display_name="Open Mistral Nemo",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/open-mistral-nemo",
    #     llm_family="mistral/open-mistral-nemo",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 7, 18),
    # ),
    # LLMConfig(
    #     slug="mistral-open-mixtral-8x22b",
    #     display_name="Open Mixtral 8X22B",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/open-mixtral-8x22b",
    #     llm_family="mistral/open-mixtral-8x22b",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 4, 17),
    # ),
    # LLMConfig(
    #     slug="mistral-open-mixtral-8x7b",
    #     display_name="Open Mixtral 8X7B",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/open-mixtral-8x7b",
    #     llm_family="mistral/open-mixtral-8x7b",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=None,
    # ),
    # LLMConfig(
    #     slug="mistral-pixtral-12b-2409",
    #     display_name="Pixtral 12B 2409",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/pixtral-12b-2409",
    #     llm_family="mistral/pixtral-12b",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2025, 1, 13),
    # ),
    # LLMConfig(
    #     slug="mistral-pixtral-large-2411",
    #     display_name="Pixtral Large 2411",
    #     company_name="Mistral",
    #     litellm_model_name="mistral/pixtral-large-2411",
    #     llm_family="mistral/pixtral-large",
    #     temperature=default_temperature,
    #     max_tokens=8000,
    #     thinking_config={},
    #     system_prompt=default_system_prompt,
    #     release_date=date(2024, 11, 18),
    # ),
]
default_models.extend(mistral_configs)


def create_model_configs(system_prompt: str | None = "You are a helpful AI assistant."):
    """Return deep copies of default models with a custom system prompt."""
    new_configs = []
    for llm_config in default_models:
        new_config = llm_config.model_copy()
        new_config.system_prompt = system_prompt
        if new_config.is_deprecated:
            continue
        new_configs.append(new_config)
    return new_configs


@cache
def model_info():
    """Gather info for each model config."""
    from bulkllm.model_registration.main import register_models
    from bulkllm.rate_limiter import RateLimiter

    register_models()

    # Initialize rate limiter to fetch rate limits
    rate_limiter = RateLimiter()
    prompt_tokens = 1_700_000  # Fixed input tokens for all models

    # Company â†’ colour and logo mappings used for charts / table
    company_colors = {
        "openai": "rgba(116, 170, 156, 1)",
        "anthropic": "rgb(204, 120, 92)",
        "google": "yellow",
        "gemini": "yellow",
        "vertex": "yellow",
        "deepseek": "rgba(83, 106, 245, 1)",
        "xai": "purple",
        "meta": "rgba(49, 111, 246, 1)",
        "llama": "rgba(49, 111, 246, 1)",
        "amazon": "rgba(255, 153, 0, 1)",
    }

    # Function to create a simpleicons logo URL for a company slug.
    def get_company_icon(c_slug: str) -> str:
        """Return an SVG logo URL from simpleicons CDN (falls back to generic icon)."""
        base = "https://cdn.simpleicons.org/"
        # Some company names differ from slug we want in URL
        simpleicons_overrides = {
            "openai": "openai",
            "anthropic": "anthropic",
            "google": "google",
            "gemini": "google",  # gemini not in simpleicons, use google icon
            "vertex": "googlecloud",
            "meta": "meta",
            "llama": "meta",
            "amazon": "amazon",
            "deepseek": "openrouter",  # fallback generic
            "xai": "x",
        }
        icon_slug = simpleicons_overrides.get(c_slug, c_slug)
        return f"{base}{icon_slug}/FFFFFF"

    # Default color for unknown companies
    default_color = "gray"

    model_entries: list[dict] = []

    for llm in default_models:
        completion_tokens = 4_800_000 if llm.is_reasoning else 1_800_000

        prompt_cost: float | None = None
        completion_cost: float | None = None
        total_cost: float | None = None

        try:
            prompt_cost, completion_cost = litellm.cost_per_token(
                model=llm.litellm_model_name,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
            )
            total_cost = prompt_cost + completion_cost
        except Exception as e:  # noqa
            logger.warning("Could not calculate cost for model %s: %s", llm.litellm_model_name, e)

        # Retrieve rate limit values from RateLimiter
        model_rl = rate_limiter.get_rate_limit_for_model(llm.litellm_model_name)
        tpm = model_rl.tpm
        itpm = model_rl.itpm
        otpm = model_rl.otpm
        rpm = model_rl.rpm

        # Determine colour & icon
        company = llm.company_name.lower()
        color = company_colors.get(company, default_color)
        company_icon = get_company_icon(company)

        # Remove trailing release-date numbers from display name (6+ digit sequences)
        sanitized_name = re.sub(r"(?:\s*\(?\d{4,}\)?)+$", "", llm.display_name).strip()

        model_entries.append(
            {
                "name": sanitized_name,
                "model_id": llm.litellm_model_name,
                "company": llm.company_name,
                "company_icon": company_icon,
                "release_date": llm.release_date.isoformat() if llm.release_date else None,
                "is_reasoning": llm.is_reasoning,
                "input_tokens": prompt_tokens,
                "output_tokens": completion_tokens,
                "tpm": tpm,
                "itpm": itpm,
                "otpm": otpm,
                "rpm": rpm,
                "prompt_cost": prompt_cost,
                "completion_cost": completion_cost,
                "total_cost": total_cost,
                "color": color,
            }
        )

    # Sort by total cost (descending where available)
    model_entries.sort(key=lambda x: x["total_cost"] if x["total_cost"] is not None else -1, reverse=True)
    return model_entries


@cache
def cheap_model_configs():
    """Return LLMConfig objects whose estimated total cost is less than $1."""
    entries = model_info()
    cheap_ids = {entry["model_id"] for entry in entries if entry["total_cost"] is not None and entry["total_cost"] < 1}
    if not cheap_ids:
        return []
    configs = create_model_configs()
    configs = [config for config in configs if not config.is_deprecated]
    return [config for config in configs if config.litellm_model_name in cheap_ids]


def model_resolver(model_slugs: list[str]) -> list[LLMConfig]:
    """Expand slugs or groups into concrete model configurations."""
    if not model_slugs:
        return cheap_model_configs()

    configs = create_model_configs()
    model_lookup = {config.slug: [config] for config in configs}
    model_group_lookup = {
        "cheap": cheap_model_configs,
        "default": cheap_model_configs,
        "all": configs,
        "reasoning": [config for config in configs if config.is_reasoning],
    }
    found_configs = []
    for slug in model_slugs:
        if slug in model_lookup:
            found_configs.extend(model_lookup[slug])
        elif slug in model_group_lookup:
            val = model_group_lookup[slug]
            if callable(val):
                val = val()
            found_configs.extend(val)
        else:
            msg = f"Unknown model config: {slug}"
            raise ValueError(msg)
    return found_configs
