Metadata-Version: 2.4
Name: pharia-inference-sdk
Version: 0.1.6
Project-URL: repository, https://github.com/Aleph-Alpha/pharia-inference-sdk.git
Author-email: Aleph Alpha Engineering <engineering@aleph-alpha.com>
License-File: LICENSE
License-File: NOTICE.md
Requires-Python: <3.13,>=3.10
Requires-Dist: aleph-alpha-client<11.0.0,>=10.2.2
Requires-Dist: ipykernel<7,>=6.29.5
Requires-Dist: lingua-language-detector>=2.0.0
Requires-Dist: opentelemetry-api<2,>=1.29.0
Requires-Dist: opentelemetry-exporter-otlp-proto-http==1.34.1
Requires-Dist: opentelemetry-sdk<2,>=1.29.0
Requires-Dist: pycountry<25,>=24.6.1
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-liquid==1.9.4
Requires-Dist: rich<15,>=14.0.0
Requires-Dist: semantic-text-splitter>=0.5.0
Description-Content-Type: text/markdown

# pharia-inference-sdk

Formerly the `intelligence_layer/core` package.

## Overview

The pharia-inference-sdk provides essential functionality for the intelligence layer.

## Installation
The SDK is published on [PyPI](https://pypi.org/project/pharia-inference-sdk/).

To add the SDK as a dependency to an existing project managed, run
```bash
pip install pharia-inference-sdk
```

## Usage

```python
from pharia_inference_sdk.core.tracer import InMemoryTracer
from pharia_inference_sdk.core.model import Llama3InstructModel, Prompt, CompleteInput
from aleph_alpha_client import Client

client=Client(token="<token>", host="<inference-api-url>")
model = Llama3InstructModel(client=client)
tracer = InMemoryTracer()

prompt = Prompt.from_text(text="What is the most common fish in swedish lakes?")
model.complete(CompleteInput(prompt=prompt, maximum_tokens=32), tracer)

# see trace in rich format
tracer._rich_render_()
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](https://github.com/Aleph-Alpha/pharia-inference-sdk/blob/main/CONTRIBUTING.md) for details on how to set up the development environment and submit changes.
