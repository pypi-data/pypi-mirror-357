{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. Make predictions using patient-level snapshots\n",
    "\n",
    "Now that the data have been prepared in snapshot form, we have a dataset of unfinished visits. The ultimate goal is to make predictions about whether an outcome of interest (admission, discharge) will happen within a prediction window. For now, I will use a simple outcome variable of admission or not, without worrying about when that admission happened. \n",
    "\n",
    "Everything shown here is standard modelling, but there are some important considerations when working with unfinished hospital visits.\n",
    "\n",
    "## Things to consider when training predictive models using snapshots\n",
    "\n",
    "**Random versus temporal splits**\n",
    "\n",
    "When dividing your data into training, validation and test sets, a random allocation will make your models appear to perform better than they actually would in practice. This is because random splits ignore the temporal nature of healthcare data, where patterns may change over time. A more realistic approach is to use temporal splits, where you train on earlier data and validate/test on later data, mimicking how the model would be deployed in a real-world setting.\n",
    "\n",
    "**Multiple snapshots per visit**\n",
    "\n",
    "To use `patientflow` your data should be in snapshot form. I showed how to create this in the last notebook. I defined a series of prediction times, and then sampled finished visit to get snapshots that represent those visits while still in progress. When you follow this method, you may end up with multiple snapshots. Is this OK, for your analysis? You will need to decide whether you include all snapshots from a single visit into a predictive model. These snapshots from the same visit are inherently correlated, which may violate assumptions of the statistical or machine learning methods you are using. \n",
    "\n",
    "**Multiple visits per patient**\n",
    "\n",
    "The patient identifier is also important, because if the same patient appears in training and test sets, there is the potential for data leakage. We took the decision to probabilistically allocate each patient to training, validation and test sets, where the probability of being allocated to each set is in proportion to the number of visits they made in any of those time periods. \n",
    "\n",
    "`patientflow` includes functions that handle of all these considerations. I demonstrate them here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload functions every time\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fake snapshots\n",
    "\n",
    "See the previous notebook for more information about how this is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>visit_number</th>\n",
       "      <th>is_admitted</th>\n",
       "      <th>age</th>\n",
       "      <th>latest_triage_score</th>\n",
       "      <th>num_bmp_orders</th>\n",
       "      <th>num_troponin_orders</th>\n",
       "      <th>num_cbc_orders</th>\n",
       "      <th>num_urinalysis_orders</th>\n",
       "      <th>num_d-dimer_orders</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snapshot_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(6, 0)</td>\n",
       "      <td>2690</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>2471</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>2987</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>3472</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            snapshot_date prediction_time  patient_id  visit_number  \\\n",
       "snapshot_id                                                           \n",
       "0              2023-01-01          (6, 0)        2690             6   \n",
       "1              2023-01-01         (9, 30)        2471            16   \n",
       "2              2023-01-01         (9, 30)        2987             9   \n",
       "3              2023-01-01         (9, 30)        3472            46   \n",
       "4              2023-01-01         (9, 30)          41            35   \n",
       "\n",
       "             is_admitted  age  latest_triage_score  num_bmp_orders  \\\n",
       "snapshot_id                                                          \n",
       "0                      0   49                  5.0               0   \n",
       "1                      0   76                  4.0               1   \n",
       "2                      0   58                  2.0               1   \n",
       "3                      0   63                  5.0               0   \n",
       "4                      0   83                  4.0               1   \n",
       "\n",
       "             num_troponin_orders  num_cbc_orders  num_urinalysis_orders  \\\n",
       "snapshot_id                                                               \n",
       "0                              0               0                      0   \n",
       "1                              1               0                      0   \n",
       "2                              1               1                      1   \n",
       "3                              0               1                      0   \n",
       "4                              0               0                      1   \n",
       "\n",
       "             num_d-dimer_orders  \n",
       "snapshot_id                      \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             1  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patientflow.generate import create_fake_snapshots\n",
    "prediction_times = [(6, 0), (9, 30), (12, 0), (15, 30), (22, 0)] \n",
    "snapshots_df=create_fake_snapshots(prediction_times=prediction_times, start_date='2023-01-01', end_date='2023-04-01')\n",
    "snapshots_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model to predict the outcome of each snapshot\n",
    "\n",
    "We decided in our work at UCLH to train a different model for each prediction time in the day. That was a design-led decision; we wanted each model to be able to pick up different signals of the outcome at different times of day. You'll see the results of this in later notebooks where I show shap plots for models at different times of day. \n",
    "\n",
    "For now, let's train a model to predict admission for the 9:30 prediction time. \n",
    "\n",
    "I will specify that the triage scores are ordinal, to make use of sklearn's OrdinalEncoder to maintain the natural order of categories. \n",
    "\n",
    "I exclude columns that are not relevant to the prediction of probability of admission, including `snapshot_date` and `prediction_time`.\n",
    "\n",
    "### Create temporal splits\n",
    "\n",
    "The `create_temporal_splits()` function below will randomly allocate each patient_id to training, validation and test sets, where the probability of being allocated to each is in proportion to the number of visits they made in any of those time periods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patient Set Overlaps (before random assignment):\n",
      "Train-Valid: 0 of 2158\n",
      "Valid-Test: 29 of 1513\n",
      "Train-Test: 100 of 2500\n",
      "All Sets: 0 of 3021 total patients\n",
      "Split sizes: [1802, 631, 1244]\n"
     ]
    }
   ],
   "source": [
    "from datetime import date   \n",
    "from patientflow.prepare import create_temporal_splits\n",
    "\n",
    "# set the temporal split\n",
    "start_training_set = date(2023, 1, 1) \n",
    "start_validation_set = date(2023, 2, 15) # 6 week training set \n",
    "start_test_set = date(2023, 3, 1) # 2 week validation set \n",
    "end_test_set = date(2023, 4, 1) # 1 month test set\n",
    "\n",
    "# create the temporal splits\n",
    "train_visits, valid_visits, test_visits = create_temporal_splits(\n",
    "    snapshots_df,\n",
    "    start_training_set,\n",
    "    start_validation_set,\n",
    "    start_test_set,\n",
    "    end_test_set,\n",
    "    col_name=\"snapshot_date\", # states which column contains the date to use when making the splits \n",
    "    patient_id=\"patient_id\", # states which column contains the patient id to use when making the splits \n",
    "    visit_col=\"visit_number\", # states which column contains the visit number to use when making the splits \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above returned information on the split sizes of each, and on how many patients were found in more than one set. In this case, 29 patients were found in the validation and test set periods. These patients will have been allocated to one set probabilistically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select one snapshot per visit\n",
    "\n",
    "You will need to decide whether you include all snapshots from a single visit into a predictive model.\n",
    "\n",
    "Since we train a different model for each prediction time, then any visits spanning more than 24 hours will have multiple rows. If your snapshots are drawn from visits to ED, this should hopefully not happen too often (though sadly it is becoming more common in the UK that people stay more than 24 hours). If your snapshots are drawn from inpatient visits, then it is very likely that you will have multiple rows per patient. \n",
    "\n",
    "We took the decision to select one visit at random, even for our ED visits. The function below gives you the option. If you specify `single_snapshot_per_visit` as True, the `train_classifier` function will expect a `visit_col` parameter. \n",
    "\n",
    "## Train a classifier to predict probability of admission\n",
    "\n",
    "Below I'm using `train_classifier()`, which is a wrapper on standard scikit-learn functions. There are a few parameters in this function to explain. \n",
    "\n",
    "- `grid`: specifies the grid to use in hyperparameter tuning.\n",
    "- `prediction_time`: is used to identify which patient snapshots to use for training.\n",
    "- `single_snapshot_per_visit`: if this is True, the function will randomly pick one snapshot for any visit, using `visit_col` as the column name that identifies the visit identifier. \n",
    "- `exclude_from_training_data`: certain columns in the data should not be used for training, including visit numbers and dates.\n",
    "- `ordinal_mappings`: the function makes use of SKLearn's Ordinal Mapping encoder.\n",
    "- `use_balanced_training`: in healthcare contexts, there are often fewer observations in the positive class. Set this to True for imbalanced samples (common for ED visits, when most patients are discharged, and for predicting inpatient discharge from hospital when most patients remain). It will downsample the negative class. \n",
    "- `calibrate_probabilities`: when you downsample the negative class, it is a good idea to calibrate the probabilities to account for this class imbalance. Setting this to True will use a sigmoid function to calibrate the predicted probabilities, ensuring they better reflect the probabilities in the original data distribution.\n",
    "- `calibration_method`: options are sigmoid or isotonic; I have found that sigmoid (the default) works better.\n",
    "\n",
    "By default, the function will use an XGBoost classifier, initialised with the hyperparameter grid provided, with log loss as the evaluation metric. Chronological cross-validation is used, with the best hyperparameters selected based on minimising log loss in the validation set. We chose XGBoost because it is quick to train, generally performs well, and handles missing values. \n",
    "\n",
    "If you wish to use a different classifier, you can use another argument:\n",
    "\n",
    "- `model_class` (not shown here):  You can pass your own model in an optional model_class argument, which expects classifier class (like XGBClassifier or other scikit-learn compatible classifiers) that can be instantiated and initialised with the parameters provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patientflow.train.classifiers import train_classifier\n",
    "\n",
    "# exclude columns that are not needed for training\n",
    "exclude_from_training_data=['patient_id', 'visit_number', 'snapshot_date', 'prediction_time']\n",
    "\n",
    "# train the patient-level model\n",
    "model = train_classifier(\n",
    "    train_visits,\n",
    "    valid_visits,\n",
    "    test_visits,\n",
    "    grid={\"n_estimators\": [20, 30, 40]},\n",
    "    prediction_time=(9, 30),\n",
    "    exclude_from_training_data=exclude_from_training_data,\n",
    "    ordinal_mappings={'latest_triage_score': [1, 2, 3, 4, 5]},\n",
    "    single_snapshot_per_visit=True,\n",
    "    visit_col='visit_number', # as we are using a single snapshot per visit, we need to specify which column contains the visit number\n",
    "    use_balanced_training=True,\n",
    "    calibrate_probabilities=True,\n",
    "    calibration_method='sigmoid'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the object returned by `train_classifier()`\n",
    "\n",
    "The function returns an object of type TrainedClassifer(). Meta data and metrics from the training process are returned with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object returned is of type: <class 'patientflow.model_artifacts.TrainedClassifier'>\n",
      "\n",
      "The metadata from the training process are returned in the `training_results` attribute:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResults(prediction_time=(9, 30), training_info={'cv_trials': [HyperParameterTrial(parameters={'n_estimators': 20}, cv_results={'train_auc': np.float64(0.9759582538006674), 'train_logloss': np.float64(0.2653240180144768), 'train_auprc': np.float64(0.9819403094340785), 'valid_auc': np.float64(0.6894655405868642), 'valid_logloss': np.float64(0.7745006485175), 'valid_auprc': np.float64(0.7046396258814147)}), HyperParameterTrial(parameters={'n_estimators': 30}, cv_results={'train_auc': np.float64(0.9845161689247697), 'train_logloss': np.float64(0.22865029363070716), 'train_auprc': np.float64(0.9884075328761277), 'valid_auc': np.float64(0.6960234557109557), 'valid_logloss': np.float64(0.8058855112351733), 'valid_auprc': np.float64(0.7067222494470633)}), HyperParameterTrial(parameters={'n_estimators': 40}, cv_results={'train_auc': np.float64(0.99099020571293), 'train_logloss': np.float64(0.20279548417803475), 'train_auprc': np.float64(0.9932432729843296), 'valid_auc': np.float64(0.6951122794688971), 'valid_logloss': np.float64(0.8493725817229816), 'valid_auprc': np.float64(0.6895395912326714)})], 'features': {'names': ['age', 'latest_triage_score', 'num_bmp_orders_0', 'num_bmp_orders_1', 'num_troponin_orders_0', 'num_troponin_orders_1', 'num_cbc_orders_0', 'num_cbc_orders_1', 'num_urinalysis_orders_0', 'num_urinalysis_orders_1', 'num_d-dimer_orders_0', 'num_d-dimer_orders_1'], 'importances': [0.12613233923912048, 0.33768123388290405, 0.15913592278957367, 0.0, 0.1018211618065834, 0.0, 0.13997837901115417, 0.0, 0.06779452413320541, 0.0, 0.06745646148920059, 0.0], 'has_importance_values': True}, 'dataset_info': {'train_valid_test_set_no': {'train_set_no': 300, 'valid_set_no': 104, 'test_set_no': 214}, 'train_valid_test_class_balance': {'y_train_class_balance': {0: 0.7133333333333334, 1: 0.2866666666666667}, 'y_valid_class_balance': {0: 0.6538461538461539, 1: 0.34615384615384615}, 'y_test_class_balance': {0: 0.6869158878504673, 1: 0.3130841121495327}}}}, calibration_info={'method': 'sigmoid'}, test_results={'test_auc': 0.7537821098588688, 'test_logloss': 0.5493430469872156, 'test_auprc': 0.6141122528416993}, balance_info={'is_balanced': True, 'original_size': 300, 'balanced_size': 172, 'original_positive_rate': np.float64(0.2866666666666667), 'balanced_positive_rate': np.float64(0.5), 'majority_to_minority_ratio': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Object returned is of type: {type(model)}')\n",
    "\n",
    "print(f'\\nThe metadata from the training process are returned in the `training_results` attribute:')\n",
    "model.training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better view of what is included within the results, here is a list of the fields returned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataclass fields in TrainingResults:\n",
      "prediction_time\n",
      "training_info\n",
      "calibration_info\n",
      "test_results\n",
      "balance_info\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "print(\"\\nDataclass fields in TrainingResults:\")\n",
    "for field in fields(model.training_results):\n",
    "    print(field.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction time has been saved with the model. This is used for validation at inference time, to make sure that the requested prediction time and model align. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction time is: (9, 30)\n"
     ]
    }
   ],
   "source": [
    "print(f'The prediction time is: {model.training_results.prediction_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object called training_info contains information related to model training. To simplify the code below, I'll assign it to a variable called results. It will tell us the size and class balance of each set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training_info object contains the following keys: dict_keys(['cv_trials', 'features', 'dataset_info'])\n",
      "\n",
      "Number in each set{'train_set_no': 300, 'valid_set_no': 104, 'test_set_no': 214}\n",
      "train: 71.3% neg, 28.7% pos\n",
      "valid: 65.4% neg, 34.6% pos\n",
      "test: 68.7% neg, 31.3% pos\n"
     ]
    }
   ],
   "source": [
    "results = model.training_results.training_info\n",
    "\n",
    "print(f\"The training_info object contains the following keys: {results.keys()}\")\n",
    "\n",
    "print(f\"\\nNumber in each set{results['dataset_info']['train_valid_test_set_no']}\")\n",
    "\n",
    "def print_class_balance(d):\n",
    "    for k in d:\n",
    "        print(f\"{k.split('_')[1]}: {d[k][0]:.1%} neg, {d[k][1]:.1%} pos\")\n",
    "\n",
    "\n",
    "print_class_balance(results['dataset_info']['train_valid_test_class_balance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class balance information is also saved in the training_results, which will store information about the differences between the class balance when forcing the training set to be balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_balanced': True,\n",
       " 'original_size': 300,\n",
       " 'balanced_size': 172,\n",
       " 'original_positive_rate': np.float64(0.2866666666666667),\n",
       " 'balanced_positive_rate': np.float64(0.5),\n",
       " 'majority_to_minority_ratio': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_results.balance_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the type of calibration done on balanced samples is saved in training_results also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'sigmoid'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_results.calibration_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of hyperparameter tuning are saved in a HyperParameterTrial object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HyperParameterTrial(parameters={'n_estimators': 20}, cv_results={'train_auc': np.float64(0.9759582538006674), 'train_logloss': np.float64(0.2653240180144768), 'train_auprc': np.float64(0.9819403094340785), 'valid_auc': np.float64(0.6894655405868642), 'valid_logloss': np.float64(0.7745006485175), 'valid_auprc': np.float64(0.7046396258814147)}),\n",
       " HyperParameterTrial(parameters={'n_estimators': 30}, cv_results={'train_auc': np.float64(0.9845161689247697), 'train_logloss': np.float64(0.22865029363070716), 'train_auprc': np.float64(0.9884075328761277), 'valid_auc': np.float64(0.6960234557109557), 'valid_logloss': np.float64(0.8058855112351733), 'valid_auprc': np.float64(0.7067222494470633)}),\n",
       " HyperParameterTrial(parameters={'n_estimators': 40}, cv_results={'train_auc': np.float64(0.99099020571293), 'train_logloss': np.float64(0.20279548417803475), 'train_auprc': np.float64(0.9932432729843296), 'valid_auc': np.float64(0.6951122794688971), 'valid_logloss': np.float64(0.8493725817229816), 'valid_auprc': np.float64(0.6895395912326714)})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results are stored in a HyperParameterTrial object\n",
    "results['cv_trials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the trial with the lowest validation logloss\n",
    "best_trial = min(results[\"cv_trials\"], key=lambda trial: trial.cv_results['valid_logloss'])\n",
    "\n",
    "# print the best parameters\n",
    "print(f'The best parameters are: {best_trial.parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results on the test set were:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_auc': 0.7537821098588688,\n",
       " 'test_logloss': 0.5493430469872156,\n",
       " 'test_auprc': 0.6141122528416993}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The results on the test set were:')\n",
    "model.training_results.test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each record in the snapshots dataframe is indexed by a unique snapshot_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here I have shown how `patientflow` can help you\n",
    "\n",
    "* handle multiple snapshots per visit and multiple visits per patient\n",
    "* impose a temporal split on your training and test sets, allowing for the point above \n",
    "* train a model to predict some later outcome using functions that handle class imbalance and calibration\n",
    "\n",
    "In the next notebook, I show how to evaluate models applied to patient snapshots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patientflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
