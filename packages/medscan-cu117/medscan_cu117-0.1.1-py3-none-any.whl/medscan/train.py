# train.py
# ----------------------------------------------------------------------------
# Script to run the medscan pipeline from the terminal using a single merged
# DataFrame (CSV) and optional filters. All comments and help texts are in English.
# Saves plots generated by evaluate() into a "plots" directory in the current working directory.
# ----------------------------------------------------------------------------

import argparse
import os
import pandas as pd
import torch
import warnings
import matplotlib.pyplot as plt

from medscan.pipeline import Pipeline
from medscan.data import Data
from medscan.config import PreprocessConfig, TrainConfig

warnings.filterwarnings("ignore", category=UserWarning)


def apply_filters(df: pd.DataFrame, filter_specs: list[str]) -> pd.DataFrame:
    """
    Apply simple equality filters to the DataFrame.
    Each filter_spec must be in the form "ColumnName=Value".
    Multiple filter_specs can be provided to chain multiple conditions (AND).
    """
    for spec in filter_specs:
        if "=" not in spec:
            raise ValueError(f"Filter spec '{spec}' is not in the form Column=Value.")
        col, val_str = spec.split("=", maxsplit=1)
        col = col.strip()
        val_str = val_str.strip()

        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame for filtering.")

        # Try to convert val_str to int or float if possible
        if val_str.isdigit():
            val = int(val_str)
        else:
            try:
                val = float(val_str)
            except ValueError:
                val = val_str  # keep as string

        df = df[df[col] == val]
        print(f"[INFO] Applied filter: {col} == {val}. Rows remaining: {len(df)}")

    return df


def main(args):
    # 1) Load the merged DataFrame (CSV) that already contains 'img_path' and all target columns
    df = pd.read_csv(args.data_path)
    print(f"[INFO] Loaded DataFrame '{args.data_path}' with {len(df)} rows.")

    # 2) Apply any requested filters (e.g., ColumnName=Value)
    if args.filter_column:
        df = apply_filters(df, args.filter_column)

    # 3) Determine target columns (either from --targets or use default list)
    if args.targets:
        targets = [t.strip() for t in args.targets.split(",") if t.strip()]
    else:
        targets = [
            "Neuro_Imaging", "Motion_Artefact", "Skull_Visibility", "Projection",
            "Contrast_fluid", "DSA", "Hemisphere", "ICA_Top_visible", "MCA_visible"
        ]
    print(f"[INFO] Using target columns: {targets}")

    # 4) Drop rows where any of the target columns are NaN
    df = df.dropna(subset=targets)
    print(f"[INFO] After dropping NaNs in targets, {len(df)} rows remain.")

    # 5) Split into train, validation, and test sets
    train_df, val_df, test_df = Data.split(
        data=df,
        train_frac=args.train_frac,
        val_frac=args.val_frac,
        seed=args.seed,
        shuffle=True,
        stratify=True,
        target_cols=targets,
        require_all_classes=True
    )

    # 6) Build PreprocessConfig from command-line arguments
    pre_cfg = PreprocessConfig(
        augment=args.augment,
        augment_factor=args.augment_factor,
        augment_methods=["elastic", "contrast", "contrast_elastic"],
        balance_on=args.balance_on,
        augmented_image_path=args.augmented_image_path,
        elastic_alpha=args.elastic_alpha,
        elastic_sigma=args.elastic_sigma,
        contrast_min=args.contrast_min,
        contrast_max=args.contrast_max,
    )

    # 7) Choose device (GPU or CPU)
    device = torch.device("cuda" if torch.cuda.is_available() and not args.force_cpu else "cpu")
    print(f"[INFO] Training device: {device}")

    # 8) Build TrainConfig from command-line arguments
    train_cfg = TrainConfig(
        input_size=args.input_size,
        batch_size=args.batch_size,
        epochs=args.epochs,
        early_stopping_patience=args.early_stopping_patience,
        device="cuda" if torch.cuda.is_available() and not args.force_cpu else "cpu",
        mixed_precision=args.mixed_precision,
        learning_rate=args.learning_rate,
        optimizer=args.optimizer,
        dropout=args.dropout,
        dropout_rate=args.dropout_rate,
        save_best_model=args.save_best_model,
        checkpoint_dir=args.checkpoint_dir,
        metric=args.metric,
        metric_mode=args.metric_mode,
        confidence_score=args.confidence_score,
        pretrained_models=args.pretrained_models.split(","),
        train_per_label=args.train_per_label
    )

    # 9) Instantiate Pipeline object and train
    model = Pipeline(
        preprocess_config=pre_cfg,
        train_config=train_cfg,
        targets=targets
    )
    model.fit(
        train_df=train_df,
        val_df=val_df,
    )

    # 10) Set up plot-saving: override plt.show() so that every figure
    #     opened by evaluate() is saved into a "plots" directory.
    plot_dir = os.path.join(os.getcwd(), "plots")
    os.makedirs(plot_dir, exist_ok=True)

    def save_show():
        """
        Custom replacement for plt.show() that saves all open figures into the 'plots' folder.
        Filenames follow the pattern: plot_<figure_number>.png
        """
        for fig_num in plt.get_fignums():
            fig = plt.figure(fig_num)
            fig_path = os.path.join(plot_dir, f"plot_{fig_num}.png")
            fig.savefig(fig_path)
            print(f"[INFO] Saved figure {fig_num} to '{fig_path}'")
        plt.close("all")

    plt.show = save_show

    # 11) Predict on test set and evaluate (plots will be saved instead of shown)
    predict_df = model.predict(test_df=test_df)
    results_df = model.evaluate(
        predict_df=predict_df,
        plots=args.plots.split(",") if args.plots else None,
        metrics=args.metrics.split(",") if args.metrics else None
    )

    print("\n[INFO] Evaluation results per target:")
    print(results_df)

    # 12) Optionally save the trained model
    if args.save_model_path:
        model.save(args.save_model_path)
        print(f"[INFO] Model saved to: {args.save_model_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=(
            "Train the medscan pipeline using a single merged DataFrame (CSV). "
            "You can optionally filter rows by specifying one or more Column=Value pairs. "
            "Plots generated by evaluate() will be saved into a 'plots' folder."
        )
    )

    # Required argument: path to the merged CSV
    parser.add_argument(
        "--data_path", type=str, required=True,
        help="Path to the CSV file containing the merged DataFrame (must include 'img_path' and target columns)."
    )

    # Optional filters
    parser.add_argument(
        "--filter_column", type=str, action="append",
        help=(
            "Filter condition in the form ColumnName=Value. "
            "Can be specified multiple times to apply multiple filters (AND)."
        )
    )

    # Targets (comma-separated)
    parser.add_argument(
        "--targets", type=str, default="",
        help=(
            "Comma-separated list of target column names. "
            "If omitted, the default set of targets will be used."
        )
    )

    # Split parameters
    parser.add_argument(
        "--train_frac", type=float, default=0.7,
        help="Fraction of data to use for training (default: 0.7)."
    )
    parser.add_argument(
        "--val_frac", type=float, default=0.15,
        help="Fraction of data to use for validation (default: 0.15)."
    )
    parser.add_argument(
        "--seed", type=int, default=123,
        help="Random seed for reproducible splits (default: 123)."
    )

    # PreprocessConfig parameters
    parser.add_argument(
        "--augment", action="store_true",
        help="Enable data augmentation (default: False)."
    )
    parser.add_argument(
        "--augment_factor", type=int, default=2,
        help="Number of augmented samples per original sample (default: 2)."
    )
    parser.add_argument(
        "--balance_on", action="store_true",
        help="Enable class balancing (default: False)."
    )
    parser.add_argument(
        "--augmented_image_path", type=str, default="augmented_images",
        help="Directory to save augmented images (default: 'augmented_images')."
    )
    parser.add_argument(
        "--elastic_alpha", type=float, default=34.0,
        help="Alpha parameter for elastic transform (default: 34.0)."
    )
    parser.add_argument(
        "--elastic_sigma", type=float, default=4.0,
        help="Sigma parameter for elastic transform (default: 4.0)."
    )
    parser.add_argument(
        "--contrast_min", type=float, default=0.4,
        help="Minimum contrast factor for augmentation (default: 0.4)."
    )
    parser.add_argument(
        "--contrast_max", type=float, default=0.9,
        help="Maximum contrast factor for augmentation (default: 0.9)."
    )

    # TrainConfig parameters
    parser.add_argument(
        "--input_size", type=int, default=224,
        help="Input image size (default: 224)."
    )
    parser.add_argument(
        "--batch_size", type=int, default=32,
        help="Batch size for DataLoader (default: 32)."
    )
    parser.add_argument(
        "--epochs", type=int, default=10,
        help="Number of training epochs (default: 10)."
    )
    parser.add_argument(
        "--early_stopping_patience", type=int, default=3,
        help="Patience for early stopping (default: 3)."
    )
    parser.add_argument(
        "--learning_rate", type=float, default=1e-3,
        help="Learning rate (default: 1e-3)."
    )
    parser.add_argument(
        "--optimizer", type=str, default="Adam",
        help="Optimizer name (e.g., 'AdamW' or 'Adam', default: 'Adam')."
    )
    parser.add_argument(
        "--dropout", action="store_true",
        help="Enable dropout (default: False)."
    )
    parser.add_argument(
        "--dropout_rate", type=float, default=0.0,
        help="Dropout rate if --dropout is set (default: 0.0)."
    )
    parser.add_argument(
        "--mixed_precision", action="store_true",
        help="Enable mixed-precision training (default: False)."
    )
    parser.add_argument(
        "--save_best_model", action="store_true",
        help="Save the best model during training (default: False)."
    )
    parser.add_argument(
        "--checkpoint_dir", type=str, default="checkpoints",
        help="Directory to save checkpoint files (default: 'checkpoints')."
    )
    parser.add_argument(
        "--metric", type=str, default="val_loss",
        help="Metric to monitor (e.g., 'val_loss' or 'val_auc', default: 'val_loss')."
    )
    parser.add_argument(
        "--metric_mode", type=str, choices=["min", "max"], default="min",
        help="Mode for the monitored metric (min or max, default: min)."
    )
    parser.add_argument(
        "--confidence_score", action="store_true",
        help="Compute confidence scores (default: False)."
    )
    parser.add_argument(
        "--pretrained_models", type=str, default="resnet34",
        help="Comma-separated list of pretrained backbone names (default: 'resnet34')."
    )
    parser.add_argument(
        "--train_per_label", action="store_true",
        help="Train each target separately (SingleHead per label). If omitted, a MultiHead model is used."
    )

    # Additional flags
    parser.add_argument(
        "--force_cpu", action="store_true",
        help="Force CPU training even if a GPU is available."
    )
    parser.add_argument(
        "--save_model_path", type=str, default="",
        help="Path to save the trained model state_dict. Leave empty to skip saving."
    )

    # Optional: plots & metrics for evaluate()
    parser.add_argument(
        "--plots", type=str, default="",
        help=(
            "Comma-separated list of plots to generate in evaluate(), "
            "choices: 'confusion_matrix', 'loss_vs_epoch', 'lr_vs_epoch'."
        )
    )
    parser.add_argument(
        "--metrics", type=str, default="",
        help="Comma-separated list of metrics to compute in evaluate(), e.g. 'AUC,accuracy,F1'."
    )

    args = parser.parse_args()
    main(args)
