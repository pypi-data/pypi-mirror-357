# -*- coding: utf-8 -*-
from setuptools import setup

package_dir = \
{'': 'src'}

packages = \
['delos']

package_data = \
{'': ['*']}

modules = \
['py']
install_requires = \
['aiofiles>=24.1.0,<25.0.0',
 'aiohttp>=3.11.13,<4.0.0',
 'fastapi>=0.115.5,<0.116.0',
 'loguru>=0.7.2,<0.8.0',
 'pydantic>=2.10.5,<3.0.0',
 'requests>=2.32.3,<3.0.0']

setup_kwargs = {
    'name': 'delos',
    'version': '0.3.12',
    'description': 'Delos client.',
    'long_description': '# Delos Delos\n\n## Delos client for interacting with the Delos API.\n\n# Installation\n\nTo install the package, use `pip`:\n\n```bash\npip install delos\n```\n\n# Client Initialization\n\nYou can create an **API key** to access all services through the **Dashboard** in **DelosPlatform**\n`https://platform.api.delos.so`.\n\n![API Key creation in Delos Platform](https://i.ibb.co/6mvm1hQ/api-key-create.png)\n\nTo create a `Delos` client instance, you need to initialize it with your API key:\n\n```python\nfrom delos import DelosClient\n\nclient = DelosClient(api_key="your-delos-api-key")\n\n```\n\n> # Documentation\n>\n> The extended documentation of the **Delos API** can be found in [Delos API Platform](https://platform.api.delos.so/docs).\n> API Reference, documentation and detailed examples that are updated regularly.\n\n# Endpoints\n\nThis `delos` client provides access to the following endpoints:\n\n**Status Endpoints**\n\n- `status_health`: Check the health of the server.\n\n**Translate Endpoints**\n\n- `translate_text`: Translate text.\n- `translate_file`: Translate a file.\n\n**Web Endpoints**\n\n- `web_search`: Perform a web search.\n\n**LLM Endpoints**\n\n- `chat`: Chat with the LLM.\n- `embed`: Embed data into the LLM.\n\n**Files Index Endpoints**\n\nAn **index** groups a set of files in order to be able to query them using natural language. There are several\noperations regarding **index management**:\n\n- `files_index_create`: Create an index.\n- `files_index_files_add`: Add files to an index.\n- `files_index_retry`: Retry failed files uploads to an index.\n- `files_index_files_delete`: Delete files from an index.\n- `files_index_delete`: Delete an index.\n- `files_index_restore`: Restore a deleted index.\n- `files_index_rename`: Rename an index.\n\nAnd regarding **index querying**\n\n- `files_index_ask`: Ask a question about the index documents (it requires that your `index.status.vectorized` is set to\n  `True`).\n- `files_index_embed`: Embed or vectorize the index contents.\n- `files_index_list`: List all indexes.\n- `files_index_details`: Get details of an index.\n\nRegarding **index tags**:\n- `files_index_tags_get`: Get the tags of an index.\n- `files_index_tags_update`: Update the tags of an index.\n- `files_index_files_tags_update`: Update the tags of a list of files in an index.\n\nThese endpoints are accessible through `delos` client methods.\n\n> ℹ️ **Info:** For all the **endpoints**, there are specific **parameters** that are required regarding the data to be\n> sent to the API.\n>\n> Endpoints may expect `text` or `files` to operate with, the `output_language` for your result, the `index_uuid` that\n> identifies the set of documents, the `model` to use for the LLM operations, etc.\n>\n> You can find the standardized parameters like the `return_type` for file translation and the `extract_type` for file\n> parser in the appropiate endpoint.\n\n---\n\n## Status Endpoints\n\n### Status Health Request\n\nTo **check the health** of the server and the validity of your API key:\n\n```python\nresponse = client.status_health()\nif response:\n    print(f"Response: {response}")\n```\n\n---\n\n## Translate Endpoints\n\n### 1. Translate Text Request\n\nTo **translate text**, you can use the `translate_text` method:\n\n```python\nresponse = client.translate_text(\n                        text="Hello, world!",\n                        output_language="fr"\n                    )\nif response:\n    print(f"Translated Text: {response}")\n```\n\n### 2. Translate File Request\n\nTo **translate a file**, use the `translate_file` method:\n\n```python\nlocal_filepath_1 = Path("/path/to/file1.pdf")\n\nresponse = client.translate_file(\n                        filepath=local_filepath_1,\n                        output_language="fr",\n                    )\n```\n\nAccording to the type of file translation you prefer, you can choose the `return_type` parameter to:\n\n| return_type        |                                                     |\n| ------------------ | --------------------------------------------------- |\n| raw_text `Default` | Returns the translated text only                    |\n| url                | Return the translated file with its layout as a URL |\n\n> 💡 **Tip:** For faster and economical translations, set the `return_type` to `raw_text` to request to translate only\n> the **text content**, without the file layout.\n\n```python\nlocal_filepath_1 = Path("/path/to/file1.pdf")\nlocal_filepath_2 = Path("/path/to/file2.pdf")\n\n# Set return_type=\'raw_text\' -> only the translated text will be returned:\nresponse = client.translate_file(\n                        filepath=local_filepath_1,\n                        output_language="fr",\n                        return_type="raw_text"\n                    )\n\n# or return_type=\'url\' -> returns a link to translated file with original file\'s layout:\nresponse = client.translate_file(\n                        filepath=local_filepath_2,\n                        output_language="fr",\n                        return_type="url"\n                    )\n\nif response:\n    print(f"Translated File Response: {response}")\n```\n\n---\n\n## Web Endpoints\n\n### Web Search Request\n\nTo perform a **web search**:\n\n```python\nresponse = client.web_search(text="What is the capital of France?")\n\n# Or, if you want to specify the output_language and filter results\nresponse = client.web_search(\n                        text="What is the capital of France?",\n                        output_language="fr",\n                        desired_urls=["wikipedia.fr"]\n                    )\nif response:\n    print(f"Search Results: {response}")\n```\n\n---\n\n## LLM Endpoints\n\nLLM Endpoints provide a way to interact with several Large Language Models and Embedders in an unified way. Currently\nsupported `model`s are:\n\n| Chat Models          | Embedding Models       |\n| -------------------- | ---------------------- |\n| _gpt-3.5_ `Legacy`   | ada-v2                 |\n| gpt-4o               | text-embedding-3-large |\n| gpt-4o-mini          |                        |\n| command-r            |                        |\n| command-r-plus       |                        |\n| llama-3-70b-instruct |                        |\n| mistral-large        |                        |\n| mistral-small        |                        |\n| claude-3.5-sonnet    |                        |\n| claude-3-haiku       |                        |\n\n### 1. Chat Request\n\nTo **chat** with the LLM:\n\n```python\nresponse = client.llm_chat(text="Hello, how are you?")\n\n# Default model is handled, so that request is equivalent to:\nresponse = client.llm_chat(\n                        text="Hello, how are you?",\n                        model="gpt-4o-mini"\n                    )\nif response:\n    print(f"Chat Response: {response}")\n```\n\nThe list of **previous messages** can be provided through the `messages` parameter:\n\n```python\nresponse = client.llm_chat(\n                        text="What about uk?",\n                        messages=[\n                            {"role": "system", "content": "You are a helpful assistant."},\n                            {"role": "user", "content": "what is the capital city of spain?"},\n                            {"role": "assistant", "content": "The capital city of Spain is Madrid."},\n                        ],\n                        model="gpt-4o-mini",\n                    )\nif response:\n    print(f"Chat Response: {response}")\n```\n\n**Custom arguments** can be provided to the request, such as the dictionary `response_format` for the `chat` endpoint,\nor the `temperature` (in between 0 and 1):\n\n```python\nresponse = client.llm_chat(\n                        text="Hello, how are you? Respond in JSON format.",\n                        model="gpt-4o-mini",\n                        temperature=0.5,\n                        response_format={"type":"json_object"}\n                    )\nif response:\n    print(f"Chat Response: {response}")\n```\n\n### 2. Chat Stream\n\nIt is also possible to **stream** the response of the chat request:\n\n```python\nresponse = client.llm_chat_stream(text="Hello, how are you?")\n\n# Default model is handled, so that request is equivalent to:\nresponse = client.llm_chat_stream(\n                        text="Hello, how are you?",\n                        model="gpt-4o-mini"\n                    )\nif response:\n    print(f"Chat Response: {response}")\n\n```\n\nThe list of **previous messages** can be provided through the `messages` parameter as in the `chat` request (previous\nsection), as well as the `temperature` (in between 0 and 1) and other parameters:\n\n```python\nresponse = client.llm_chat_stream(\n                        text="What about uk?",\n                        messages=[\n                            {"role": "system", "content": "You are a helpful assistant."},\n                            {"role": "user", "content": "what is the capital city of spain?"},\n                            {"role": "assistant", "content": "The capital city of Spain is Madrid."},\n                        ],\n                        model="gpt-4o-mini",\n                        temperature=0.5,\n                    )\nif response:\n    print(f"Chat Response: {response}")\n```\n\nThe **response** in this case is a StreamingResponse, containing a generator which responses are similar to the\nfollowing, in order to keep compatibility with data stream protocols:\n\n```\n0: "\n\n0:"The"\n\n0:" capital"\n\n0:" city"\n\n0:" of"\n\n0:" the"\n\n0:" United"\n\n0:" Kingdom"\n\n0:" is"\n\n0:" London"\n\n0:"."\n\n2: "{\'id\': \'572a1c1e-ccc8-43bd-b4f1-3138016f7251\', \'choices\': [{\'delta\': {}, \'finish_reason\': \'stop\'}], \'request_id\': \'572a1c1e-ccc8-43bd-b4f1-3138016f7251\', \'response_id\': \'a1086bb8-16f7-4ca2-b27f-91bc8063e615\', \'status_code\': \'200\', \'status\': \'success\', \'message\': \'Chat response received.\\n(All the 3 previous `messages` have been read.)\', \'timestamp\': \'2025-02-14T09:18:32.032696+00:00\', \'cost\': \'1.8e-05\'}"\n\n```\n\n### 3. Embed Request\n\nTo **embed** some text using a LLM, using `ada-v2` model:\n\n```python\nresponse = client.llm_embed(text="Hello, how are you?", model="ada-v2")\nif response:\n    print(f"Embed Response: {response}")\n```\n\nOr using `text-embedding-ada-002` model:\n\n```python\nresponse = client.llm_embed(text="Hello, how are you?", model="text-embedding-ada-002")\nif response:\n    print(f"Embed Response: {response}")\n```\n\n\n### Files Index\n\nIndex group a set of files in order to be able to query them using natural language. The **Index attributes** are:\n\n| Attributes | Meaning                                                                                                                                        |\n| ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| index_uuid | Unique identifier of the index. It is randomly generated when the index is created and cannot be altered.                                      |\n| name       | Human-friendly name for the index, can be modified through the `rename_index` endpoint.                                                        |\n| created_at | Creation date                                                                                                                                  |\n| updated_at | Last operation performed in index                                                                                                              |\n| expires_at | Expiration date of the index. It will only be set once the `delete_index` request is explictly performed. (Default: None)                      |\n| status     | Status of the index. It will be `active`, and only when programmed for deletion it will be `countdown` (2h timeout before effective deletion). |\n| vectorized | Boolean status of the index. When `True`, the index is ready to be queried.                                                                    |\n| files      | List of files in the index. Contains their filehash, filename and size                                                                         |\n| storage    | Storage details of the index: total size in bytes and MB, number of files.                                                                     |\n|            |\n\nThe following **Index operations** are available:\n\n- `INDEX_LIST`: List all indexes.\n- `INDEX_DETAILS`: Get details of an index.\n- `INDEX_CREATE`: Create a new index and parse files.\n- `INDEX_ADD_FILES`: Add files to an existing index.\n- `INDEX_DELETE_FILES`: Delete files from an index.\n- `INDEX_RETRY`: Retry failed files uploads to an index.\n- `INDEX_DELETE`: Delete an index. **Warning**: _This is a delayed (2h) operation, allowed to be reverted with\n  `INDEX_RESTORE`. After 2h, the index will be **deleted and not recoverable**._\n- `INDEX_RESTORE`: Restore a deleted index _(within the 2h after it was marked for deletion)_.\n- `INDEX_EMBED`: Embed index contents.\n- `INDEX_ASK`: Ask a question to the index. It requires that `INDEX_EMBED` is performed to allow index contents\n  querying.\n- `INDEX_TAGS_GET`: Get the tags of an index.\n- `INDEX_TAGS_UPDATE`: Update the tags of an index.\n- `INDEX_FILES_TAGS_UPDATE`: Update the tags of a list of files in an index.\n\n### Files Index Requests\n\n#### 1. Existing Index Overview\n\nTo **list all indexes** in your organization, files included and storage details:\n\n```python\nresponse = client.files_index_list()\nif response:\n    print(f"List Indexes Response: {response}")\n```\n\nWith **get details** of an index you can see the list of files in the index, their filehashes, their size, the `status`\nof the index and the `vectorized` boolean status (find more details about the Index fields above):\n\n```python\nresponse = client.files_index_details(index_uuid="index-uuid")\nif response:\n    print(f"Index Details Response: {response}")\n```\n\n#### 2. Index Management\n\nTo **create a new index** and parse files, provide the list of **filepaths** you want to parse:\n\n```python\nlocal_filepaths = [Path("/path/to/file1.docx"), Path("/path/to/file2.pdf")]\n\nresponse = client.files_index_create(\n                        filepaths=local_filepaths,\n                        name="Cooking Recipes"\n                        read_images=True, # read images from the files, default is False\n                    )\nif response:\n    print(f"Index Create Response: {response}")\n```\n\nLet\'s say the new index has been created with the UUID `d55a285b-0a0d-4ba5-a918-857f63bc9063`. This UUID will be used in\nthe following requests, particularly in the `index_details` whenever some information about the index is needed.\n\nYou can **rename the index** with the `rename_index` method:\n\n```python\nindex_uuid = "d55a285b-0a0d-4ba5-a918-857f63bc9063"\nresponse = client.files_index_rename(\n                        index_uuid=index_uuid,\n                        name="Best Recipes"\n                    )\nif response:\n    print(f"Rename Index Response: {response}")\n```\n\nTo **add files** to an existing index, provide the list of **filepaths** you want to add:\n\n```python\nindex_uuid = "d55a285b-0a0d-4ba5-a918-857f63bc9063"\nlocal_filepath_3 = [Path("/path/to/file3.txt")]\n\nresponse = client.files_index_files_add(\n                        index_uuid=index_uuid,\n                        filepaths=local_filepath_3\n                        read_images=True, # read images from the files, default is False\n                    )\nif response:\n    print(f"Add Files to Index Response: {response}")\n```\n\nTo **delete files** from an existing index, specify the **filehashes** of the files you want to delete. You can see\nfilehashes contained in an index by requesting the index details. See a file deletion example below:\n\n```python\nindex_uuid = "d55a285b-0a0d-4ba5-a918-857f63bc9063"\nfilehashes_to_delete = ["2fa92ab4627c199a2827a363469bf4e513c67b758c34d1e316c2968ed68b9634"]\n\nresponse = client.files_index_files_delete(\n                        index_uuid=index_uuid,\n                        files_hashes=filehashes_to_delete\n                    )\nif response:\n    print(f"Delete Files from Index Response: {response}")\n```\n\nTo **delete an index** (it will be marked for deletion which will become effective **after 2h**):\n\n```python\nresponse = client.files_index_delete(index_uuid="index-to-delete-uuid")\nif response:\n    print(f"Delete Index Response: {response}")\n```\n\nTo **restore an index** marked for deletion (only possible during the 2h after the `INDEX_DELETE` was requested):\n\n```python\nresponse = client.files_index_restore(index_uuid="index-to-restore-uuid")\nif response:\n    print(f"Restore Index Response: {response}")\n```\n\n#### 3. Index Querying\n\nTo **embed** or **vectorize index contents** in order to allow the query operations:\n\n```python\nresponse = client.files_index_embed(index_uuid="index-uuid", run_in_background=True)\nif response:\n    print(f"Embed Index Response: {response}")\n```\n\nTo **ask a question** about the index documents (it requires that your `index.status.vectorized` is set to `True`):\n\n```python\nresponse = client.files_index_ask(\n                        index_uuid="index-uuid",\n                        question="What is Delos?"\n                    )\nif response:\n    print(f"Ask Index Response: {response}")\n```\n\nIndex files can be filtered for querying by using any of these options:\n- Providing the `active_files` parameter with the list of `file_id` to be used in the research\n- Selecting by `tags`\n\nFor example, to ask the question using all the files in the index that match any of the `tags`:\n\n```python\nresponse = client.files_index_ask(\n                        index_uuid="index-uuid",\n                        question="What is Delos?"\n                        tags=["2024", "2023"]\n                    )\nif response:\n    print(f"Ask Index Response: {response}")\n```\n\nOr only on some of the files:\n\n```python\nresponse = client.files_index_ask(\n                        index_uuid="index-uuid",\n                        question="What is Delos?",\n                        active_files=["abc123def456", "789ghi012jkl"]\n                    )\nif response:\n    print(f"Ask Index Response: {response}")\n```\nBoth filters can be combined.\n\n> **Hint:** You can find the `file_id` of the files contained in the index by using the `files_index_details` method.\n\n\n## Requests Usage and Storage\n\nAll request responses show the **number of tokens** and **cost** consumed by the request. The **storage** for index\ndocuments is **limited** up to your organization\'s quota and is shared between all indexes within your organization.\nContents **do not expire**, but they can be deleted by performing an explicit request through the API endpoints or\nthrough the **Delos Platform** at `https://platform.api.delos.so/`.\n\nIn the **Delos Platform**, you can monitor the requests performed by your organization with your API Key and the files\nstored in the Index Storage.\n\n![API key usage in Delos Platform](https://i.ibb.co/VTD35z1/api-key-usage.png)\n\nThrough both the native requests towards Delos and the Python client, you can handle and delete files directly from the\nDelos Platform.\n',
    'author': 'Maria',
    'author_email': 'mariaibanez@delosintelligence.fr',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'package_dir': package_dir,
    'packages': packages,
    'package_data': package_data,
    'py_modules': modules,
    'install_requires': install_requires,
    'python_requires': '>=3.11,<4.0',
}


setup(**setup_kwargs)
