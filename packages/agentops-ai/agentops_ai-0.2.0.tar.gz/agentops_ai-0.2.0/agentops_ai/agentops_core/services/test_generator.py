import ast
import os
from typing import Dict, Any, Optional
from dotenv import load_dotenv
import re
from openai import OpenAI
from pathlib import Path

load_dotenv()


class TestGenerator:
    """Service for generating tests using OpenAI API."""

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        """Initialize the test generator.
        Args:
            api_key: OpenAI API key (defaults to env var)
            model: OpenAI model to use
        """
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self.model = model or os.environ.get("AGENTOPS_MODEL", "gpt-4o-mini")
        self.client = OpenAI(api_key=self.api_key)

    def parse_code(self, code: str) -> Dict[str, Any]:
        """Parse Python code to extract structure.

        Args:
            code: Python code as string

        Returns:
            Dict containing code structure
        """
        try:
            ast.parse(code)
            # Extract classes, functions, etc.
            # Implementation here
            return {"success": True, "structure": {}}
        except SyntaxError as e:
            return {"success": False, "error": str(e)}

    def generate_tests(self, target: str, framework: str = "pytest") -> dict:
        """Generate tests for the target file and return a result dictionary."""
        try:
            with open(target, "r") as f:
                code = f.read()

            structure = self._analyze_code(code)
            prompt = self._create_prompt(code, structure, framework)
            response = self._generate_test_code(prompt)

            match = re.search(r"```python\n(.*?)```", response, re.DOTALL)
            code_to_process = match.group(1) if match else response

            fixed_code, error = self.auto_fix_syntax_errors(code_to_process)

            if error:
                return {"code": fixed_code, "success": False, "error": str(error)}
            else:
                return {"code": fixed_code, "success": True, "error": None}

        except Exception as e:
            return {
                "code": None,
                "success": False,
                "error": f"An unexpected error occurred: {str(e)}",
            }

    def _create_prompt(
        self, code: str, structure: Dict[str, Any], framework: str
    ) -> str:
        """Create a prompt for the OpenAI API that guarantees only valid Python code in a single code block, no prose or markdown outside."""
        # Process numeric literals in the code before creating the prompt
        code = self._validate_numeric_literals(code)

        summary = []
        if structure:
            functions = structure.get("functions", [])
            if functions:
                summary.append("Functions:")
                for f in functions:
                    summary.append(
                        f"- {getattr(f, 'name', str(f))}({', '.join(p['name'] for p in getattr(f, 'parameters', []))})"
                    )
        summary_text = "\n".join(summary)

        # Check if this is a Pydantic model file
        is_pydantic = (
            "from pydantic import" in code or "class" in code and "BaseModel" in code
        )

        if is_pydantic:
            prompt = (
                "def example(): pass\n\n"
                "You are AgentOps QA Agent. You must output only valid Python code inside a single ```python code block. "
                "Do not explain, comment, or output anything else outside the block. No markdown, no prose, no TODOs.\n\n"
                "Start output with:\n```python\n\n"
                "End output with:\n```\n\n"
                "Your task is to generate a complete, ready-to-run pytest test file for the Pydantic model(s) below. "
                "The file must:\n"
                "- Import the model class directly from the module\n"
                "- Test model creation with valid data\n"
                "- Test model validation with invalid data\n"
                "- Test default values\n"
                "- Test optional fields\n"
                "- Use idiomatic pytest\n"
                "- Use assert statements with helpful error messages\n"
                "- Add `@pytest.mark.generated_by_agentops` above every test\n"
                "- Include a Python docstring at the top with this fingerprint block:\n"
                '"""\n'
                "Auto-generated by AgentOps QA Agent v0.4\n"
                "Date: 2025-05-24\n"
                "Target: Pydantic model(s) (see below)\n"
                "LLM Confidence Score: 86%\n"
                "Generation Prompt Hash: a9f7e2c3\n"
                "Regeneration Policy: Re-evaluate on diff; regenerate if confidence < 70%\n"
                '"""\n\n'
                "Important:\n"
                "- Do not include any non-Python content.\n"
                "- Do not wrap your output in markdown.\n"
                "- Assume temperature=0.0 and sufficient max_tokens.\n"
                "- The output should be a single test file stored under `.agentops/tests/<mirrored path>/`.\n"
                "- When using numeric values in tests:\n"
                "  * Always use explicit decimal points for floats (e.g., 1.0 instead of 1.)\n"
                "  * Use proper scientific notation (e.g., 1e-6 instead of 1.e-6)\n"
                "  * Ensure negative numbers have no space after the minus sign\n"
                "  * Use proper numeric literals for all test values\n"
                "  * Use proper numeric literals in JSON/dict values\n\n"
                f"# Code summary:\n{summary_text}\n\n# Full code to test:\n{code}"
            )
        else:
            prompt = (
                "def example(): pass\n\n"
                "You are AgentOps QA Agent. You must output only valid Python code inside a single ```python code block. "
                "Do not explain, comment, or output anything else outside the block. No markdown, no prose, no TODOs.\n\n"
                "Start output with:\n```python\n\n"
                "End output with:\n```\n\n"
                "Your task is to generate a complete, ready-to-run pytest test file for all public functions and methods in the code below. "
                "The file must:\n"
                "- Use idiomatic pytest, with @pytest.mark.parametrize where appropriate\n"
                "- Use assert statements with helpful error messages\n"
                "- Add `@pytest.mark.generated_by_agentops` above every test\n"
                "- Include a Python docstring at the top with this fingerprint block:\n"
                '"""\n'
                "Auto-generated by AgentOps QA Agent v0.4\n"
                "Date: 2025-05-24\n"
                "Target: auto-discovered functions (see below)\n"
                "LLM Confidence Score: 86%\n"
                "Generation Prompt Hash: a9f7e2c3\n"
                "Regeneration Policy: Re-evaluate on diff; regenerate if confidence < 70%\n"
                '"""\n\n'
                "Important:\n"
                "- Do not include any non-Python content.\n"
                "- Do not wrap your output in markdown.\n"
                "- Assume temperature=0.0 and sufficient max_tokens.\n"
                "- The output should be a single test file stored under `.agentops/tests/<mirrored path>/`.\n"
                "- When using numeric values in tests:\n"
                "  * Always use explicit decimal points for floats (e.g., 1.0 instead of 1.)\n"
                "  * Use proper scientific notation (e.g., 1e-6 instead of 1.e-6)\n"
                "  * Ensure negative numbers have no space after the minus sign\n"
                "  * Use proper numeric literals for all test values\n\n"
                f"# Code summary:\n{summary_text}\n\n# Full code to test:\n{code}"
            )
        return prompt

    def _load_prompt(self, prompt_file: str) -> str:
        """Load a prompt from the prompts directory.

        Args:
            prompt_file: Name of the prompt file

        Returns:
            Prompt content as string
        """
        prompt_path = Path(__file__).parent.parent.parent / "prompts" / prompt_file
        try:
            with open(prompt_path, "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            # Fallback to hardcoded prompt if file not found
            return "You are an expert test engineer. Your task is to generate comprehensive tests for the provided code."

    def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """Call the OpenAI API with the given prompt."""
        try:
            # Load system prompt from file
            system_prompt = self._load_prompt("test_generation.txt")

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ],
            )
            # No debug print of the raw response
            return {"success": True, "data": response}
        except Exception as e:
            print("[AgentOps DEBUG] OpenAI API Exception:", e)
            return {"success": False, "error": str(e)}

    def _process_response(
        self, response: Dict[str, Any], framework: str
    ) -> Dict[str, Any]:
        """Process the OpenAI API response."""
        if not response["success"]:
            print("[AgentOps DEBUG] OpenAI response not successful:", response)
            return {"success": False, "error": response["error"]}
        data = response["data"]
        # Extract the test code from the OpenAI response
        try:
            raw_content = data.choices[0].message.content
            # Extract code from ```python ... ``` block
            match = re.search(r"```python(.*?)```", raw_content, re.DOTALL)
            if match:
                test_code = match.group(1).strip()
            else:
                test_code = raw_content.strip()
            confidence = 1.0 if test_code else 0.0
            return {"success": True, "tests": test_code, "confidence": confidence}
        except Exception as e:
            print("[AgentOps DEBUG] Failed to parse OpenAI response:", e)
            return {"success": False, "error": f"Failed to parse OpenAI response: {e}"}

    def write_tests_to_file(
        self,
        test_code: str,
        output_dir: str = "tests",
        base_name: str = "test_generated.py",
    ) -> str:
        """Write the generated test code to a file and return the file path."""
        os.makedirs(output_dir, exist_ok=True)
        file_path = os.path.join(output_dir, base_name)
        with open(file_path, "w") as f:
            f.write(test_code)
        return file_path

    def _dedupe_and_group_imports(self, code):
        lines = code.split("\n")
        import_lines = [
            line
            for line in lines
            if line.strip().startswith("import ") or line.strip().startswith("from ")
        ]
        import_lines = list(dict.fromkeys(import_lines))
        non_import_lines = [line for line in lines if line not in import_lines]
        return "\n".join(import_lines + [""] + non_import_lines)

    def _decorate_tests(self, code):
        lines = code.split("\n")
        out = []
        for i, line in enumerate(lines):
            if line.strip().startswith("def test_"):
                out.append("@pytest.mark.generated_by_agentops")
            out.append(line)
        return "\n".join(out)

    def _add_risk_comments(self, code):
        lines = code.split("\n")
        out = []
        for i, line in enumerate(lines):
            if line.strip().startswith("def test_"):
                out.append("# Risk: medium | Importance: auto-generated by AgentOps")
            out.append(line)
        return "\n".join(out)

    def _add_assertion_messages(self, code):
        import re

        def add_msg(match):
            assertion = match.group(0)
            if "assert " in assertion and "," not in assertion:
                expr = assertion[len("assert ") :].strip()
                return f"assert {expr}, 'Assertion failed: {expr}'"
            return assertion

        return re.sub(r"assert [^,\n]+", add_msg, code)

    def _validate_numeric_literals(self, code: str) -> str:
        """Process and validate numeric literals in the code to ensure they are valid Python literals."""
        import re

        def process_numeric(match):
            num = match.group(0)
            # Handle scientific notation
            if "e" in num.lower():
                base, exp = num.lower().split("e")
                # Ensure base has decimal point if it's a float
                if "." not in base and "e" not in base:
                    base = f"{base}.0"
                return f"{base}e{exp}"

            # Handle decimal numbers
            if "." in num:
                # Remove leading zeros before decimal point
                parts = num.split(".")
                if parts[0].startswith("0") and len(parts[0]) > 1:
                    parts[0] = parts[0].lstrip("0") or "0"
                # Ensure decimal part exists
                if len(parts) == 1:
                    parts.append("0")
                return ".".join(parts)

            # Handle integers
            if num.startswith("0") and len(num) > 1:
                return num.lstrip("0") or "0"

            return num

        # Match numeric literals in the code
        # This pattern matches:
        # - Integers (e.g., 123, 0, -123)
        # - Floats (e.g., 123.456, 0.0, -123.456)
        # - Scientific notation (e.g., 1e-6, 1.23e+4)
        # - Numbers with leading zeros (e.g., 00123, 00.123)
        pattern = r"-?\d*\.?\d+(?:[eE][-+]?\d+)?"

        # Process all numeric literals in the code
        processed_code = re.sub(pattern, process_numeric, code)

        return processed_code

    def _post_process_test_code(self, test_code: str) -> str:
        """Post-process the generated test code to ensure it's valid Python."""
        # Extract the code block from the response
        code_block = re.search(r"```python\n(.*?)```", test_code, re.DOTALL)
        if code_block:
            code = code_block.group(1)
        else:
            code = test_code

        # Process numeric literals
        code = self._validate_numeric_literals(code)

        # Fix common issues
        code = re.sub(
            r"(\d+)\.(?=\s|$)", r"\1.0", code
        )  # Add .0 to trailing decimal points
        code = re.sub(r"\.(?=\d)", r"0.", code)  # Add leading 0 to decimal points
        code = re.sub(r"(\d+)e(\d+)", r"\1e\2", code)  # Fix scientific notation
        code = re.sub(
            r"(\d+)\.e(\d+)", r"\1.0e\2", code
        )  # Fix decimal points in scientific notation
        code = re.sub(r"-\s*(\d+)", r"-\1", code)  # Fix spaces in negative numbers

        # Fix percentage values in dicts
        code = re.sub(r":\s*(\d+)%", r": \1.0", code)  # Convert percentages to decimals
        code = re.sub(r":\s*(\d+)\.(\d+)%", r": \1.\2", code)  # Fix decimal percentages

        return code

    def auto_fix_syntax_errors(
        self, test_code: str, max_attempts: int = 5
    ) -> tuple[str, SyntaxError | None]:
        """
        Deterministically fix syntax errors.
        Returns (fixed_code, None) on success.
        Returns (original_code, Exception) on unrecoverable or persistent failure.
        """
        import re

        current_code = test_code
        for attempt in range(max_attempts):
            try:
                compile(current_code, "<string>", "exec")
                return current_code, None
            except SyntaxError as e:
                if "invalid decimal literal" not in str(e) or e.lineno is None:
                    unrecoverable_error = SyntaxError(f"Cannot fix syntax error: {e}")
                    unrecoverable_error.lineno = e.lineno
                    unrecoverable_error.offset = e.offset
                    return test_code, unrecoverable_error

                if attempt == max_attempts - 1:
                    failure_error = SyntaxError(
                        f"Failed to fix decimal literal after {max_attempts} attempts. Last error: {e}"
                    )
                    failure_error.lineno = e.lineno
                    failure_error.offset = e.offset
                    return test_code, failure_error

                lines = current_code.split("\n")
                line_index = e.lineno - 1

                if line_index >= len(lines):
                    return test_code, e

                faulty_line = lines[line_index]

                def number_replacer(match):
                    num_str = match.group(0)
                    try:
                        if "." in num_str or "e" in num_str.lower():
                            return str(float(num_str))
                        else:
                            return str(int(num_str))
                    except ValueError:
                        return num_str

                number_like_pattern = r"\b\d*\.?\d+(?:[eE][-+]?\d+)?\b"
                fixed_line = re.sub(number_like_pattern, number_replacer, faulty_line)

                if faulty_line == fixed_line:
                    stuck_error = SyntaxError(
                        f"Could not fix line {e.lineno}: {faulty_line}"
                    )
                    stuck_error.lineno = e.lineno
                    stuck_error.offset = e.offset
                    return test_code, stuck_error

                lines[line_index] = fixed_line
                current_code = "\n".join(lines)

        return current_code, None

    def generate_tests_from_requirement(
        self, file_path: str, requirement_text: str, confidence: float
    ) -> Dict[str, Any]:
        """Generate tests based on a specific functional requirement.

        This is the requirement-driven test generation method for the MVP workflow.

        Args:
            file_path: Path to the source file
            requirement_text: The approved functional requirement
            confidence: Confidence score from requirement inference

        Returns:
            Test generation result dictionary
        """
        try:
            # Read the source code
            with open(file_path, "r") as f:
                source_code = f.read()

            # Create requirement-driven prompt
            prompt = self._create_requirement_driven_prompt(
                source_code, file_path, requirement_text, confidence
            )

            # Generate test code
            response = self._generate_test_code(prompt)

            # Extract code from response
            match = re.search(r"```python\n(.*?)```", response, re.DOTALL)
            code_to_process = match.group(1) if match else response

            # Fix syntax errors
            fixed_code, error = self.auto_fix_syntax_errors(code_to_process)

            if error:
                return {"code": fixed_code, "success": False, "error": str(error)}
            else:
                return {"code": fixed_code, "success": True, "error": None}

        except Exception as e:
            return {
                "code": None,
                "success": False,
                "error": f"Requirement-based test generation failed: {str(e)}",
            }

    def _create_requirement_driven_prompt(
        self, source_code: str, file_path: str, requirement_text: str, confidence: float
    ) -> str:
        """Create a prompt for requirement-driven test generation.

        Args:
            source_code: The source code to test
            file_path: Path to the source file
            requirement_text: The functional requirement
            confidence: Confidence score

        Returns:
            Formatted prompt for LLM
        """
        return f"""
You are AgentOps QA Agent generating tests based on a validated functional requirement.

CRITICAL: You must output ONLY valid Python code inside a single ```python code block.
Do not explain, comment, or output anything else outside the block.

Your task:
Generate a complete pytest test file that specifically validates the following requirement:

REQUIREMENT: {requirement_text}

The test must:
1. Import necessary modules from the source file
2. Create test cases that verify the requirement is met
3. Use descriptive test names that relate to the requirement
4. Include helpful assertion messages
5. Add `@pytest.mark.generated_by_agentops` above every test function
6. Use proper pytest patterns and conventions

Include this docstring at the top:
\"\"\"
Auto-generated by AgentOps QA Agent v0.2 (MVP)
Target: {file_path}
Requirement: {requirement_text}
Confidence: {confidence:.1%}
Generation: Requirement-driven test generation
\"\"\"

Source code to test:
```python
{source_code}
```

Remember:
- Output ONLY Python code in a single ```python code block
- Focus tests on validating the specific requirement
- Use proper numeric literals (no trailing decimals like 1.)
- Make tests deterministic and reliable
"""

    def qa_validate_test_file(self, test_code: str) -> str:
        """Use a separate LLM call to validate and fix the generated test code."""
        # This function can be expanded to use a different model or prompt for validation
        validation_prompt = (
            "You are a QA engineer. Your task is to validate the following pytest test file. "
            "If the file is valid, return it as is. If it has syntax errors or other issues, "
            "fix them and return the corrected file. The file should be wrapped in a single ```python code block.\n\n"
            f"Test file to validate:\n{test_code}"
        )

        return self._generate_test_code(validation_prompt)

    def _rewrite_imports_to_absolute(self, code: str, module_path: str) -> str:
        """Rewrite relative and placeholder imports in the test code to absolute imports from the project root."""
        import re

        # Compute the absolute import base from the module_path
        # E.g., for .agentops/tests/agentops_ai/agentops_cli/test_main.py, base is agentops_ai.agentops_cli
        base = (
            module_path.replace(".agentops/tests/", "")
            .replace("/", ".")
            .rsplit(".", 1)[0]
        )

        # For Pydantic models, we want to import directly from the module
        if "from pydantic import" in code:
            # Extract the module name from the path
            module_name = os.path.basename(module_path).replace(".py", "")
            # Replace relative imports with direct imports
            code = re.sub(
                r"from \. import ([\w, ]+)", f"from {module_name} import \\1", code
            )
            code = re.sub(
                r"from \.\. import ([\w, ]+)", f"from {module_name} import \\1", code
            )
            return code

        def repl(match):
            rel = match.group(1)
            name = match.group(2)
            # Only handle .. imports for now
            if rel == "..":
                # Remove last component from base
                abs_base = ".".join(base.split(".")[:-1])
                return f"from {abs_base} import {name}"
            elif rel == ".":
                return f"from {base} import {name}"
            return match.group(0)

        # Replace relative imports
        code = re.sub(r"from (\.+)\s*import\s*([\w, ]+)", repl, code)
        # Replace 'from your_module import ...' with absolute import
        code = re.sub(
            r"from your_module import ([\w, ]+)", f"from {base} import \\1", code
        )
        # Replace 'from . import ...' and 'from .. import ...' at the start of lines
        code = re.sub(
            r"^from \. import ([\w, ]+)",
            f"from {base} import \\1",
            code,
            flags=re.MULTILINE,
        )
        code = re.sub(
            r"^from \.\. import ([\w, ]+)",
            f"from {'.'.join(base.split('.')[:-1])} import \\1",
            code,
            flags=re.MULTILINE,
        )
        return code

    def _create_api_test_prompt(self, code: str, filename: str) -> str:
        return f'''
You are a QA test generation agent.

You are given one or more API endpoint definitions from a Python web framework (e.g., FastAPI, Flask).

Your job is to generate a **valid pytest test file** that tests each endpoint's basic success case, with:
- The correct HTTP method
- Required parameters (from path, query, or JSON body)
- Expected status code (e.g., 200 OK)

Use `TestClient` if the framework is FastAPI or Flask.

Each test must:
- Be wrapped with `@pytest.mark.generated_by_agentops`
- Include a meaningful assert for `status_code` and (if JSON) a key/value check
- Be deterministic, with fixed test data

Embed this fingerprint docstring at the top of the file:

"""
Auto-generated by AgentOps QA Agent v0.4
Date: 2025-05-25
Target: API endpoint(s) from {filename}
LLM Confidence Score: 86%
Generation Prompt Hash: a9f7e2c3
Regeneration Policy: Re-evaluate on diff; regenerate if confidence < 70%
"""

Do NOT include markdown. Return only valid Python code inside a ```python code block.

Example endpoint:
@app.post("/login")
def login(username: str, password: str):
    ...
from fastapi.testclient import TestClient
from app import app
import pytest

client = TestClient(app)

@pytest.mark.generated_by_agentops
def test_login_success():
    response = client.post("/login", json={"username": "test", "password": "secret"})
    assert response.status_code == 200
    assert "token" in response.json()

---

### ðŸ›  Extend This Prompt For:
- Auth headers (`Authorization: Bearer ...`)
- Query parameters (`/items/?id=1&status=ok`)
- Parametrize input/output cases
- Error scenarios (401, 422, etc.)

Below are the API endpoint definitions:
{code}
'''

    def _analyze_code(self, code: str) -> Dict[str, Any]:
        """Analyze the code to extract functions, classes, and other structures."""
        # This is a placeholder for a more sophisticated code analysis
        return {"functions": [], "classes": []}

    def _generate_test_code(self, prompt: str) -> str:
        """Generate test code using the OpenAI API."""
        return (
            self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                top_p=1.0,
                max_tokens=2048,
            )
            .choices[0]
            .message.content
        )
