{
    "config": {
        "abort": {
            "download_failed": "O download do modelo falhou"
        },
        "error": {
            "cannot_connect": "A liga\u00e7\u00e3o falhou",
            "unknown": "Erro inesperado"
        },
        "progress": {
            "download": "Aguarde enquanto o modelo \u00e9 transferido, o que pode demorar muito tempo. Consulte os registos do seu servidor Ollama para obter mais informa\u00e7\u00f5es."
        },
        "step": {
            "download": {
                "title": "A transferir modelo"
            },
            "user": {
                "data": {
                    "model": "Modelo",
                    "url": "URL"
                }
            }
        }
    },
    "options": {
        "step": {
            "init": {
                "data": {
                    "keep_alive": "Manter vivo",
                    "llm_hass_api": "Controlar Home Assistant",
                    "max_history": "M\u00e1ximo de mensagens do hist\u00f3rico",
                    "num_ctx": "Tamanho da janela de contexto",
                    "prompt": "Instru\u00e7\u00f5es"
                },
                "data_description": {
                    "keep_alive": "Dura\u00e7\u00e3o em segundos para que Ollama mantenha o modelo em mem\u00f3ria. -1 = indefinido, 0 = nunca.",
                    "num_ctx": "N\u00famero m\u00e1ximo de tokens de texto que o modelo pode processar. Diminuir para reduzir a RAM do Ollama, ou aumentar para um grande n\u00famero de entidades expostas.",
                    "prompt": "Instruir como o LLM deve responder. Isto pode ser um template."
                }
            }
        }
    }
}