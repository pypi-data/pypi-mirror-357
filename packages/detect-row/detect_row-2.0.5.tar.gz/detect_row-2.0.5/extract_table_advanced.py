#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tr√≠ch xu·∫•t b·∫£ng v√† rows - Phi√™n b·∫£n Advanced
==========================================

Script n√†y s·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t n√¢ng cao:
1. Ph√°t hi·ªán b·∫£ng v·ªõi contour analysis
2. Ph√¢n t√≠ch c·∫•u tr√∫c b·∫£ng (grid detection)
3. Tr√≠ch xu·∫•t rows v·ªõi machine learning approach
4. Lo·∫°i b·ªè header th√¥ng minh
5. Optimization cho ƒë·ªô ch√≠nh x√°c cao

S·ª≠ d·ª•ng: python extract_table_advanced.py
"""

import os
import cv2
import numpy as np
import json
from datetime import datetime
from typing import List, Tuple, Dict, Any
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
import sys

# Import detect_row
try:
    from detect_row import AdvancedTableExtractor
    print(f"‚úÖ Imported detect_row successfully")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    exit(1)

def ensure_dir(path: str):
    """T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥"""
    os.makedirs(path, exist_ok=True)
    print(f"üìÅ Created directory: {path}")

def advanced_line_detection(image: np.ndarray, min_line_length: int = 50) -> List[List[int]]:
    """
    Ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª ngang n√¢ng cao v·ªõi HoughLinesP
    
    Args:
        image: ·∫¢nh ƒë·∫ßu v√†o (grayscale)
        min_line_length: ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa ƒë∆∞·ªùng k·∫ª
    
    Returns:
        List[List[int]]: Danh s√°ch c√°c ƒë∆∞·ªùng k·∫ª ngang [x1, y1, x2, y2]
    """
    # Canny edge detection
    edges = cv2.Canny(image, 50, 150, apertureSize=3)
    
    # HoughLinesP ƒë·ªÉ ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª
    lines = cv2.HoughLinesP(
        edges, 
        rho=1, 
        theta=np.pi/180, 
        threshold=100,
        minLineLength=min_line_length,
        maxLineGap=10
    )
    
    horizontal_lines = []
    
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i ƒë∆∞·ªùng ngang kh√¥ng (g√≥c nh·ªè)
            if abs(y2 - y1) <= 5:  # ƒê·ªô l·ªách d·ªçc nh·ªè
                horizontal_lines.append([x1, y1, x2, y2])
    
    print(f"üîç Ph√°t hi·ªán {len(horizontal_lines)} ƒë∆∞·ªùng k·∫ª ngang b·∫±ng HoughLines")
    return horizontal_lines

def cluster_horizontal_lines(lines: List[List[int]], eps: int = 10) -> List[int]:
    """
    Gom nh√≥m c√°c ƒë∆∞·ªùng k·∫ª ngang th√†nh c√°c nh√≥m theo t·ªça ƒë·ªô y
    
    Args:
        lines: Danh s√°ch ƒë∆∞·ªùng k·∫ª ngang
        eps: Kho·∫£ng c√°ch t·ªëi ƒëa ƒë·ªÉ gom nh√≥m
    
    Returns:
        List[int]: Danh s√°ch t·ªça ƒë·ªô y ƒë·∫°i di·ªán cho t·ª´ng nh√≥m
    """
    if not lines:
        return []
    
    # L·∫•y t·ªça ƒë·ªô y trung b√¨nh c·ªßa m·ªói ƒë∆∞·ªùng k·∫ª
    y_coords = []
    for line in lines:
        x1, y1, x2, y2 = line
        y_avg = (y1 + y2) // 2
        y_coords.append([y_avg])
    
    # S·ª≠ d·ª•ng DBSCAN ƒë·ªÉ gom nh√≥m
    if len(y_coords) > 1:
        clustering = DBSCAN(eps=eps, min_samples=1).fit(y_coords)
        labels = clustering.labels_
        
        # T√≠nh t·ªça ƒë·ªô y trung b√¨nh cho m·ªói cluster
        unique_labels = set(labels)
        clustered_lines = []
        
        for label in unique_labels:
            if label != -1:  # B·ªè qua noise
                cluster_points = [y_coords[i][0] for i in range(len(labels)) if labels[i] == label]
                avg_y = int(np.mean(cluster_points))
                clustered_lines.append(avg_y)
        
        clustered_lines.sort()
        print(f"üìä Gom nh√≥m th√†nh {len(clustered_lines)} ƒë∆∞·ªùng k·∫ª ch√≠nh")
        return clustered_lines
    else:
        return [y_coords[0][0]] if y_coords else []

def analyze_table_structure(image: np.ndarray) -> Dict[str, Any]:
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c b·∫£ng n√¢ng cao
    
    Args:
        image: ·∫¢nh b·∫£ng
    
    Returns:
        Dict: Th√¥ng tin c·∫•u tr√∫c b·∫£ng
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    height, width = gray.shape
    
    # Ph√°t hi·ªán ƒë∆∞·ªùng k·∫ª ngang
    horizontal_lines = advanced_line_detection(gray, min_line_length=width//4)
    
    # Gom nh√≥m ƒë∆∞·ªùng k·∫ª
    clustered_y = cluster_horizontal_lines(horizontal_lines, eps=15)
    
    # Th√™m vi·ªÅn tr√™n v√† d∆∞·ªõi n·∫øu c·∫ßn
    if not clustered_y or clustered_y[0] > 20:
        clustered_y.insert(0, 0)
    if not clustered_y or clustered_y[-1] < height - 20:
        clustered_y.append(height)
    
    # Ph√¢n t√≠ch rows
    rows_info = []
    for i in range(len(clustered_y) - 1):
        y1 = clustered_y[i]
        y2 = clustered_y[i + 1]
        row_height = y2 - y1
        
        # Ph√¢n t√≠ch n·ªôi dung row
        row_region = gray[y1:y2, :]
        
        # T√≠nh m·∫≠t ƒë·ªô pixel t·ªëi (text)
        _, binary = cv2.threshold(row_region, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        text_density = np.sum(binary == 255) / binary.size
        
        # Ph√¢n t√≠ch horizontal projection
        h_projection = np.sum(binary, axis=1)
        has_strong_text = np.max(h_projection) > width * 10  # Ng∆∞·ª°ng text m·∫°nh
        
        row_info = {
            "index": i,
            "y1": y1,
            "y2": y2,
            "height": row_height,
            "text_density": text_density,
            "has_strong_text": has_strong_text,
            "is_likely_header": i == 0 and (text_density > 0.05 or row_height > 50)
        }
        rows_info.append(row_info)
    
    structure = {
        "total_rows": len(rows_info),
        "rows": rows_info,
        "horizontal_lines": horizontal_lines,
        "clustered_lines": clustered_y,
        "table_height": height,
        "table_width": width
    }
    
    return structure

def extract_rows_advanced(image: np.ndarray, structure: Dict[str, Any], skip_header: bool = True) -> List[Dict[str, Any]]:
    """
    Tr√≠ch xu·∫•t rows v·ªõi ph∆∞∆°ng ph√°p n√¢ng cao
    
    Args:
        image: ·∫¢nh b·∫£ng g·ªëc
        structure: C·∫•u tr√∫c b·∫£ng ƒë√£ ph√¢n t√≠ch  
        skip_header: C√≥ b·ªè qua header kh√¥ng
    
    Returns:
        List[Dict]: Danh s√°ch th√¥ng tin rows ƒë√£ tr√≠ch xu·∫•t
    """
    extracted_rows = []
    
    for row_info in structure["rows"]:
        i = row_info["index"]
        y1 = row_info["y1"]
        y2 = row_info["y2"]
        
        # Quy·∫øt ƒë·ªãnh c√≥ tr√≠ch xu·∫•t row n√†y kh√¥ng
        should_extract = True
        skip_reason = ""
        
        # B·ªè qua header n·∫øu c·∫ßn
        if skip_header and row_info.get("is_likely_header", False):
            should_extract = False
            skip_reason = "Header row"
        
        # B·ªè qua row qu√° th·∫•p
        if row_info["height"] < 20:
            should_extract = False
            skip_reason = "Too small"
        
        # B·ªè qua row kh√¥ng c√≥ text
        if row_info["text_density"] < 0.005:
            should_extract = False
            skip_reason = "No text content"
        
        if should_extract:
            # Tr√≠ch xu·∫•t row v·ªõi margin nh·ªè
            margin = 2
            y1_adj = max(0, y1 + margin)
            y2_adj = min(image.shape[0], y2 - margin)
            
            row_image = image[y1_adj:y2_adj, :]
            
            # L∆∞u th√¥ng tin row ƒë√£ tr√≠ch xu·∫•t
            extracted_info = {
                "original_index": i,
                "extracted_index": len(extracted_rows),
                "y1": y1_adj,
                "y2": y2_adj,
                "height": y2_adj - y1_adj,
                "text_density": row_info["text_density"],
                "has_strong_text": row_info["has_strong_text"],
                "row_image": row_image,
                "skip_reason": None
            }
            extracted_rows.append(extracted_info)
            
            print(f"‚úÖ Tr√≠ch xu·∫•t Row {len(extracted_rows)}: y={y1_adj}-{y2_adj} (h={y2_adj - y1_adj}px) - Density: {row_info['text_density']:.3f}")
        else:
            print(f"‚è≠Ô∏è B·ªè qua Row {i}: {skip_reason} - y={y1}-{y2} (h={row_info['height']}px)")
    
    return extracted_rows

def save_extracted_rows(rows: List[Dict[str, Any]], output_dir: str, table_name: str) -> List[str]:
    """
    L∆∞u c√°c rows ƒë√£ tr√≠ch xu·∫•t
    
    Args:
        rows: Danh s√°ch rows ƒë√£ tr√≠ch xu·∫•t
        output_dir: Th∆∞ m·ª•c l∆∞u
        table_name: T√™n b·∫£ng
    
    Returns:
        List[str]: Danh s√°ch ƒë∆∞·ªùng d·∫´n files ƒë√£ l∆∞u
    """
    saved_files = []
    
    for row_info in rows:
        extracted_idx = row_info["extracted_index"]
        row_image = row_info["row_image"]
        
        # T·∫°o t√™n file
        filename = f"{table_name}_row_{extracted_idx:02d}.jpg"
        filepath = os.path.join(output_dir, filename)
        
        # L∆∞u ·∫£nh
        cv2.imwrite(filepath, row_image)
        saved_files.append(filepath)
        
        print(f"üíæ ƒê√£ l∆∞u: {filename}")
    
    return saved_files

def create_visualization(image: np.ndarray, structure: Dict[str, Any], output_path: str):
    """
    T·∫°o visualization cho vi·ªác ph√¢n t√≠ch b·∫£ng
    
    Args:
        image: ·∫¢nh b·∫£ng g·ªëc
        structure: C·∫•u tr√∫c ƒë√£ ph√¢n t√≠ch
        output_path: ƒê∆∞·ªùng d·∫´n l∆∞u visualization
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 8))
    
    # ·∫¢nh g·ªëc v·ªõi ƒë∆∞·ªùng k·∫ª ƒë∆∞·ª£c ph√°t hi·ªán
    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    axes[0].set_title('B·∫£ng g·ªëc + ƒê∆∞·ªùng k·∫ª ph√°t hi·ªán')
    
    # V·∫Ω c√°c ƒë∆∞·ªùng k·∫ª ƒë√£ gom nh√≥m
    for y in structure["clustered_lines"]:
        axes[0].axhline(y=y, color='red', linestyle='--', linewidth=2, alpha=0.7)
    
    # ƒê√°nh s·ªë rows
    for i, row in enumerate(structure["rows"]):
        y_center = (row["y1"] + row["y2"]) // 2
        axes[0].text(10, y_center, f'Row {i}', color='blue', fontsize=12, fontweight='bold')
    
    axes[0].set_xlim(0, image.shape[1])
    axes[0].set_ylim(image.shape[0], 0)
    
    # Bi·ªÉu ƒë·ªì text density
    row_indices = [r["index"] for r in structure["rows"]]
    text_densities = [r["text_density"] for r in structure["rows"]]
    
    axes[1].bar(row_indices, text_densities, alpha=0.7)
    axes[1].set_title('M·∫≠t ƒë·ªô Text theo Row')
    axes[1].set_xlabel('Row Index')
    axes[1].set_ylabel('Text Density')
    axes[1].grid(True, alpha=0.3)
    
    # ƒê√°nh d·∫•u header
    for i, row in enumerate(structure["rows"]):
        if row.get("is_likely_header", False):
            axes[1].bar(i, row["text_density"], color='red', alpha=0.8, label='Header')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"üìä ƒê√£ l∆∞u visualization: {output_path}")

def main():
    """H√†m ch√≠nh"""
    # L·∫•y ƒë∆∞·ªùng d·∫´n ·∫£nh t·ª´ tham s·ªë d√≤ng l·ªánh
    if len(sys.argv) > 1:
        image_path = sys.argv[1]
    else:
        image_path = "image0524.png"  # Fallback n·∫øu kh√¥ng c√≥ tham s·ªë
    
    output_base = "advanced_extraction_output"
    
    print(f"üöÄ TR√çCH XU·∫§T B·∫¢NG N√ÇNG CAO (ADVANCED)")
    print(f"üì∏ ·∫¢nh ƒë·∫ßu v√†o: {image_path}")
    print(f"üìÅ Th∆∞ m·ª•c ƒë·∫ßu ra: {output_base}")
    print(f"‚è∞ Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if not os.path.exists(image_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh: {image_path}")
        return
    
    # T·∫°o th∆∞ m·ª•c output
    ensure_dir(output_base)
    ensure_dir(f"{output_base}/tables")
    ensure_dir(f"{output_base}/rows")
    ensure_dir(f"{output_base}/analysis")
    ensure_dir(f"{output_base}/debug")
    
    # B∆∞·ªõc 1: Tr√≠ch xu·∫•t b·∫£ng
    print(f"\n{'='*60}")
    print("B∆Ø·ªöC 1: TR√çCH XU·∫§T B·∫¢NG")
    print(f"{'='*60}")
    
    extractor = AdvancedTableExtractor(
        input_dir=os.path.dirname(image_path),
        output_dir=f"{output_base}/tables",
        debug_dir=f"{output_base}/debug"
    )
    
    result = extractor.process_image(image_path, margin=5, check_text=True)
    
    # T√¨m c√°c b·∫£ng ƒë√£ tr√≠ch xu·∫•t
    table_files = [f for f in os.listdir(f"{output_base}/tables") if f.endswith('.jpg')]
    
    if not table_files:
        print("‚ùå Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c b·∫£ng n√†o!")
        return
    
    print(f"‚úÖ Tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(table_files)} b·∫£ng")
    
    # B∆∞·ªõc 2: Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t rows cho t·ª´ng b·∫£ng
    print(f"\n{'='*60}")
    print("B∆Ø·ªöC 2: PH√ÇN T√çCH V√Ä TR√çCH XU·∫§T ROWS")
    print(f"{'='*60}")
    
    all_results = []
    
    for i, table_file in enumerate(sorted(table_files)):
        table_path = os.path.join(f"{output_base}/tables", table_file)
        table_name = os.path.splitext(table_file)[0]
        
        print(f"\n--- X·ª≠ l√Ω {table_file} ({i+1}/{len(table_files)}) ---")
        
        # ƒê·ªçc ·∫£nh b·∫£ng
        table_image = cv2.imread(table_path)
        if table_image is None:
            print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc {table_path}")
            continue
        
        # Ph√¢n t√≠ch c·∫•u tr√∫c b·∫£ng
        print("üîç Ph√¢n t√≠ch c·∫•u tr√∫c b·∫£ng...")
        structure = analyze_table_structure(table_image)
        
        print(f"üìä Ph√°t hi·ªán {structure['total_rows']} rows ti·ªÅm nƒÉng")
        
        # Tr√≠ch xu·∫•t rows (b·ªè qua header)
        print("‚úÇÔ∏è Tr√≠ch xu·∫•t rows...")
        extracted_rows = extract_rows_advanced(table_image, structure, skip_header=True)
        
        # L∆∞u rows
        saved_files = save_extracted_rows(extracted_rows, f"{output_base}/rows", table_name)
        
        # T·∫°o visualization
        viz_path = os.path.join(f"{output_base}/analysis", f"{table_name}_analysis.png")
        create_visualization(table_image, structure, viz_path)
        
        # L∆∞u ph√¢n t√≠ch JSON (s·ª≠a l·ªói serialization)
        analysis_file = os.path.join(f"{output_base}/analysis", f"{table_name}_structure.json")
        try:
            with open(analysis_file, 'w', encoding='utf-8') as f:
                # Lo·∫°i b·ªè row_image v√† convert numpy types
                clean_structure = {}
                for k, v in structure.items():
                    if k == 'rows':
                        clean_rows_structure = []
                        for row in v:
                            clean_row_struct = {}
                            for rk, rv in row.items():
                                # Convert numpy bool to Python bool
                                if isinstance(rv, (bool, np.bool_)):
                                    clean_row_struct[rk] = bool(rv)
                                elif isinstance(rv, (int, np.integer)):
                                    clean_row_struct[rk] = int(rv)
                                elif isinstance(rv, (float, np.floating)):
                                    clean_row_struct[rk] = float(rv)
                                else:
                                    clean_row_struct[rk] = rv
                            clean_rows_structure.append(clean_row_struct)
                        clean_structure[k] = clean_rows_structure
                    else:
                        clean_structure[k] = v
                
                clean_rows = []
                for row in extracted_rows:
                    clean_row = {}
                    for k, v in row.items():
                        if k != 'row_image':
                            # Convert numpy types to Python types
                            if isinstance(v, (bool, np.bool_)):
                                clean_row[k] = bool(v)
                            elif isinstance(v, (int, np.integer)):
                                clean_row[k] = int(v)
                            elif isinstance(v, (float, np.floating)):
                                clean_row[k] = float(v)
                            else:
                                clean_row[k] = v
                    clean_rows.append(clean_row)
                
                analysis_data = {
                    "table_name": table_name,
                    "structure": clean_structure,
                    "extracted_rows": clean_rows,
                    "saved_files": saved_files
                }
                json.dump(analysis_data, f, indent=2, ensure_ascii=False)
                print(f"üìÑ ƒê√£ l∆∞u ph√¢n t√≠ch: {analysis_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói l∆∞u JSON: {e}, b·ªè qua...")
        
        all_results.append({
            "table_name": table_name,
            "total_rows_detected": structure['total_rows'],
            "rows_extracted": len(extracted_rows),
            "saved_files": saved_files
        })
    
    # B∆∞·ªõc 3: T·∫°o b√°o c√°o t·ªïng h·ª£p
    print(f"\n{'='*60}")
    print("B∆Ø·ªöC 3: B√ÅO C√ÅO T·ªîNG H·ª¢P")
    print(f"{'='*60}")
    
    total_tables = len(all_results)
    total_rows = sum(r["rows_extracted"] for r in all_results)
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "method": "Advanced Table Extraction",
        "total_tables": total_tables,
        "total_rows_extracted": total_rows,
        "results": all_results
    }
    
    # L∆∞u b√°o c√°o
    summary_file = os.path.join(output_base, "advanced_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # B√°o c√°o text
    report_file = os.path.join(output_base, "advanced_report.txt")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("B√ÅO C√ÅO TR√çCH XU·∫§T B·∫¢NG N√ÇNG CAO\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Ph∆∞∆°ng ph√°p: Advanced Table Extraction\n")
        f.write(f"T·ªïng s·ªë b·∫£ng: {total_tables}\n")
        f.write(f"T·ªïng s·ªë rows: {total_rows}\n\n")
        
        f.write("CHI TI·∫æT:\n")
        f.write("-" * 20 + "\n")
        for result in all_results:
            f.write(f"\nB·∫£ng: {result['table_name']}\n")
            f.write(f"  Rows ph√°t hi·ªán: {result['total_rows_detected']}\n")
            f.write(f"  Rows tr√≠ch xu·∫•t: {result['rows_extracted']}\n")
            f.write(f"  Files l∆∞u: {len(result['saved_files'])}\n")
    
    # T·ªïng k·∫øt
    print(f"üéâ HO√ÄN TH√ÄNH TR√çCH XU·∫§T N√ÇNG CAO!")
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {total_tables} b·∫£ng")
    print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t: {total_rows} rows")
    print(f"üìÅ K·∫øt qu·∫£ l∆∞u t·∫°i: {output_base}/")
    print(f"  üìä B·∫£ng: {output_base}/tables/")
    print(f"  üìã Rows: {output_base}/rows/")
    print(f"  üìà Ph√¢n t√≠ch: {output_base}/analysis/")
    print(f"  üêõ Debug: {output_base}/debug/")
    
    # Hi·ªÉn th·ªã danh s√°ch rows
    rows_dir = f"{output_base}/rows"
    if os.path.exists(rows_dir):
        row_files = sorted([f for f in os.listdir(rows_dir) if f.endswith('.jpg')])
        if row_files:
            print(f"\nüìã {len(row_files)} rows ƒë√£ tr√≠ch xu·∫•t:")
            for row_file in row_files:
                print(f"  - {row_file}")

if __name__ == "__main__":
    main() 