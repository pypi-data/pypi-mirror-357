#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WORKFLOW Tá»° Äá»˜NG HOÃ€N CHá»ˆNH - TRÃCH XUáº¤T Báº¢NG
==============================================

Script tá»± Ä‘á»™ng cháº¡y toÃ n bá»™ quy trÃ¬nh:
1. Kiá»ƒm tra há»‡ thá»‘ng vÃ  GPU
2. Tá»‘i Æ°u cáº¥u hÃ¬nh theo hardware
3. TrÃ­ch xuáº¥t báº£ng tá»« táº¥t cáº£ áº£nh
4. TrÃ­ch xuáº¥t cá»™t vá»›i merge tÃ¹y chá»‰nh
5. Quáº£n lÃ½ bá»™ nhá»› hiá»‡u quáº£
6. Táº¡o bÃ¡o cÃ¡o káº¿t quáº£

CÃ¡ch sá»­ dá»¥ng:
    python run_complete_workflow.py
    python run_complete_workflow.py --config config_template.json
    python run_complete_workflow.py --column-groups "custom:1,2;result:3,4"
    python run_complete_workflow.py --max-memory 8 --use-gpu
"""

import os
import sys
import json
import time
import argparse
import logging
import traceback
from pathlib import Path
from datetime import datetime
import cv2
import numpy as np
import psutil
import gc

# Add current directory to path
sys.path.insert(0, '.')

try:
    from detect_row import AdvancedTableExtractor, AdvancedColumnExtractor
    from detect_row.gpu_support import GPUManager, MemoryManager
except ImportError as e:
    print(f"âŒ Lá»—i import: {e}")
    print("Vui lÃ²ng kiá»ƒm tra cÃ i Ä‘áº·t package detect_row")
    sys.exit(1)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WorkflowManager:
    """Quáº£n lÃ½ workflow tá»± Ä‘á»™ng"""
    
    def __init__(self, config_file=None, max_memory_gb=4, use_gpu=True):
        self.config_file = config_file
        self.max_memory_gb = max_memory_gb
        self.use_gpu = use_gpu
        self.start_time = time.time()
        
        # Load configuration
        self.config = self.load_config()
        
        # Initialize managers
        self.gpu_manager = GPUManager()
        self.memory_manager = MemoryManager(max_memory_gb=max_memory_gb)
        
        # Setup directories
        self.setup_directories()
        
        # Results tracking
        self.results = {
            'processed_images': 0,
            'detected_tables': 0,
            'extracted_columns': 0,
            'errors': [],
            'processing_times': []
        }
    
    def load_config(self):
        """Load cáº¥u hÃ¬nh tá»« file"""
        if self.config_file and os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"âœ… ÄÃ£ load config tá»« {self.config_file}")
                return config
            except Exception as e:
                logger.warning(f"âš ï¸ Lá»—i load config: {e}, sá»­ dá»¥ng config máº·c Ä‘á»‹nh")
        
        # Default config
        return {
            "table_extraction": {
                "min_area_ratio": 0.003,
                "max_area_ratio": 0.25,
                "min_aspect_ratio": 1.0,
                "max_aspect_ratio": 15.0
            },
            "column_extraction": {
                "min_column_width": 20,
                "vertical_line_threshold": 0.4
            },
            "performance": {
                "batch_size": 8,
                "num_workers": 4
            }
        }
    
    def setup_directories(self):
        """Thiáº¿t láº­p thÆ° má»¥c"""
        directories = [
            'input',
            'output/tables_and_columns/tables',
            'output/tables_and_columns/columns', 
            'debug/tables_and_columns',
            'reports'
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
        
        logger.info("âœ… ÄÃ£ thiáº¿t láº­p thÆ° má»¥c")
    
    def check_system_status(self):
        """Kiá»ƒm tra tráº¡ng thÃ¡i há»‡ thá»‘ng"""
        logger.info("ğŸ” Kiá»ƒm tra há»‡ thá»‘ng...")
        
        # Memory check
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        
        logger.info(f"ğŸ’¾ RAM: {memory_gb:.1f}GB (sá»­ dá»¥ng {memory.percent:.1f}%)")
        
        # GPU check
        if self.use_gpu and self.gpu_manager.is_gpu_available():
            gpu_info = self.gpu_manager.get_gpu_info()
            logger.info(f"ğŸ® GPU: {gpu_info}")
        else:
            logger.info("ğŸ–¥ï¸ Sá»­ dá»¥ng CPU")
        
        # Disk space
        disk = psutil.disk_usage('.')
        disk_free_gb = disk.free / (1024**3)
        logger.info(f"ğŸ’¿ Disk free: {disk_free_gb:.1f}GB")
        
        # Warning checks
        if memory.percent > 80:
            logger.warning("âš ï¸ RAM sá»­ dá»¥ng cao!")
        if disk_free_gb < 1:
            logger.warning("âš ï¸ Disk space tháº¥p!")
    
    def find_input_images(self):
        """TÃ¬m áº£nh Ä‘áº§u vÃ o"""
        input_dir = Path('input')
        
        if not input_dir.exists():
            logger.error("âŒ ThÆ° má»¥c input/ khÃ´ng tá»“n táº¡i")
            return []
        
        # Supported formats
        formats = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']
        
        images = []
        for fmt in formats:
            images.extend(input_dir.glob(fmt))
            images.extend(input_dir.glob(fmt.upper()))
        
        logger.info(f"ğŸ“ TÃ¬m tháº¥y {len(images)} áº£nh trong input/")
        return sorted(images)
    
    def optimize_batch_size(self, images):
        """Tá»‘i Æ°u batch size theo memory"""
        if not images:
            return 1
        
        # Estimate memory per image
        sample_image = cv2.imread(str(images[0]))
        if sample_image is None:
            return 1
        
        # Estimate memory usage (rough calculation)
        image_size_mb = sample_image.nbytes / (1024 * 1024)
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        # Conservative estimate: use 50% of available memory
        usable_memory_gb = min(available_memory_gb * 0.5, self.max_memory_gb)
        
        optimal_batch_size = max(1, int((usable_memory_gb * 1024) / (image_size_mb * 3)))  # 3x safety factor
        
        # Limit to reasonable range
        optimal_batch_size = min(optimal_batch_size, 16)
        optimal_batch_size = max(optimal_batch_size, 1)
        
        logger.info(f"âš™ï¸ Batch size tá»‘i Æ°u: {optimal_batch_size}")
        return optimal_batch_size
    
    def process_image_batch(self, image_batch, column_groups=None):
        """Xá»­ lÃ½ má»™t batch áº£nh"""
        batch_results = []
        
        for image_path in image_batch:
            try:
                start_time = time.time()
                
                logger.info(f"ğŸ”„ Xá»­ lÃ½: {image_path.name}")
                
                # 1. Table extraction
                table_extractor = AdvancedTableExtractor(
                    input_dir=str(image_path.parent),
                    output_dir="output/tables_and_columns/tables",
                    debug_dir="debug/tables_and_columns"
                )
                
                tables = table_extractor.extract_tables_from_image(image_path.name)
                
                if not tables:
                    logger.warning(f"âš ï¸ KhÃ´ng phÃ¡t hiá»‡n báº£ng trong {image_path.name}")
                    continue
                
                logger.info(f"âœ… PhÃ¡t hiá»‡n {len(tables)} báº£ng")
                self.results['detected_tables'] += len(tables)
                
                # 2. Column extraction cho má»—i báº£ng
                for i, table_info in enumerate(tables):
                    table_name = f"{image_path.stem}_table_{i}"
                    
                    # Column extraction
                    column_extractor = AdvancedColumnExtractor(
                        input_dir="output/tables_and_columns/tables",
                        output_dir=f"output/tables_and_columns/columns/{table_name}",
                        debug_dir=f"debug/tables_and_columns/columns/{table_name}"
                    )
                    
                    table_image_name = f"{table_name}.jpg"
                    columns = column_extractor.extract_columns_from_image(
                        table_image_name, 
                        column_groups=column_groups
                    )
                    
                    if columns:
                        logger.info(f"âœ… TrÃ­ch xuáº¥t {len(columns)} cá»™t tá»« {table_name}")
                        self.results['extracted_columns'] += len(columns)
                
                # Track processing time
                processing_time = time.time() - start_time
                self.results['processing_times'].append(processing_time)
                
                logger.info(f"â±ï¸ HoÃ n thÃ nh {image_path.name} trong {processing_time:.2f}s")
                
                batch_results.append({
                    'image': image_path.name,
                    'tables': len(tables),
                    'processing_time': processing_time
                })
                
                self.results['processed_images'] += 1
                
            except Exception as e:
                error_msg = f"Lá»—i xá»­ lÃ½ {image_path.name}: {e}"
                logger.error(f"âŒ {error_msg}")
                self.results['errors'].append(error_msg)
                
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(traceback.format_exc())
        
        return batch_results
    
    def process_all_images(self, images, column_groups=None):
        """Xá»­ lÃ½ táº¥t cáº£ áº£nh theo batch"""
        if not images:
            logger.warning("âš ï¸ KhÃ´ng cÃ³ áº£nh Ä‘á»ƒ xá»­ lÃ½")
            return []
        
        # Optimize batch size
        batch_size = self.optimize_batch_size(images)
        
        all_results = []
        total_batches = (len(images) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ {len(images)} áº£nh trong {total_batches} batch")
        
        for batch_idx in range(0, len(images), batch_size):
            batch_num = (batch_idx // batch_size) + 1
            
            logger.info(f"ğŸ“¦ Batch {batch_num}/{total_batches}")
            
            # Get batch
            batch = images[batch_idx:batch_idx + batch_size]
            
            # Monitor memory before processing
            memory_before = psutil.virtual_memory().percent
            
            # Process batch
            batch_results = self.process_image_batch(batch, column_groups)
            all_results.extend(batch_results)
            
            # Memory management
            memory_after = psutil.virtual_memory().percent
            logger.info(f"ğŸ’¾ Memory: {memory_before:.1f}% â†’ {memory_after:.1f}%")
            
            if memory_after > 85:
                logger.warning("âš ï¸ Memory cao, cháº¡y garbage collection...")
                gc.collect()
                
                if self.gpu_manager.is_gpu_available():
                    try:
                        import torch
                        torch.cuda.empty_cache()
                        logger.info("ğŸ—‘ï¸ ÄÃ£ dá»n cache GPU")
                    except:
                        pass
            
            # Progress report
            processed = min(batch_idx + batch_size, len(images))
            progress = (processed / len(images)) * 100
            logger.info(f"ğŸ“Š Tiáº¿n Ä‘á»™: {progress:.1f}% ({processed}/{len(images)})")
        
        return all_results
    
    def generate_report(self, processing_results):
        """Táº¡o bÃ¡o cÃ¡o chi tiáº¿t"""
        report_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"reports/workflow_report_{report_time}.json"
        
        # Calculate statistics
        total_time = time.time() - self.start_time
        avg_time_per_image = (
            sum(self.results['processing_times']) / len(self.results['processing_times'])
            if self.results['processing_times'] else 0
        )
        
        # Create detailed report
        report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_runtime": f"{total_time:.2f}s",
                "config_file": self.config_file,
                "max_memory_gb": self.max_memory_gb,
                "use_gpu": self.use_gpu
            },
            "system_info": {
                "platform": os.name,
                "cpu_count": psutil.cpu_count(),
                "total_memory_gb": psutil.virtual_memory().total / (1024**3),
                "gpu_available": self.gpu_manager.is_gpu_available(),
                "gpu_info": self.gpu_manager.get_gpu_info() if self.gpu_manager.is_gpu_available() else None
            },
            "processing_summary": {
                "total_images_processed": self.results['processed_images'],
                "total_tables_detected": self.results['detected_tables'],
                "total_columns_extracted": self.results['extracted_columns'],
                "total_errors": len(self.results['errors']),
                "avg_time_per_image": f"{avg_time_per_image:.2f}s",
                "success_rate": f"{(self.results['processed_images'] / (self.results['processed_images'] + len(self.results['errors'])) * 100):.1f}%" if (self.results['processed_images'] + len(self.results['errors'])) > 0 else "0.0%"
            },
            "detailed_results": processing_results,
            "errors": self.results['errors'],
            "performance_metrics": {
                "processing_times": self.results['processing_times'],
                "memory_usage": self.memory_manager.get_usage_history() if hasattr(self.memory_manager, 'get_usage_history') else []
            }
        }
        
        # Save report
        os.makedirs('reports', exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ BÃ¡o cÃ¡o Ä‘Ã£ lÆ°u: {report_file}")
        
        # Print summary
        self.print_summary(report)
        
        return report_file
    
    def print_summary(self, report):
        """In tÃ³m táº¯t káº¿t quáº£"""
        print("\n" + "="*60)
        print("ğŸ“Š TÃ“M Táº®T Káº¾T QUáº¢ WORKFLOW")
        print("="*60)
        
        summary = report['processing_summary']
        metadata = report['metadata']
        
        print(f"â±ï¸  Thá»i gian cháº¡y: {metadata['total_runtime']}")
        print(f"ğŸ“¸ áº¢nh Ä‘Ã£ xá»­ lÃ½: {summary['total_images_processed']}")
        print(f"ğŸ“‹ Báº£ng phÃ¡t hiá»‡n: {summary['total_tables_detected']}")
        print(f"ğŸ“Š Cá»™t trÃ­ch xuáº¥t: {summary['total_columns_extracted']}")
        print(f"âŒ Lá»—i: {summary['total_errors']}")
        print(f"âœ… Tá»‰ lá»‡ thÃ nh cÃ´ng: {summary['success_rate']}")
        print(f"âš¡ Thá»i gian TB/áº£nh: {summary['avg_time_per_image']}")
        
        if self.results['errors']:
            print(f"\nğŸš¨ CÃC Lá»–I:")
            for error in self.results['errors'][:5]:  # Show first 5 errors
                print(f"   â€¢ {error}")
            if len(self.results['errors']) > 5:
                print(f"   ... vÃ  {len(self.results['errors']) - 5} lá»—i khÃ¡c")
        
        # File structure
        print(f"\nğŸ“ Káº¾T QUáº¢ TRONG:")
        print(f"   ğŸ“‹ Báº£ng: output/tables_and_columns/tables/")
        print(f"   ğŸ“Š Cá»™t: output/tables_and_columns/columns/")
        print(f"   ğŸ› Debug: debug/tables_and_columns/")
        print(f"   ğŸ“‹ BÃ¡o cÃ¡o: reports/")
        
        print("="*60)
    
    def run_complete_workflow(self, column_groups=None):
        """Cháº¡y workflow hoÃ n chá»‰nh"""
        try:
            logger.info("ğŸš€ Báº®T Äáº¦U WORKFLOW Tá»° Äá»˜NG")
            
            # 1. System check
            self.check_system_status()
            
            # 2. Find images
            images = self.find_input_images()
            if not images:
                logger.error("âŒ KhÃ´ng cÃ³ áº£nh Ä‘á»ƒ xá»­ lÃ½!")
                return None
            
            # 3. Process all images
            processing_results = self.process_all_images(images, column_groups)
            
            # 4. Generate report
            report_file = self.generate_report(processing_results)
            
            logger.info("ğŸ‰ WORKFLOW HOÃ€N THÃ€NH!")
            return report_file
            
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Workflow bá»‹ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
            return None
        except Exception as e:
            logger.error(f"ğŸ’¥ Lá»—i nghiÃªm trá»ng: {e}")
            logger.debug(traceback.format_exc())
            return None

def parse_column_groups(groups_str):
    """Parse column groups tá»« string"""
    if not groups_str:
        return None
    
    groups = {}
    for group_def in groups_str.split(';'):
        if ':' in group_def:
            name, cols = group_def.split(':', 1)
            col_list = [int(c.strip()) for c in cols.split(',')]
            groups[name.strip()] = col_list
    
    return groups

def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description='Workflow tá»± Ä‘á»™ng trÃ­ch xuáº¥t báº£ng',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
VÃ­ dá»¥ sá»­ dá»¥ng:
  python run_complete_workflow.py
  python run_complete_workflow.py --config config_template.json
  python run_complete_workflow.py --column-groups "header:1;content:2,3;footer:4"
  python run_complete_workflow.py --max-memory 8 --use-gpu
  python run_complete_workflow.py --no-gpu --max-memory 2
        """
    )
    
    parser.add_argument('--config', 
                       help='File cáº¥u hÃ¬nh JSON')
    parser.add_argument('--column-groups',
                       help='Äá»‹nh nghÄ©a nhÃ³m cá»™t (format: name:1,2;name2:3)')
    parser.add_argument('--max-memory', type=float, default=4,
                       help='Giá»›i háº¡n memory (GB), máº·c Ä‘á»‹nh: 4')
    parser.add_argument('--use-gpu', action='store_true', default=True,
                       help='Sá»­ dá»¥ng GPU (máº·c Ä‘á»‹nh)')
    parser.add_argument('--no-gpu', action='store_true',
                       help='KhÃ´ng sá»­ dá»¥ng GPU')
    parser.add_argument('--verbose', action='store_true',
                       help='Hiá»ƒn thá»‹ log chi tiáº¿t')
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Parse column groups
    column_groups = parse_column_groups(args.column_groups)
    
    # Determine GPU usage
    use_gpu = args.use_gpu and not args.no_gpu
    
    # Create and run workflow
    workflow = WorkflowManager(
        config_file=args.config,
        max_memory_gb=args.max_memory,
        use_gpu=use_gpu
    )
    
    if column_groups:
        logger.info(f"ğŸ“Š Sá»­ dá»¥ng nhÃ³m cá»™t: {column_groups}")
    
    report_file = workflow.run_complete_workflow(column_groups)
    
    if report_file:
        print(f"\nğŸ“‹ BÃ¡o cÃ¡o chi tiáº¿t: {report_file}")
    else:
        print("\nâŒ Workflow khÃ´ng hoÃ n thÃ nh")
        sys.exit(1)

if __name__ == "__main__":
    main() 