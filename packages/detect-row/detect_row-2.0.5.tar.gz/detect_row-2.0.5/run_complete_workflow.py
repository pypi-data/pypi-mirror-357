#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WORKFLOW T·ª∞ ƒê·ªòNG HO√ÄN CH·ªàNH - TR√çCH XU·∫§T B·∫¢NG
==============================================

Script t·ª± ƒë·ªông ch·∫°y to√†n b·ªô quy tr√¨nh:
1. Ki·ªÉm tra h·ªá th·ªëng v√† GPU
2. T·ªëi ∆∞u c·∫•u h√¨nh theo hardware
3. Tr√≠ch xu·∫•t b·∫£ng t·ª´ t·∫•t c·∫£ ·∫£nh
4. Tr√≠ch xu·∫•t c·ªôt v·ªõi merge t√πy ch·ªânh
5. Qu·∫£n l√Ω b·ªô nh·ªõ hi·ªáu qu·∫£
6. T·∫°o b√°o c√°o k·∫øt qu·∫£

C√°ch s·ª≠ d·ª•ng:
    python run_complete_workflow.py
    python run_complete_workflow.py --config config_template.json
    python run_complete_workflow.py --column-groups "custom:1,2;result:3,4"
    python run_complete_workflow.py --max-memory 8 --use-gpu
"""

import os
import sys
import json
import time
import argparse
import logging
import traceback
from pathlib import Path
from datetime import datetime
import cv2
import numpy as np
import psutil
import gc

# Add current directory to path
sys.path.insert(0, '.')

try:
    from detect_row import AdvancedTableExtractor, AdvancedColumnExtractor
    from detect_row.gpu_support import GPUManager, MemoryManager
except ImportError as e:
    print(f"‚ùå L·ªói import: {e}")
    print("Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t package detect_row")
    sys.exit(1)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WorkflowManager:
    """Qu·∫£n l√Ω workflow t·ª± ƒë·ªông"""
    
    def __init__(self, config_file=None, max_memory_gb=4, use_gpu=True):
        self.config_file = config_file
        self.max_memory_gb = max_memory_gb
        self.use_gpu = use_gpu
        self.start_time = time.time()
        
        # Load configuration
        self.config = self.load_config()
        
        # Initialize managers
        self.gpu_manager = GPUManager()
        self.memory_manager = MemoryManager(max_memory_gb=max_memory_gb)
        
        # Setup directories
        self.setup_directories()
        
        # Results tracking
        self.results = {
            'processed_images': 0,
            'detected_tables': 0,
            'extracted_columns': 0,
            'errors': [],
            'processing_times': []
        }
    
    def load_config(self):
        """Load c·∫•u h√¨nh t·ª´ file"""
        if self.config_file and os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"‚úÖ ƒê√£ load config t·ª´ {self.config_file}")
                return config
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è L·ªói load config: {e}, s·ª≠ d·ª•ng config m·∫∑c ƒë·ªãnh")
        
        # Default config
        return {
            "table_extraction": {
                "min_area_ratio": 0.003,
                "max_area_ratio": 0.25,
                "min_aspect_ratio": 1.0,
                "max_aspect_ratio": 15.0
            },
            "column_extraction": {
                "min_column_width": 20,
                "vertical_line_threshold": 0.4
            },
            "performance": {
                "batch_size": 8,
                "num_workers": 4
            }
        }
    
    def setup_directories(self):
        """Thi·∫øt l·∫≠p th∆∞ m·ª•c"""
        directories = [
            'input',
            'output/tables_and_columns/tables',
            'output/tables_and_columns/columns', 
            'debug/tables_and_columns',
            'reports'
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
        
        logger.info("‚úÖ ƒê√£ thi·∫øt l·∫≠p th∆∞ m·ª•c")
    
    def check_system_status(self):
        """Ki·ªÉm tra tr·∫°ng th√°i h·ªá th·ªëng"""
        logger.info("üîç Ki·ªÉm tra h·ªá th·ªëng...")
        
        # Memory check
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        
        logger.info(f"üíæ RAM: {memory_gb:.1f}GB (s·ª≠ d·ª•ng {memory.percent:.1f}%)")
        
        # GPU check
        if self.use_gpu and self.gpu_manager.is_gpu_available():
            gpu_info = self.gpu_manager.get_gpu_info()
            logger.info(f"üéÆ GPU: {gpu_info}")
        else:
            logger.info("üñ•Ô∏è S·ª≠ d·ª•ng CPU")
        
        # Disk space
        disk = psutil.disk_usage('.')
        disk_free_gb = disk.free / (1024**3)
        logger.info(f"üíø Disk free: {disk_free_gb:.1f}GB")
        
        # Warning checks
        if memory.percent > 80:
            logger.warning("‚ö†Ô∏è RAM s·ª≠ d·ª•ng cao!")
        if disk_free_gb < 1:
            logger.warning("‚ö†Ô∏è Disk space th·∫•p!")
    
    def find_input_images(self):
        """T√¨m ·∫£nh ƒë·∫ßu v√†o"""
        input_dir = Path('input')
        
        if not input_dir.exists():
            logger.error("‚ùå Th∆∞ m·ª•c input/ kh√¥ng t·ªìn t·∫°i")
            return []
        
        # Supported formats
        formats = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']
        
        images = []
        for fmt in formats:
            images.extend(input_dir.glob(fmt))
            images.extend(input_dir.glob(fmt.upper()))
        
        logger.info(f"üìÅ T√¨m th·∫•y {len(images)} ·∫£nh trong input/")
        return sorted(images)
    
    def optimize_batch_size(self, images):
        """T·ªëi ∆∞u batch size theo memory"""
        if not images:
            return 1
        
        # Estimate memory per image
        sample_image = cv2.imread(str(images[0]))
        if sample_image is None:
            return 1
        
        # Estimate memory usage (rough calculation)
        image_size_mb = sample_image.nbytes / (1024 * 1024)
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        # Conservative estimate: use 50% of available memory
        usable_memory_gb = min(available_memory_gb * 0.5, self.max_memory_gb)
        
        optimal_batch_size = max(1, int((usable_memory_gb * 1024) / (image_size_mb * 3)))  # 3x safety factor
        
        # Limit to reasonable range
        optimal_batch_size = min(optimal_batch_size, 16)
        optimal_batch_size = max(optimal_batch_size, 1)
        
        logger.info(f"‚öôÔ∏è Batch size t·ªëi ∆∞u: {optimal_batch_size}")
        return optimal_batch_size
    
    def process_image_batch(self, image_batch, column_groups=None):
        """X·ª≠ l√Ω m·ªôt batch ·∫£nh"""
        batch_results = []
        
        for image_path in image_batch:
            try:
                start_time = time.time()
                
                logger.info(f"üîÑ X·ª≠ l√Ω: {image_path.name}")
                
                # 1. Table extraction
                table_extractor = AdvancedTableExtractor(
                    input_dir=str(image_path.parent),
                    output_dir="output/tables_and_columns/tables",
                    debug_dir="debug/tables_and_columns"
                )
                
                tables = table_extractor.extract_tables_from_image(image_path.name)
                
                if not tables:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán b·∫£ng trong {image_path.name}")
                    continue
                
                logger.info(f"‚úÖ Ph√°t hi·ªán {len(tables)} b·∫£ng")
                self.results['detected_tables'] += len(tables)
                
                # 2. Column extraction cho m·ªói b·∫£ng
                for i, table_info in enumerate(tables):
                    table_name = f"{image_path.stem}_table_{i}"
                    
                    # Column extraction
                    column_extractor = AdvancedColumnExtractor(
                        input_dir="output/tables_and_columns/tables",
                        output_dir=f"output/tables_and_columns/columns/{table_name}",
                        debug_dir=f"debug/tables_and_columns/columns/{table_name}"
                    )
                    
                    table_image_name = f"{table_name}.jpg"
                    columns = column_extractor.extract_columns_from_image(
                        table_image_name, 
                        column_groups=column_groups
                    )
                    
                    if columns:
                        logger.info(f"‚úÖ Tr√≠ch xu·∫•t {len(columns)} c·ªôt t·ª´ {table_name}")
                        self.results['extracted_columns'] += len(columns)
                
                # Track processing time
                processing_time = time.time() - start_time
                self.results['processing_times'].append(processing_time)
                
                logger.info(f"‚è±Ô∏è Ho√†n th√†nh {image_path.name} trong {processing_time:.2f}s")
                
                batch_results.append({
                    'image': image_path.name,
                    'tables': len(tables),
                    'processing_time': processing_time
                })
                
                self.results['processed_images'] += 1
                
            except Exception as e:
                error_msg = f"L·ªói x·ª≠ l√Ω {image_path.name}: {e}"
                logger.error(f"‚ùå {error_msg}")
                self.results['errors'].append(error_msg)
                
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(traceback.format_exc())
        
        return batch_results
    
    def process_all_images(self, images, column_groups=None):
        """X·ª≠ l√Ω t·∫•t c·∫£ ·∫£nh theo batch"""
        if not images:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ ·∫£nh ƒë·ªÉ x·ª≠ l√Ω")
            return []
        
        # Optimize batch size
        batch_size = self.optimize_batch_size(images)
        
        all_results = []
        total_batches = (len(images) + batch_size - 1) // batch_size
        
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(images)} ·∫£nh trong {total_batches} batch")
        
        for batch_idx in range(0, len(images), batch_size):
            batch_num = (batch_idx // batch_size) + 1
            
            logger.info(f"üì¶ Batch {batch_num}/{total_batches}")
            
            # Get batch
            batch = images[batch_idx:batch_idx + batch_size]
            
            # Monitor memory before processing
            memory_before = psutil.virtual_memory().percent
            
            # Process batch
            batch_results = self.process_image_batch(batch, column_groups)
            all_results.extend(batch_results)
            
            # Memory management
            memory_after = psutil.virtual_memory().percent
            logger.info(f"üíæ Memory: {memory_before:.1f}% ‚Üí {memory_after:.1f}%")
            
            if memory_after > 85:
                logger.warning("‚ö†Ô∏è Memory cao, ch·∫°y garbage collection...")
                gc.collect()
                
                if self.gpu_manager.is_gpu_available():
                    try:
                        import torch
                        torch.cuda.empty_cache()
                        logger.info("üóëÔ∏è ƒê√£ d·ªçn cache GPU")
                    except:
                        pass
            
            # Progress report
            processed = min(batch_idx + batch_size, len(images))
            progress = (processed / len(images)) * 100
            logger.info(f"üìä Ti·∫øn ƒë·ªô: {progress:.1f}% ({processed}/{len(images)})")
        
        return all_results
    
    def generate_report(self, processing_results):
        """T·∫°o b√°o c√°o chi ti·∫øt"""
        report_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"reports/workflow_report_{report_time}.json"
        
        # Calculate statistics
        total_time = time.time() - self.start_time
        avg_time_per_image = (
            sum(self.results['processing_times']) / len(self.results['processing_times'])
            if self.results['processing_times'] else 0
        )
        
        # Create detailed report
        report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_runtime": f"{total_time:.2f}s",
                "config_file": self.config_file,
                "max_memory_gb": self.max_memory_gb,
                "use_gpu": self.use_gpu
            },
            "system_info": {
                "platform": os.name,
                "cpu_count": psutil.cpu_count(),
                "total_memory_gb": psutil.virtual_memory().total / (1024**3),
                "gpu_available": self.gpu_manager.is_gpu_available(),
                "gpu_info": self.gpu_manager.get_gpu_info() if self.gpu_manager.is_gpu_available() else None
            },
            "processing_summary": {
                "total_images_processed": self.results['processed_images'],
                "total_tables_detected": self.results['detected_tables'],
                "total_columns_extracted": self.results['extracted_columns'],
                "total_errors": len(self.results['errors']),
                "avg_time_per_image": f"{avg_time_per_image:.2f}s",
                "success_rate": f"{(self.results['processed_images'] / (self.results['processed_images'] + len(self.results['errors'])) * 100):.1f}%" if (self.results['processed_images'] + len(self.results['errors'])) > 0 else "0.0%"
            },
            "detailed_results": processing_results,
            "errors": self.results['errors'],
            "performance_metrics": {
                "processing_times": self.results['processing_times'],
                "memory_usage": self.memory_manager.get_usage_history() if hasattr(self.memory_manager, 'get_usage_history') else []
            }
        }
        
        # Save report
        os.makedirs('reports', exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìã B√°o c√°o ƒë√£ l∆∞u: {report_file}")
        
        # Print summary
        self.print_summary(report)
        
        return report_file
    
    def print_summary(self, report):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        print("\n" + "="*60)
        print("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ WORKFLOW")
        print("="*60)
        
        summary = report['processing_summary']
        metadata = report['metadata']
        
        print(f"‚è±Ô∏è  Th·ªùi gian ch·∫°y: {metadata['total_runtime']}")
        print(f"üì∏ ·∫¢nh ƒë√£ x·ª≠ l√Ω: {summary['total_images_processed']}")
        print(f"üìã B·∫£ng ph√°t hi·ªán: {summary['total_tables_detected']}")
        print(f"üìä C·ªôt tr√≠ch xu·∫•t: {summary['total_columns_extracted']}")
        print(f"‚ùå L·ªói: {summary['total_errors']}")
        print(f"‚úÖ T·ªâ l·ªá th√†nh c√¥ng: {summary['success_rate']}")
        print(f"‚ö° Th·ªùi gian TB/·∫£nh: {summary['avg_time_per_image']}")
        
        if self.results['errors']:
            print(f"\nüö® C√ÅC L·ªñI:")
            for error in self.results['errors'][:5]:  # Show first 5 errors
                print(f"   ‚Ä¢ {error}")
            if len(self.results['errors']) > 5:
                print(f"   ... v√† {len(self.results['errors']) - 5} l·ªói kh√°c")
        
        # File structure
        print(f"\nüìÅ K·∫æT QU·∫¢ TRONG:")
        print(f"   üìã B·∫£ng: output/tables_and_columns/tables/")
        print(f"   üìä C·ªôt: output/tables_and_columns/columns/")
        print(f"   üêõ Debug: debug/tables_and_columns/")
        print(f"   üìã B√°o c√°o: reports/")
        
        print("="*60)
    
    def run_complete_workflow(self, column_groups=None):
        """Ch·∫°y workflow ho√†n ch·ªânh"""
        try:
            logger.info("üöÄ B·∫ÆT ƒê·∫¶U WORKFLOW T·ª∞ ƒê·ªòNG")
            
            # 1. System check
            self.check_system_status()
            
            # 2. Find images
            images = self.find_input_images()
            if not images:
                logger.error("‚ùå Kh√¥ng c√≥ ·∫£nh ƒë·ªÉ x·ª≠ l√Ω!")
                return None
            
            # 3. Process all images
            processing_results = self.process_all_images(images, column_groups)
            
            # 4. Generate report
            report_file = self.generate_report(processing_results)
            
            logger.info("üéâ WORKFLOW HO√ÄN TH√ÄNH!")
            return report_file
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è Workflow b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
            return None
        except Exception as e:
            logger.error(f"üí• L·ªói nghi√™m tr·ªçng: {e}")
            logger.debug(traceback.format_exc())
            return None

def parse_column_groups(groups_str):
    """Parse column groups t·ª´ string"""
    if not groups_str:
        return None
    
    groups = {}
    for group_def in groups_str.split(';'):
        if ':' in group_def:
            name, cols = group_def.split(':', 1)
            col_list = [int(c.strip()) for c in cols.split(',')]
            groups[name.strip()] = col_list
    
    return groups

def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description='Workflow t·ª± ƒë·ªông tr√≠ch xu·∫•t b·∫£ng',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª• s·ª≠ d·ª•ng:
  python run_complete_workflow.py
  python run_complete_workflow.py --config config_template.json
  python run_complete_workflow.py --column-groups "header:1;content:2,3;footer:4"
  python run_complete_workflow.py --max-memory 8 --use-gpu
  python run_complete_workflow.py --no-gpu --max-memory 2
        """
    )
    
    parser.add_argument('--config', 
                       help='File c·∫•u h√¨nh JSON')
    parser.add_argument('--column-groups',
                       help='ƒê·ªãnh nghƒ©a nh√≥m c·ªôt (format: name:1,2;name2:3)')
    parser.add_argument('--max-memory', type=float, default=4,
                       help='Gi·ªõi h·∫°n memory (GB), m·∫∑c ƒë·ªãnh: 4')
    parser.add_argument('--use-gpu', action='store_true', default=True,
                       help='S·ª≠ d·ª•ng GPU (m·∫∑c ƒë·ªãnh)')
    parser.add_argument('--no-gpu', action='store_true',
                       help='Kh√¥ng s·ª≠ d·ª•ng GPU')
    parser.add_argument('--verbose', action='store_true',
                       help='Hi·ªÉn th·ªã log chi ti·∫øt')
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Parse column groups
    column_groups = parse_column_groups(args.column_groups)
    
    # Determine GPU usage
    use_gpu = args.use_gpu and not args.no_gpu
    
    # Create and run workflow
    workflow = WorkflowManager(
        config_file=args.config,
        max_memory_gb=args.max_memory,
        use_gpu=use_gpu
    )
    
    if column_groups:
        logger.info(f"üìä S·ª≠ d·ª•ng nh√≥m c·ªôt: {column_groups}")
    
    report_file = workflow.run_complete_workflow(column_groups)
    
    if report_file:
        print(f"\nüìã B√°o c√°o chi ti·∫øt: {report_file}")
    else:
        print("\n‚ùå Workflow kh√¥ng ho√†n th√†nh")
        sys.exit(1)

if __name__ == "__main__":
    main() 